# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/base_lf.ipynb.

# %% auto 0
__all__ = ['device', 'bs', 'ps', 'x', 'y', 'max_corr', 'seed_everything', 'random_sinusoid', 'C_z1z2', 'Cdiff_Rand', 'Max_Corr',
           'Cdiff_Sup']

# %% ../nbs/base_lf.ipynb 3
#import self_supervised
#import torch
from fastai.vision.all import *
import random
import os
import numpy as np
#from self_supervised.augmentations import *
#from self_supervised.layers import *

# %% ../nbs/base_lf.ipynb 4
device='cuda' if torch.cuda.is_available() else 'cpu'

# %% ../nbs/base_lf.ipynb 6
def seed_everything(seed=42):
    """"
    Seed everything.
    """   
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True

# %% ../nbs/base_lf.ipynb 7
def random_sinusoid(x,ps=500,std=0.1,seed=0):
    
    seed_everything(seed=seed)
    
    X = torch.randn(6,ps).to(device) #use this to get t,s,u,v,a,b i.e. random components of sinusoid
    
    t,s = std*X[0:2,:]
    u,v = X[2:4,:]
    a,b = 0.2*X[4:6,:]

    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)

# %% ../nbs/base_lf.ipynb 11
def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs,indep=True):
    
    if indep == False:
        C1 =  (z1norm.T @ z2norm_2) / bs
        C2 = (z1norm_2.T @ z2norm) / bs
        cdiff = (0.5*C1.pow(2) + 0.5*C2.pow(2))
        
    elif indep == True:
        cdiff =  (z1norm_2.T @ z2norm_2) / bs
        
    return cdiff

# %% ../nbs/base_lf.ipynb 12
class Cdiff_Rand:
    
    def __init__(self,seed,bs,ps,std=0.1,K=2):
        self.seed=seed
        self.bs=bs
        self.ps=ps
        self.std=std
        self.K=K

    def __call__(self,z1norm,z2norm):
        
        K=self.K
        cdiff_rand=0
        for i in range(K):

            z1norm_2,z2norm_2 = random_sinusoid(x=z1norm,ps=ps,std=std,seed=self.seed+i), random_sinusoid(x=z2norm,ps=ps,std=std,seed=2*self.seed+i)
            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)

        cdiff_rand=(1/K)*cdiff_rand
    
        return cdiff_rand

# %% ../nbs/base_lf.ipynb 13
class Max_Corr(nn.Module):
    def __init__(self,ps):
        super().__init__()
        self.m1 = nn.Sequential(nn.Linear(ps,ps),nn.Sigmoid(),nn.Linear(ps,ps)) #feedforward net one hidden layer
        self.m2 = nn.Sequential(nn.Linear(ps,ps),nn.ReLU(),nn.Linear(ps,ps)) #feedforward net one hidden layer
    
    def forward(self,x,y):
        return self.m1(x),self.m2(y)

# %% ../nbs/base_lf.ipynb 15
bs,ps=32,500
x,y=torch.rand(bs,ps),torch.rand(bs,ps)
max_corr = Max_Corr(ps=ps)
test([max_corr(x,y)[0].shape,max_corr(x,y)[1].shape], [x.shape,y.shape],  all_equal)

# %% ../nbs/base_lf.ipynb 16
class Cdiff_Sup:
    
    def __init__(self,I,inner_steps,bs):
        
        self.I=I
        self.inner_steps=inner_steps
        self.bs=bs
        self.max_corr = Max_Corr()
        if device == 'cuda':
            self.max_corr.cuda()
        
    def inner_step(self,z1norm,z2norm):
    
        max_corr=self.max_corr
        I=self.I
        bs=self.bs
        inner_steps=self.inner_steps
        z1norm=z1norm.detach()
        z2norm=z2norm.detach()
        max_corr = Max_Corr()
        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)
        
        for i in range(inner_steps):
            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)        
            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)
            inner_loss=-1*(cdiff_2*(1-I)).mean()
            optimizer.zero_grad()
            inner_loss.backward()
            optimizer.step()
        
        for p in max_corr.parameters():
            p.requires_grad=False
            
        return max_corr
    
    def __call__(self,z1norm,z2norm):
        
            max_corr =  self.inner_step(z1norm,z2norm)
            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)
            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)
    
            return cdiff_sup
