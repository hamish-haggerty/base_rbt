{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_lf\n",
    "\n",
    "> In this module we have all the classes and functions we need to define\n",
    "  the different kinds of `lf` that we need when training modified forms of BT.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from base_rbt.base_model import *\n",
    "from fastai.vision.all import *\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the base functions and classes to build `lf` variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def random_sinusoid(x,std=0.1,seed=0):\n",
    "    \n",
    "    device=default_device().type\n",
    "    ps = x.shape[1]\n",
    "    #TODO: Get this working in a reproducible way\n",
    "    #seed_everything(seed=seed) \n",
    "    X = torch.randn(6,ps).to(device) #use this to get t,s,u,v,a,b i.e. random components of sinusoid\n",
    "    \n",
    "    t,s = std*X[0:2,:]\n",
    "    u,v = X[2:4,:]\n",
    "    a,b = 0.2*X[4:6,:]\n",
    "\n",
    "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that shape of output is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "bs,ps=32,500\n",
    "znorm=torch.rand(bs,ps)\n",
    "\n",
    "test(random_sinusoid(znorm).shape,znorm.shape,  all_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There is something funny going on with random seed. For now, I'm going to just treat\n",
    "the sinusoids as completely random and see how we go. Since the batch size will be smaller\n",
    "might not be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#todo: (provided we uncomment seed_everything(seed=seed) from above) then this test will often fail; however\n",
    "#if we simply remove the last line `seed_everything(seed=10)` then it always passes. Weird. \n",
    "# bs,ps=32,500\n",
    "# znorm=torch.rand(bs,ps)\n",
    "# test_eq(random_sinusoid(znorm).sum().item(),-46.482173919677734)\n",
    "# seed_everything(seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: we need to write other tests to make sure this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,indep=True):\n",
    "    \n",
    "    bs = z1norm.shape[0]\n",
    "    if indep == False:\n",
    "        C1 =  (z1norm.T @ z2norm_2) / bs\n",
    "        C2 = (z1norm_2.T @ z2norm) / bs\n",
    "        cdiff = (0.5*C1.pow(2) + 0.5*C2.pow(2))\n",
    "        \n",
    "    elif indep == True:\n",
    "        cdiff =  (z1norm_2.T @ z2norm_2) / bs\n",
    "        \n",
    "    return cdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that shape of output is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,d = 32,100 \n",
    "z1norm,z1norm_2,z2norm,z2norm_2 = torch.rand(32,100),torch.rand(32,100),torch.rand(32,100),torch.rand(32,100)\n",
    "\n",
    "test(C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2).shape,torch.rand(d,d).shape,  all_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Cdiff_Rand:\n",
    "    \n",
    "    def __init__(self,seed,std=0.1,K=2,indep=False):\n",
    "        self.seed=seed\n",
    "        self.std=std\n",
    "        self.K=K\n",
    "        self.indep=indep\n",
    "\n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "        K=self.K\n",
    "        cdiff_rand=0\n",
    "        for i in range(K):\n",
    "            \n",
    "            z1norm_2,z2norm_2 = random_sinusoid(x=z1norm,std=self.std,seed=self.seed+i), random_sinusoid(x=z2norm,std=self.std,seed=2*self.seed+i)\n",
    "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,indep=self.indep)\n",
    "\n",
    "        cdiff_rand=(1/K)*cdiff_rand\n",
    "    \n",
    "        return cdiff_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Max_Corr(nn.Module):\n",
    "    def __init__(self,\n",
    "                 qs #qs will tend to be =ps i.e. projection dimension, although this is not required. \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.m1 = nn.Sequential(nn.Linear(qs,qs),nn.Sigmoid(),nn.Linear(qs,qs)) #feedforward net one hidden layer\n",
    "        self.m2 = nn.Sequential(nn.Linear(qs,qs),nn.ReLU(),nn.Linear(qs,qs)) #feedforward net one hidden layer\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        return self.m1(x),self.m2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that shape of output is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "bs,ps=32,500\n",
    "x,y=torch.rand(bs,ps),torch.rand(bs,ps)\n",
    "max_corr = Max_Corr(qs=ps)\n",
    "test([max_corr(x,y)[0].shape,max_corr(x,y)[1].shape], [x.shape,y.shape],  all_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Cdiff_Sup:\n",
    "    \n",
    "    def __init__(self,I,qs,inner_steps,indep=True):\n",
    "        \n",
    "        self.I=I\n",
    "        self.qs=qs\n",
    "        self.inner_steps=inner_steps\n",
    "        self.indep=indep\n",
    "        self.max_corr = Max_Corr(qs=qs)\n",
    "        if default_device().type == 'cuda':\n",
    "            self.max_corr.cuda()\n",
    "        \n",
    "    def inner_step(self,z1norm,z2norm):\n",
    "    \n",
    "        max_corr=self.max_corr\n",
    "        inner_steps=self.inner_steps\n",
    "        z1norm=z1norm.detach()\n",
    "        z2norm=z2norm.detach()\n",
    "        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
    "        \n",
    "        for i in range(inner_steps):\n",
    "            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)        \n",
    "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,indep=self.indep)\n",
    "            inner_loss=-1*(cdiff_2*(1-self.I)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            inner_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        for p in max_corr.parameters():\n",
    "            p.requires_grad=False\n",
    "            \n",
    "        return max_corr\n",
    "    \n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "            max_corr =  self.inner_step(z1norm,z2norm)\n",
    "            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
    "            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,indep=self.indep)\n",
    "    \n",
    "            return cdiff_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
