{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics\n",
    "\n",
    "> Metrics (e.g. ROC) and also a `predict_model` function that is more efficient than FastAI defaults when the dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions given xval and yval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from statistics import mean,stdev\n",
    "\n",
    "import numpy as np\n",
    "import scikitplot \n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score,precision_recall_curve,roc_curve,auc,classification_report,confusion_matrix\n",
    "#from scipy import interp\n",
    "from numpy import interp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_model(xval,yval,model,aug_pipelines_test,numavg=3,criterion = CrossEntropyLossFlat(),deterministic=False):\n",
    "    \"Note that this assumes xval is entire validation set. If it doesn't fit in memory, can't use this guy\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    N=xval.shape[0]\n",
    "    \n",
    "    if not deterministic:\n",
    "\n",
    "        probs=0\n",
    "        for _ in range(numavg):\n",
    "\n",
    "            probs += torch.softmax(model(aug_pipelines_test[0](xval)),dim=1) #test time augmentation. This also gets around issue of randomness in the dataloader in each session...\n",
    "\n",
    "        probs *= 1/numavg\n",
    "        \n",
    "    else:\n",
    "        probs = torch.softmax(model(xval),dim=1)\n",
    "\n",
    "    \n",
    "    ypred = cast(torch.argmax(probs, dim=1),TensorCategory)\n",
    "\n",
    "    correct = (ypred == yval)#.type(torch.FloatTensor)\n",
    "\n",
    "    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)\n",
    "    num_correct = correct.sum()\n",
    "    accuracy = num_correct/N\n",
    "\n",
    "    #val_loss = criterion(scores,yval)\n",
    "    \n",
    "    return probs,ypred,accuracy.item()#,val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def predict_ensemble(yval,scores1,scores2):\n",
    "    \"scores can be normalized (softmax) or not\"\n",
    "\n",
    "    N=yval.shape[0]\n",
    "\n",
    "    scores = 0.5*scores1 + 0.5*scores2\n",
    "\n",
    "    ypred = cast(torch.argmax(scores, dim=1),TensorCategory)\n",
    "\n",
    "    correct = (ypred == yval)#.type(torch.FloatTensor)\n",
    "\n",
    "    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)\n",
    "    num_correct = correct.sum()\n",
    "    accuracy = num_correct/N\n",
    "    \n",
    "    return ypred,accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def classification_report_wrapper(ypred, y, int_to_classes, print_report=True):\n",
    "    # Convert ypred and y to numpy arrays\n",
    "    ypred = ypred.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "\n",
    "    # Get the unique classes in y\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # Create labels from unique_y and vocab\n",
    "    labels = [int_to_classes[i] for i in unique_y]\n",
    "\n",
    "    # Get the classification report as a dictionary\n",
    "    report = classification_report(y, ypred, target_names=labels, output_dict=True)\n",
    "\n",
    "    if print_report:\n",
    "        print(classification_report(y, ypred, target_names=labels))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Dog       1.00      0.67      0.80         3\n",
      "         Cat       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.75      0.83      0.73         4\n",
      "weighted avg       0.88      0.75      0.77         4\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Dog       0.75      1.00      0.86         3\n",
      "         Cat       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "ypred=np.array([0,1,0,1])\n",
    "ypred2=np.array([0,0,0,0])\n",
    "y=np.array([0,0,0,1])\n",
    "int_to_classes={0:'Dog',1:'Cat'}\n",
    "labels = ['Dog','Cat']\n",
    "#classification_report_wrapper(ypred, y, int_to_classes)\n",
    "\n",
    "report = classification_report(y, ypred, target_names=labels,output_dict=True)\n",
    "report2 = classification_report(y, ypred2, target_names=labels,output_dict=True)\n",
    "print(classification_report(y, ypred, target_names=labels))\n",
    "print(classification_report(y, ypred2, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#This code comes from https://gist.github.com/geblanco/5cfe4a3224e021113968a8c7ebe31419\n",
    "def format_classification_report(data_dict):\n",
    "\n",
    "\n",
    "    non_label_keys = [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "    y_type = \"binary\"\n",
    "    digits = 5\n",
    "\n",
    "    target_names = [\n",
    "        \"%s\" % key for key in data_dict.keys() if key not in non_label_keys\n",
    "    ]\n",
    "\n",
    "    # labelled micro average\n",
    "    micro_is_accuracy = (y_type == \"multiclass\" or y_type == \"binary\")\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    p = [data_dict[l][headers[0]] for l in target_names]\n",
    "    r = [data_dict[l][headers[1]] for l in target_names]\n",
    "    f1 = [data_dict[l][headers[2]] for l in target_names]\n",
    "    s = [data_dict[l][headers[3]] for l in target_names]\n",
    "\n",
    "    rows = zip(target_names, p, r, f1, s)\n",
    "\n",
    "    if y_type.startswith(\"multilabel\"):\n",
    "        average_options = (\"micro\", \"macro\", \"weighted\", \"samples\")\n",
    "    else:\n",
    "        average_options = (\"micro\", \"macro\", \"weighted\")\n",
    "\n",
    "    longest_last_line_heading = \"weighted avg\"\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = \"{:>{width}s} \" + \" {:>15}\" * len(headers)\n",
    "    report = head_fmt.format(\"\", *headers, width=width)\n",
    "    report += \"\\n\\n\"\n",
    "    row_fmt = \"{:>{width}s} \" + \" {:>15.{digits}f}\" * 3 + \" {:>15}\\n\"\n",
    "\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "    report += \"\\n\"\n",
    "\n",
    "    # compute all applicable averages\n",
    "    for average in average_options:\n",
    "        if average.startswith(\"micro\") and micro_is_accuracy:\n",
    "            line_heading = \"accuracy\"\n",
    "        else:\n",
    "            line_heading = average + \" avg\"\n",
    "\n",
    "        if line_heading == \"accuracy\":\n",
    "            avg = [data_dict[line_heading], sum(s)]\n",
    "            \n",
    "            row_fmt_accuracy = \"{:>{width}s} \" + \\\n",
    "                   \" {:>15.{digits}}\" * 2 + \" {:>15.{digits}f}\" + \\\n",
    "                   \" {:>15}\\n\"\n",
    "\n",
    "            report += row_fmt_accuracy.format(line_heading, \"\", \"\",\n",
    "                                              *avg, width=width,\n",
    "                                              digits=digits)\n",
    "        else:\n",
    "            avg = list(data_dict[line_heading].values())\n",
    "            report += row_fmt.format(line_heading, *avg,\n",
    "                                     width=width, digits=digits)\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def Mean_Report(reports, classes):\n",
    "    N = len(reports)\n",
    "    mean_report = {}\n",
    "    for k in reports[0].keys():\n",
    "        if k!='accuracy':\n",
    "            mean_report[k] = {}\n",
    "            for metric in reports[0][k].keys():\n",
    "                att = 0\n",
    "                for _report in reports:\n",
    "                    att += _report[k][metric]\n",
    "                mean_report[k][metric] = att / N\n",
    "        else:\n",
    "            att = 0\n",
    "            for _report in reports:\n",
    "                att += _report[k]\n",
    "                \n",
    "            mean_report[k] = att / N\n",
    "    return mean_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_confusion_matrix(ypred, y, vocab):\n",
    "    # Convert ypred and y to numpy arrays\n",
    "    ypred = ypred.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    \n",
    "    # Get the class labels from vocab\n",
    "    labels = [vocab[i] for i in range(len(vocab))]\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y, ypred)\n",
    "    \n",
    "    # Create a DataFrame from the confusion matrix\n",
    "    df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
    "    \n",
    "    # Use seaborn to create a heatmap of the confusion matrix with blue and white colors\n",
    "    sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We monkey patch some plotting functions from scikitplot and edit them - we need greater control of legend etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _plot_precision_recall(y_true, y_probas,\n",
    "                          title='Precision-Recall Curve',\n",
    "                          plot_micro=True,\n",
    "                          classes_to_plot=None, ax=None,\n",
    "                          figsize=None, cmap='nipy_spectral',\n",
    "                          title_fontsize=\"large\",\n",
    "                          text_fontsize=\"small\"):\n",
    "    \"\"\"Generates the Precision Recall Curve from labels and probabilities. This is moneky patched from: \n",
    "    https://github.com/reiinakano/scikit-plot/blob/26007fbf9f05e915bd0f6acb86850b01b00944cf/scikitplot/metrics.py\n",
    "\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_probas = np.array(y_probas)\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    probas = y_probas\n",
    "\n",
    "    if classes_to_plot is None:\n",
    "        classes_to_plot = classes\n",
    "\n",
    "    binarized_y_true = label_binarize(y_true, classes=classes)\n",
    "    if len(classes) == 2:\n",
    "        binarized_y_true = np.hstack(\n",
    "            (1 - binarized_y_true, binarized_y_true))\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.set_title(title, fontsize=title_fontsize)\n",
    "\n",
    "    indices_to_plot = np.in1d(classes, classes_to_plot)\n",
    "    for i, to_plot in enumerate(indices_to_plot):\n",
    "        if to_plot:\n",
    "            average_precision = average_precision_score(\n",
    "                binarized_y_true[:, i],\n",
    "                probas[:, i])\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                y_true, probas[:, i], pos_label=classes[i])\n",
    "            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n",
    "            ax.plot(recall, precision, lw=2,\n",
    "                    label='{0} '\n",
    "                          '(area = {1:0.3f})'.format(classes[i],\n",
    "                                                     average_precision),\n",
    "                    color=color)\n",
    "\n",
    "    if plot_micro:\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            binarized_y_true.ravel(), probas.ravel())\n",
    "        average_precision = average_precision_score(binarized_y_true,\n",
    "                                                    probas,\n",
    "                                                    average='micro')\n",
    "        ax.plot(recall, precision,\n",
    "                label='micro-avg '\n",
    "                      '(area = {0:0.3f})'.format(average_precision),\n",
    "                color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc='lower left', fontsize=text_fontsize)\n",
    "    return ax\n",
    "\n",
    "def _plot_roc(y_true, y_probas, title='ROC Curves',\n",
    "                   plot_micro=True, plot_macro=True, classes_to_plot=None,\n",
    "                   ax=None, figsize=None, cmap='nipy_spectral',\n",
    "                   title_fontsize=\"large\", text_fontsize=\"medium\"):\n",
    "    \"\"\"Generates the ROC curves from labels and predicted scores/probabilities. Monkey patched like above function.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_probas = np.array(y_probas)\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    probas = y_probas\n",
    "\n",
    "    if classes_to_plot is None:\n",
    "        classes_to_plot = classes\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    ax.set_title(title, fontsize=title_fontsize)\n",
    "\n",
    "    fpr_dict = dict()\n",
    "    tpr_dict = dict()\n",
    "\n",
    "    indices_to_plot = np.in1d(classes, classes_to_plot)\n",
    "    for i, to_plot in enumerate(indices_to_plot):\n",
    "        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true, probas[:, i],\n",
    "                                                pos_label=classes[i])\n",
    "        if to_plot:\n",
    "            roc_auc = auc(fpr_dict[i], tpr_dict[i])\n",
    "            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n",
    "            ax.plot(fpr_dict[i], tpr_dict[i], lw=2, color=color,\n",
    "                    label='{0} (area = {1:0.2f})'\n",
    "                          ''.format(classes[i], roc_auc))\n",
    "\n",
    "    if plot_micro:\n",
    "        binarized_y_true = label_binarize(y_true, classes=classes)\n",
    "        if len(classes) == 2:\n",
    "            binarized_y_true = np.hstack(\n",
    "                (1 - binarized_y_true, binarized_y_true))\n",
    "        fpr, tpr, _ = roc_curve(binarized_y_true.ravel(), probas.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr,\n",
    "                label='micro-avg'\n",
    "                      '(area = {0:0.2f})'.format(roc_auc),\n",
    "                color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    if plot_macro:\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr_dict[x] for x in range(len(classes))]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(len(classes)):\n",
    "            mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= len(classes)\n",
    "        roc_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "        ax.plot(all_fpr, mean_tpr,\n",
    "                label='macro-avg'\n",
    "                      '(area = {0:0.2f})'.format(roc_auc),\n",
    "                color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=text_fontsize)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=text_fontsize)\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc='lower right', fontsize=text_fontsize)\n",
    "    return ax\n",
    "\n",
    "#| export\n",
    "\n",
    "def plot_roc(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "\n",
    "    #scikitplot.metrics.plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "    _plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "\n",
    "def plot_pr(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "\n",
    "    #scikitplot.metrics.plot_precision_recall(_ytest, probs)#,plot_micro=True,plot_macro=False)\n",
    "    _plot_precision_recall(_ytest, probs,plot_micro=True)\n",
    "\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to plot ROC curve and precision-recall curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def plot_roc(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "    \n",
    "    #scikitplot.metrics.plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "    _plot_roc(_ytest, probs,plot_micro=True,plot_macro=False)\n",
    "\n",
    "def plot_pr(ytest,probs,int_to_classes):\n",
    "    \n",
    "    #We want the AUC dict; and we want a plot as well.\n",
    "    \n",
    "    ytest = ytest.cpu().numpy()\n",
    "    _ytest = [int_to_classes[i] for i in ytest] #e.g. ['AK','MEL',...]\n",
    "    \n",
    "    #scikitplot.metrics.plot_precision_recall(_ytest, probs)#,plot_micro=True,plot_macro=False)\n",
    "    _plot_precision_recall(_ytest, probs,plot_micro=True)\n",
    "\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def Auc_Dict(ytest,probs,int_to_classes=None):\n",
    "    \"Mostly used to verify results of plot (debug)\"\n",
    "\n",
    "    # # define your ytest and probs variables\n",
    "    # ytest = ['cat', 'dog', 'bear', 'dog', 'cat']\n",
    "    # probs = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1], [0.2, 0.3, 0.5], [0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])\n",
    "\n",
    "    # calculate AUC for each class\n",
    "    auc_dict = {}\n",
    "    for i, cls in enumerate(np.unique(ytest)):\n",
    "        y_true = np.array([1 if y == cls else 0 for y in ytest]) # one-vs-all labels\n",
    "        y_score = probs[:, i] # get probability for current class\n",
    "        auc = roc_auc_score(y_true, y_score)#, multi_class='ovr')\n",
    "        auc_dict[cls] = auc\n",
    "\n",
    "    if int_to_classes!=None:\n",
    "        auc_dict = {int_to_classes[i]:auc_dict[i] for i in auc_dict}\n",
    "    \n",
    "    return auc_dict\n",
    "\n",
    "\n",
    "def Pr_Dict(ytest,probs,int_to_classes=None):\n",
    "    \"Mostly used to verify results of plot (debug)\"\n",
    "\n",
    "    # # define your ytest and probs variables\n",
    "    # ytest = ['cat', 'dog', 'bear', 'dog', 'cat']\n",
    "    # probs = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1], [0.2, 0.3, 0.5], [0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])\n",
    "\n",
    "    # calculate AUC for each class\n",
    "    pr_dict = {}\n",
    "    for i, cls in enumerate(np.unique(ytest)):\n",
    "        y_true = np.array([1 if y == cls else 0 for y in ytest]) # one-vs-all labels\n",
    "        y_score = probs[:, i] # get probability for current class\n",
    "        pr = average_precision_score(y_true, y_score)#, multi_class='ovr')\n",
    "        pr_dict[cls] = pr\n",
    "\n",
    "    if int_to_classes!=None:\n",
    "        pr_dict = {int_to_classes[i]:pr_dict[i] for i in pr_dict}\n",
    "    \n",
    "    return pr_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_whole_model(dls_test, model, aug_pipelines_test, numavg=3, criterion=CrossEntropyLossFlat(), deterministic=False):\n",
    "    \"\"\"\n",
    "    Predicts the labels and probabilities for the entire test set using the specified model and data augmentation pipelines.\n",
    "    Returns a dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "\n",
    "    Args:\n",
    "        dls_test: The test dataloader.\n",
    "        model: The trained model.\n",
    "        aug_pipelines_test: The test data augmentation pipelines.\n",
    "        numavg: The number of times to perform test-time augmentation.\n",
    "        criterion: The loss function to use for computing the accuracy.\n",
    "        deterministic: Whether to use deterministic computation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the labels, probabilities, predicted labels, and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_len = len(dls_test.dataset)\n",
    "    y = torch.zeros(total_len, dtype=torch.long)\n",
    "    probs = torch.zeros(total_len, model.head.out_features)\n",
    "    ypred = torch.zeros(total_len, dtype=torch.long)\n",
    "    start_idx = 0\n",
    "\n",
    "    progress_bar = tqdm(dls_test.train, desc=\"Predicting\", unit=\"batch\")\n",
    "    for xval, yval in progress_bar:\n",
    "        end_idx = start_idx + len(xval)\n",
    "        _probs, _ypred, acc = predict_model(xval, yval, model, aug_pipelines_test, numavg, criterion, deterministic)\n",
    "        y[start_idx:end_idx] = yval\n",
    "        probs[start_idx:end_idx] = _probs\n",
    "        ypred[start_idx:end_idx] = _ypred\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    acc = (ypred == y).float().mean().item()\n",
    "\n",
    "    # Return the predictions and labels in a dictionary\n",
    "    #return {'y': y, 'probs': probs, 'ypred': ypred, 'acc': acc}\n",
    "    return y, probs, ypred, acc\n",
    "\n",
    "def get_dls_metrics(dls,model,aug_pipelines_test,int_to_classes): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from model and dataloader\"\n",
    "\n",
    "    print('running `predict_whole_model`')\n",
    "    ytest,probs,preds,Acc = predict_whole_model(dls,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, ytest,int_to_classes, print_report=True)\n",
    "    \n",
    "    plot_roc(ytest,probs,int_to_classes)\n",
    "    auc_dict = Auc_Dict(ytest,probs,int_to_classes)\n",
    "    print(f'auc_dict is: {auc_dict}')\n",
    "    plot_pr(ytest,probs,int_to_classes)\n",
    "    pr_dict = Pr_Dict(ytest,probs,int_to_classes)\n",
    "    print(f'auc_dict is: {pr_dict}')\n",
    "\n",
    "    metrics['ytest']=ytest\n",
    "    metrics['probs']=probs\n",
    "    metrics['preds']=preds\n",
    "    metrics['acc']=Acc\n",
    "    metrics['auc_dict']=auc_dict\n",
    "    metrics['pr_dict']=pr_dict\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_xval_metrics(xval,yval,model,aug_pipelines_test,int_to_classes,numavg=3): #note that we can't call dls.vocab as it might be smaller on the test set\n",
    "    \"get metrics from gives batch (xval,yval)\"\n",
    "\n",
    "    probs,preds,Acc = predict_model(xval,yval,model,aug_pipelines_test,numavg=3)\n",
    "    metrics = classification_report_wrapper(preds, yval,int_to_classes, print_report=True)\n",
    "    metrics['acc']=Acc\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# def Mean_Results(results,vocab):\n",
    "#     \"Get mean classif report and display it\"\n",
    "\n",
    "#     lst = list(vocab) + ['accuracy', 'macro avg', 'weighted avg']\n",
    "#     reports=[]\n",
    "#     accs=[]\n",
    "#     for i in results.keys():\n",
    "#         if type(i)!=int:\n",
    "#             continue\n",
    "#         report = {j:results[i][j] for j in results[i].keys() if j in lst}\n",
    "#         reports.append(report)\n",
    "#         accs.append(results[i]['acc'])\n",
    "#     mean_report = Mean_Report(reports,vocab)\n",
    "#     print(format_classification_report(mean_report))\n",
    "    \n",
    "#     print(f'mean acc is {mean(accs)} with std {stdev(accs)}')\n",
    "\n",
    "#     return mean_report\n",
    "from statistics import mean, stdev\n",
    "\n",
    "def Mean_Results(results, vocab):\n",
    "    \"Get mean classification report and display it\"\n",
    "\n",
    "    lst = list(vocab) + ['accuracy', 'macro avg', 'weighted avg']\n",
    "    reports = []\n",
    "    accs = []\n",
    "    weighted_f1_scores = []  # List to store weighted F1 scores\n",
    "\n",
    "    # Loop through each result, assuming integer keys are the valid entries\n",
    "    for i in results.keys():\n",
    "        if type(i) != int:\n",
    "            continue\n",
    "        report = {j: results[i][j] for j in results[i].keys() if j in lst}\n",
    "        reports.append(report)\n",
    "        accs.append(results[i]['acc'])\n",
    "        \n",
    "        # Collect weighted average F1-score\n",
    "        if 'weighted avg' in report and 'f1-score' in report['weighted avg']:\n",
    "            weighted_f1_scores.append(report['weighted avg']['f1-score'])\n",
    "\n",
    "    # Compute mean report using a helper function, assumed to be defined\n",
    "    mean_report = Mean_Report(reports, vocab)\n",
    "\n",
    "    # Output the classification report formatted\n",
    "    print(format_classification_report(mean_report))\n",
    "\n",
    "    # Calculate and print mean and standard deviation for accuracy\n",
    "    mean_acc = mean(accs)\n",
    "    std_acc = stdev(accs)\n",
    "    print(f'Mean Accuracy: {mean_acc:.5f} with Standard Deviation: {std_acc:.5f}')\n",
    "\n",
    "    # Calculate and print mean and standard deviation for weighted F1-score\n",
    "    if weighted_f1_scores:\n",
    "        mean_weighted_f1 = mean(weighted_f1_scores)\n",
    "        std_weighted_f1 = stdev(weighted_f1_scores)\n",
    "        print(f'Mean Weighted F1-Score: {mean_weighted_f1:.5f} with Standard Deviation: {std_weighted_f1:.5f}')\n",
    "    else:\n",
    "        print(\"No Weighted F1-Scores to display.\")\n",
    "\n",
    "    return mean_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify `Mean_Results` looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision          recall        f1-score         support\n",
      "\n",
      "     class_0          0.30039         0.35205         0.31980            32.4\n",
      "     class_1          0.28549         0.26679         0.27078            32.6\n",
      "     class_2          0.30861         0.27295         0.28879            35.0\n",
      "\n",
      "    accuracy                                          0.29600           100.0\n",
      "   macro avg          0.29816         0.29726         0.29312           100.0\n",
      "weighted avg          0.30786         0.29600         0.29784           100.0\n",
      "\n",
      "Mean Accuracy: 0.29600 with Standard Deviation: 0.03782\n",
      "Mean Weighted F1-Score: 0.29784 with Standard Deviation: 0.03696\n",
      "{'class_0': {'precision': 0.300386621439253, 'recall': 0.352046783625731, 'f1-score': 0.3197979273160579, 'support': 32.4}, 'class_1': {'precision': 0.2854909867172676, 'recall': 0.2667929101651265, 'f1-score': 0.27078374361956453, 'support': 32.6}, 'class_2': {'precision': 0.30861150721631664, 'recall': 0.2729542606516291, 'f1-score': 0.2887901999130813, 'support': 35.0}, 'accuracy': 0.296, 'macro avg': {'precision': 0.29816303845761244, 'recall': 0.29726465148082887, 'f1-score': 0.29312395694956794, 'support': 100.0}, 'weighted avg': {'precision': 0.3078614159675055, 'recall': 0.296, 'f1-score': 0.29783728004232646, 'support': 100.0}}\n"
     ]
    }
   ],
   "source": [
    "#|hide \n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import tensor\n",
    "\n",
    "# Simulate data\n",
    "def simulate_data(num_samples, num_classes):\n",
    "    y_true = torch.randint(0, num_classes, (num_samples,))\n",
    "    y_pred = torch.randint(0, num_classes, (num_samples,))\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Generate mock results\n",
    "def generate_mock_results(num_experiments, num_samples, num_classes, vocab):\n",
    "    results = {}\n",
    "    for i in range(num_experiments):\n",
    "        y_true, y_pred = simulate_data(num_samples, num_classes)\n",
    "        report = classification_report(y_true.numpy(), y_pred.numpy(), target_names=vocab, output_dict=True)\n",
    "        results[i] = report\n",
    "        results[i]['acc'] = (y_true == y_pred).float().mean().item()\n",
    "    return results\n",
    "\n",
    "# Test Mean_Results\n",
    "def test_Mean_Results():\n",
    "    vocab = ['class_0', 'class_1', 'class_2']\n",
    "    num_experiments = 5  # Number of simulated experiments\n",
    "    num_samples = 100  # Number of samples per experiment\n",
    "    num_classes = len(vocab)  # Number of classes\n",
    "\n",
    "    results = generate_mock_results(num_experiments, num_samples, num_classes, vocab)\n",
    "    mean_report = Mean_Results(results, vocab)\n",
    "    print(mean_report)\n",
    "\n",
    "test_Mean_Results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
