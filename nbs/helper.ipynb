{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32189ee",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> utility stuff.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d83fcf",
   "metadata": {},
   "source": [
    "Rename this guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e903be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39912107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.test import *\n",
    "import torch\n",
    "import random \n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_grad_on(model):\n",
    "    \"\"\"\n",
    "    Test that all grads are on for modules with parameters.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Check each parameter in the module\n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            assert param.requires_grad, f\"Gradients are off for {name}.{param_name}\"\n",
    "\n",
    "def test_grad_off(model):\n",
    "    \"\"\"\n",
    "    Test that all non-batch norm grads are off, but batch norm grads are on.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Distinguish between BatchNorm and other layers\n",
    "        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert param.requires_grad, f\"BatchNorm parameter does not require grad in {name}.{param_name}\"\n",
    "        else:\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert not param.requires_grad, f\"Gradients are on for non-BatchNorm layer {name}.{param_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
