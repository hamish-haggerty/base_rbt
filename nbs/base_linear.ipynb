{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_linear\n",
    "\n",
    "> API needed for linear evaluation protocol\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import self_supervised\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import kornia.augmentation as korniatfm\n",
    "import torchvision.transforms as tvtfm\n",
    "\n",
    "from base_rbt.helper import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API:\n",
    "\n",
    "\n",
    "- Train and then test linear head. Requires inputs: encoder, dls_val, augpipe_val, indim,outdim, num_epochs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#Batch level augmentations for linear classifier. At present time, just RandomResizedCrop and Normalization.\n",
    "def get_linear_batch_augs(size,resize=True,\n",
    "                    resize_scale=(0.08, 1.0),resize_ratio=(3/4, 4/3),\n",
    "                    stats=None,cuda=default_device().type == 'cuda',xtra_tfms=[]):\n",
    "    \n",
    "    \"Input batch augmentations implemented in tv+kornia+fastai\"\n",
    "    tfms = []\n",
    "    if resize:tfms += [tvtfm.RandomResizedCrop((size, size), scale=resize_scale, ratio=resize_ratio)]\n",
    "    if stats is not None: tfms += [Normalize.from_stats(*stats, cuda=cuda)]\n",
    "    tfms += xtra_tfms\n",
    "    pipe = Pipeline(tfms, split_idx = 0)\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for linear evaluation is a frozen encoder with a trainable linear head on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#Linear model \n",
    "class LinearModel(Module):\n",
    "    \"\"\"Linear model\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,\n",
    "                 indim=1024,#dimension of encoder output\n",
    "                 outdim=10, #number of classes\n",
    "                ):\n",
    "        self.encoder=encoder\n",
    "        self.L = nn.Linear(indim,outdim) \n",
    "        \n",
    "    def forward(self,x):return self.L(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'callback' for linear evaluation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearBt(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self,aug_pipelines,n_in, show_batch=False, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1= aug_pipelines[0]\n",
    "        self.aug2=Pipeline( split_idx = 0) #empty pipeline\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        self.n_in=n_in\n",
    "        self._show_batch=show_batch\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "            \n",
    "    def before_batch(self):\n",
    "\n",
    "        if self.n_in == 1:\n",
    "            xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))                            \n",
    "        elif self.n_in == 3:\n",
    "            xi,xj = self.aug1(TensorImage(self.x)), self.aug2(TensorImage(self.x))\n",
    "        self.learn.xb = (xi,)\n",
    "\n",
    "        if self._show_batch:\n",
    "            self.learn.aug_x = torch.cat([xi, xj])\n",
    "\n",
    "    def lf(self, pred, *yb):        \n",
    "        loss=self.criterion(pred,self.y)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        if self._show_batch==False:\n",
    "            print('Need to set show_batch=True')\n",
    "            return\n",
    "        bs = self.learn.aug_x.size(0)//2\n",
    "        x1,x2  = self.learn.aug_x[:bs], self.learn.aug_x[bs:]\n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone(),full=False).clamp(0,1) #full=True / False\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]]\n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide  \n",
    "encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1) #Need an encoder (this will be trained by BT)\n",
    "\n",
    "#First check gradients are on\n",
    "test_grad_on(encoder)\n",
    "\n",
    "#Next check that we can turn them off:\n",
    "OffGrad_encoder=grad_on(encoder,on=False)\n",
    "test_grad_off(OffGrad_encoder)\n",
    "\n",
    "# Original encoder gradients are now off\n",
    "test_grad_off(encoder)\n",
    "\n",
    "\n",
    "#Check that we can turn them back on:\n",
    "OffGrad_encoder=grad_on(encoder,on=True)\n",
    "test_grad_on(OffGrad_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage: First inputs needed. In the next cell we get dls_val and dls_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Inputs needed: First set hps and get dls_val and dls_test\n",
    "device=default_device().type\n",
    "cuda=(device=='cuda')\n",
    "#hps\n",
    "n_in=3\n",
    "bs=4\n",
    "bs_test=4 #Make sure it divides length of test set\n",
    "size=32\n",
    "\n",
    "#get the data\n",
    "path = untar_data(URLs.CIFAR)\n",
    "fnames = get_image_files(path / \"train\")\n",
    "fnames=fnames.shuffle()\n",
    "def label_func(fname):\n",
    "    return fname.name.split('_')[1].strip('png').strip('.')\n",
    "\n",
    "#labels for train,eval and test\n",
    "labels = [label_func(fname) for fname in fnames]\n",
    "\n",
    "#Used for training encoder i.e. BT\n",
    "dls_val = ImageDataLoaders.from_lists(path, fnames[0:1000], labels[0:1000],bs=bs, item_tfms=[Resize(size=size)], #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=0,device=device)\n",
    "\n",
    "dls_test = dls_val = ImageDataLoaders.from_lists(path, fnames[1000:2000], labels[1000:2000],bs=bs_test, item_tfms=[Resize(size=size)], #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=0,device=device)\n",
    "\n",
    "\n",
    "\n",
    "set(labels) #Check that the labels make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentations and learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=n_in) #Need an encoder (this will be trained by BT)\n",
    "fastai_encoder.eval()\n",
    "encoder = grad_on(fastai_encoder,on=False)\n",
    "model = LinearModel(encoder=encoder,indim=1024)\n",
    "aug_pipelines_val = [get_linear_batch_augs(size=size,stats=cifar_stats,resize_scale=(0.3, 1.0))]\n",
    "bt = LinearBt(aug_pipelines_val,show_batch=True,n_in=n_in,print_augs=True)\n",
    "learn = Learner(dls_val,model, cbs=[bt],metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_linear_batch(dls,n_in,aug,n=2,print_augs=True):\n",
    "    \"Given a linear learner, show a batch\"\n",
    "    bt = LinearBt(aug,show_batch=True,n_in=n_in,print_augs=print_augs)\n",
    "    learn = Learner(dls,model=None, cbs=[bt])\n",
    "    b = dls.one_batch()\n",
    "    learn._split(b)\n",
    "    learn('before_batch')\n",
    "    axes = learn.linear_bt.show(n=n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "show_linear_batch(dls=dls_val,n_in=n_in,n=2,aug=aug_pipelines_val,print_augs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to evaluate on test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need is Main_Linear_Eval in order to map from the inputs (see its init method: need an encoder, aug_pipeline\n",
    "etc) to accuracy. i.e. this is a wrapper for the whole API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#Given validation set, test set, encoder, etc return accuracy via __call__\n",
    "class Main_Linear_Eval:\n",
    "    \n",
    "    def __init__(self,size,n_in,indim,numfit, #size e.g. 32, n_in e.g. 1 or 3, indim  is encoder output dim, numfit number of epochs training \n",
    "                 dls_val,dls_test, #dls_val for training linear, dls_test for evaluation\n",
    "                 stats, #e.g. cifar_stats\n",
    "                 aug_pipelines_val, #generally simple (crop and normalizatiom)\n",
    "                 encoder\n",
    "                ):\n",
    "    \n",
    "        store_attr()\n",
    "        self.encoder=encoder\n",
    "        if self.encoder is not None:\n",
    "            self.model = LinearModel(encoder=self.encoder,indim=indim)\n",
    "     \n",
    "    #Use this guy to put the model into evaluation mode\n",
    "    def Eval_Mode(self,_model):\n",
    "\n",
    "        aug_pipelines = get_linear_batch_augs(size=self.size,resize=False,stats=self.stats)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def call(x):\n",
    "            return _model(aug_pipelines(x))\n",
    "\n",
    "        return call\n",
    "        \n",
    "    #Evaluate linear model on dls_test\n",
    "    def eval_linear(self):\n",
    "\n",
    "        eval_model = self.Eval_Mode(self.model)\n",
    "        N=len(self.dls_test.train)*self.dls_test.bs\n",
    "        test_eq(N,len(self.dls_test.train_ds)) #check that batch size divides length of test set\n",
    "\n",
    "        num_correct=0\n",
    "        for x,y in self.dls_test.train:\n",
    "\n",
    "            ypred=eval_model(x) \n",
    "            correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "            num_correct += correct.sum()\n",
    "\n",
    "        accuracy = num_correct/N\n",
    "        return accuracy.item()\n",
    "        \n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        #train linear classifier on dls_eval. Requires inputs: encoder, aug_pipeline, dls, \n",
    "        bt = LinearBt(self.aug_pipelines_val,show_batch=True,n_in=self.n_in,print_augs=False)\n",
    "        learn = Learner(self.dls_val,self.model, cbs=[bt])\n",
    "        learn.fit(self.numfit)\n",
    "        \n",
    "        #eval linear classifier\n",
    "        acc = self.eval_linear()\n",
    "        \n",
    "        return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#Main as a function -> But at the moment I think I prefer it as a class; we can setup once, and then modify the encoder\n",
    "#each time\n",
    "\n",
    "# def Main_Linear_Eval(n_in, #either 1 or 3 mostly\n",
    "#                      indim, #Output dimension of encoder\n",
    "#                      numfit, #Number of epochs to fit linear head\n",
    "#                      dls_val, #Used to train linear head\n",
    "#                      dls_test, #Used to evaluate (test) the trained linear head\n",
    "#                      aug_pipelines_val, #Aug pipeline when training linear head (typically just cropping / normalization)\n",
    "#                      encoder): #Encoder\n",
    "    \n",
    "#     model = LinearModel(encoder=encoder,indim=indim)\n",
    "#     #train linear classifier on dls_eval. Requires inputs: encoder, aug_pipeline, dls, \n",
    "#     bt = LinearBt(aug_pipelines_val,show_batch=True,n_in=n_in,print_augs=False)\n",
    "#     learn = Learner(dls_val,model, cbs=[bt])\n",
    "#     learn.fit(numfit)\n",
    "\n",
    "#     #eval linear classifier\n",
    "#     acc = eval_linear(model,dls_test)\n",
    "\n",
    "#     return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire API can be reduced to the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#setup\n",
    "fastai_encoder.eval() #eval mode\n",
    "fastai_encoder = grad_on(fastai_encoder,on=False) #turn off gradients\n",
    "main_linear_eval = Main_Linear_Eval(size=size,n_in=n_in,numfit=1,indim=1024, #size,n_in=3 (color channels),number of epochs to fit linear, and output dimension of encoder\n",
    "                        dls_val=dls_val,dls_test=dls_test, #dls for training linear and evaluating linear\n",
    "                        stats=cifar_stats,\n",
    "                        aug_pipelines_val=aug_pipelines_val, #aug_pipeline for training \n",
    "                        encoder=fastai_encoder #encoder\n",
    "                                    )\n",
    "#main_linear_eval.encoder = self.encoder (usage within BT training)\n",
    "acc=main_linear_eval()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
