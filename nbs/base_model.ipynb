{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_model\n",
    "\n",
    "> In this module we have the base model, learner and other things we need to train encoder\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "import sys\n",
    "import self_supervised\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import kornia.augmentation as korniatfm\n",
    "import torchvision.transforms as tvtfm\n",
    "from base_rbt.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the base functions and classes to train a basic BT-style model. Note that this (mostly) all comes directly from here: `https://github.com/KeremTurgutlu/self_supervised/blob/main/nbs/14%20-%20barlow_twins.ipynb`\n",
    "but we needed to extend some of the functionality for our purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the base classes and functions needed for image augmentation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#My edited version of RandTransform\n",
    "class RandomGaussianBlur(RandTransform):\n",
    "    \"Randomly apply gaussian blur with probability `p` with a value of s\"\n",
    "    order = 11\n",
    "    def __init__(self, \n",
    "                 p=1.0, #debugging (bug in libraries implementation)\n",
    "                 prob=0.5,#the real probability\n",
    "                 s=(8,32), #kernel\n",
    "                 sig=None, #sig_val is either manually input OR\n",
    "                 blur_r=(0.1,2),#is randomly chosen from uniform with these bounds\n",
    "                 same_on_batch=False, \n",
    "                 **kwargs): \n",
    "        \n",
    "        store_attr()\n",
    "        super().__init__(p=p, **kwargs)\n",
    "\n",
    "    def encodes(self, x:TensorImage):\n",
    "        \n",
    "        if isinstance(self.s, int):   s = (self.s,self.s)\n",
    "        elif isinstance(self.s, tuple) or isinstance(self.s, list): s=self.s\n",
    "     \n",
    "        #Default for ImageNet from BYOL / BT papers\n",
    "        if self.sig is None:\n",
    "            sig_val = np.random.uniform(self.blur_r[0],self.blur_r[1])\n",
    "        \n",
    "        else:\n",
    "            sig_val = self.sig\n",
    "            \n",
    "\n",
    "        tfm = korniatfm.RandomGaussianBlur(kernel_size=s,sigma=(sig_val,sig_val),same_on_batch=self.same_on_batch,p=self.prob)\n",
    "        return tfm(x)\n",
    "\n",
    "#Delete later: leaving for backward compatibility for now\n",
    "# class RandomGaussianBlur(RandTransform):\n",
    "#     \"Randomly apply gaussian blur with probability `p` with a value of s\"\n",
    "#     order = 11\n",
    "#     def __init__(self, p=0.5, s=(8,32), same_on_batch=False, **kwargs): \n",
    "#         store_attr()\n",
    "#         super().__init__(p=p, **kwargs)\n",
    "        \n",
    "#     def encodes(self, x:TensorImage):\n",
    "#         if isinstance(self.s, tuple): s = np.random.randint(*self.s)\n",
    "#         if isinstance(self.s, list):  s = np.random.randint(*self.s)\n",
    "#         if isinstance(self.s, int):   s = self.s\n",
    "#         s2 = int(s/4)*2+1\n",
    "#         tfm = korniatfm.RandomGaussianBlur((s2,s2),(s,s),same_on_batch=self.same_on_batch,p=1.) #p=1. is a bug\n",
    "#                                             #kernel #sigma\n",
    "        \n",
    "#         return tfm(x)\n",
    "\n",
    "\n",
    "class RandomCenterDropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.5, min_dropout_size=(20, 20), max_dropout_size=(60, 60), fill_value=0, same_on_batch=False):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.min_dropout_size = min_dropout_size\n",
    "        self.max_dropout_size = max_dropout_size\n",
    "        self.fill_value = fill_value\n",
    "        self.same_on_batch = same_on_batch\n",
    "        #self.id_transform = tvtfm.RandomResizedCrop((256, 256), scale=(1.,1.), ratio=(1.,1.))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Check if the augmentation should be applied to the whole batch or individually\n",
    "\n",
    "        #x=self.id_transform(x)\n",
    "\n",
    "        if self.same_on_batch:\n",
    "            if random.random() < self.p:\n",
    "                return self._apply_dropout(x)\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            # Apply dropout individually with given probability\n",
    "            for i in range(x.size(0)):\n",
    "                tem=random.random()\n",
    "                #print(f\"tem is: {tem}\")\n",
    "                if tem < self.p:\n",
    "                    x[i] = self._apply_dropout(x[i].unsqueeze(0)).squeeze(0)\n",
    "            return x\n",
    "\n",
    "    def _apply_dropout(self, img):\n",
    "        for i,count in enumerate(range(img.size(0))):  # Iterate through batch if necessary\n",
    "            _, h, w = img[i].shape\n",
    "            # Generate random dropout size within specified limits\n",
    "            dh = random.randint(self.min_dropout_size[0], self.max_dropout_size[0])\n",
    "            dw = random.randint(self.min_dropout_size[1], self.max_dropout_size[1])\n",
    "            \n",
    "            x1 = int(max((w - dw) / 2, 0))\n",
    "            y1 = int(max((h - dh) / 2, 0))\n",
    "            \n",
    "            mask = torch.ones_like(img[i])\n",
    "            mask[:, y1:y1+dh, x1:x1+dw] = self.fill_value\n",
    "            img[i] = img[i] * mask\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "def get_BT_batch_augs(size,\n",
    "                    flip=True,crop=True,noise=True,rotate=True,jitter=True,bw=True,blur=True,solar=True,cutout=False, #Whether to use  given aug or not\n",
    "                    resize_scale=(0.08, 1.0),resize_ratio=(3/4, 4/3),noise_std=0.025, rotate_deg=30,jitter_s=.6,blur_s=(4,32),#hps of diff augs\n",
    "                    blur_r=(0.1,2),blur_sig=None,sol_t=0.05,sol_a=0.05,min_dropout_size=(25, 100),max_dropout_size=(50,150), #hps of diff augs\n",
    "                    flip_p=0.5, rotate_p=0.3,noise_p=0.2, jitter_p=0.3, bw_p=0.3, blur_p=0.3,sol_p=0.1,cut_p=0.0, #prob of performing aug\n",
    "                    same_on_batch=False,stats=imagenet_stats,cuda=default_device().type == 'cuda',xtra_tfms=[]\n",
    "                    ):\n",
    "    \"Input batch augmentations implemented in tv+kornia+fastai\"\n",
    "    \n",
    "    tfms = []\n",
    "\n",
    "    korniatfm.RandomHorizontalFlip.order = RandomResizedCrop.order-1\n",
    "\n",
    "\n",
    "    if crop: tfms += [tvtfm.RandomResizedCrop((size, size), scale=resize_scale, ratio=resize_ratio)]\n",
    "    #Unfortunately for some reason this doesn't work, so we can't apply \"same_on_batch=False\"\n",
    "    #tfms += [korniatfm.RandomResizedCrop((size, size), scale=resize_scale, ratio=resize_ratio, same_on_batch=same_on_batch)]\n",
    "\n",
    "    if cutout: tfms+=[RandomCenterDropout(min_dropout_size=min_dropout_size,max_dropout_size=max_dropout_size, fill_value=0, p=cut_p,same_on_batch=same_on_batch)]\n",
    "    \n",
    "  \n",
    "    if flip: tfms += [korniatfm.RandomHorizontalFlip(p=flip_p,same_on_batch=same_on_batch)]\n",
    "\n",
    "    if rotate: tfms += [Rotate(max_deg=rotate_deg, p=rotate_p, batch=same_on_batch)]\n",
    "\n",
    "                                             #brightness,contrast,saturation,hue\n",
    "    if jitter: tfms += [korniatfm.ColorJitter(0.4*jitter_s, 0.4*jitter_s, 0.2*jitter_s, 0.1*jitter_s, p=jitter_p, same_on_batch=same_on_batch)]\n",
    "    \n",
    "    if bw:     tfms += [korniatfm.RandomGrayscale(p=bw_p, same_on_batch=same_on_batch)]\n",
    "        \n",
    "    #sig will usually be None\n",
    "    if blur:   tfms += [RandomGaussianBlur(prob=blur_p, s=blur_s,sig=blur_sig,blur_r=blur_r, same_on_batch=same_on_batch)]\n",
    "\n",
    "    korniatfm.RandomSolarize.order = RandomGaussianBlur.order + 1 #we want to apply solarization after RandomGaussianBlur\n",
    "    \n",
    "    if solar: tfms += [korniatfm.RandomSolarize(p=sol_p,thresholds=sol_t, additions=sol_a,same_on_batch=same_on_batch)]\n",
    "\n",
    "    if noise: tfms+=[korniatfm.RandomGaussianNoise(mean=0.0, std=noise_std, same_on_batch=same_on_batch, p=noise_p)]\n",
    "    \n",
    "    if stats is not None: tfms += [Normalize.from_stats(*stats, cuda=cuda)]\n",
    "\n",
    "    tfms += xtra_tfms\n",
    "\n",
    "    pipe = Pipeline(tfms, split_idx = 0)\n",
    "    return pipe\n",
    "\n",
    "@delegates(get_BT_batch_augs)\n",
    "def get_multi_aug_pipelines(size, **kwargs): return get_BT_batch_augs(size, **kwargs)\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(size=size,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_bt_cifar10_aug_pipelines(size):\n",
    "    aug_pipelines_1 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                                                    bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n",
    "                                                    resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,\n",
    "                                                    bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.5,sol_p=0.0,\n",
    "                                                    stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n",
    "                                                    )\n",
    "\n",
    "    aug_pipelines_2 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                                                    bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n",
    "                                                    resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,sol_t=0.01,sol_a=0.01,\n",
    "                                                    bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.1,sol_p=0.2,\n",
    "                                                    stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n",
    "                                                    )\n",
    "\n",
    "    bt_cifar10_aug_pipelines = [aug_pipelines_1,aug_pipelines_2]\n",
    "\n",
    "    return bt_cifar10_aug_pipelines\n",
    "\n",
    "#Add other augmentations here e.g. BYOL augs\n",
    "IMAGENET_Augs = dict(flip_p1=0.5,flip_p2=0.5,jitter_p1=0.8,jitter_p2=0.8,bw_p1=0.2,\n",
    "                bw_p2=0.2,blur_p1=1.0,blur_p2=0.1,sol_p1=0.0,sol_p2=0.2,noise_p1=0.0,\n",
    "                noise_p2=0.0,cut_p=0,resize_scale=(0.7, 1.0),resize_ratio=(3/4, 4/3),rotate_deg=45.0,\n",
    "                rotate_p=0.5,blur_r=(0.1,2),blur_s=13,sol_t=0.1,sol_a=0.1,noise_std=0.1,min_dropout_size=None,max_dropout_size=None,\n",
    "                )\n",
    "\n",
    "DERMNET_Augs = IMAGENET_Augs.copy()\n",
    "DERMNET_Augs['min_dropout_size']=(50, 185)\n",
    "DERMNET_Augs['max_dropout_size']=(100,190)\n",
    "DERMNET_Augs['cut_p']=0.5\n",
    "\n",
    "def helper_get_bt_augs(size,Augs=IMAGENET_Augs):\n",
    "\n",
    "\n",
    "    aug_pipelines_1 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                        rotate=True,jitter=True,noise=True,bw=True,blur=True,solar=True,cutout=True, #Whether to use aug or not\n",
    "                        resize_scale=Augs['resize_scale'],resize_ratio=Augs['resize_ratio'],\n",
    "                        noise_std=Augs['noise_std'], rotate_deg=Augs['rotate_deg'],\n",
    "                        blur_r=Augs['blur_r'],blur_s=Augs['blur_s'],sol_t=Augs['sol_t'],sol_a=Augs['sol_a'],\n",
    "                        min_dropout_size=Augs['min_dropout_size'],max_dropout_size=Augs['max_dropout_size'],\n",
    "                        flip_p=Augs['flip_p1'], rotate_p=Augs['rotate_p'],noise_p=Augs['noise_p1'],\n",
    "                        jitter_p=Augs['jitter_p1'], bw_p=Augs['bw_p1'], blur_p=Augs['blur_p1'],\n",
    "                        sol_p=Augs['sol_p1'],cut_p=Augs['cut_p'], #prob of performing aug\n",
    "                        same_on_batch=False,stats=None)\n",
    "\n",
    "    aug_pipelines_2 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                        rotate=True,jitter=True,noise=True,bw=True,blur=True,solar=True,cutout=True, #Whether to use aug or not\n",
    "                        resize_scale=Augs['resize_scale'],resize_ratio=Augs['resize_ratio'],\n",
    "                        noise_std=Augs['noise_std'], rotate_deg=Augs['rotate_deg'],\n",
    "                        blur_r=Augs['blur_r'],blur_s=Augs['blur_s'],sol_t=Augs['sol_t'],sol_a=Augs['sol_a'],\n",
    "                        min_dropout_size=Augs['min_dropout_size'],max_dropout_size=Augs['max_dropout_size'],\n",
    "                        flip_p=Augs['flip_p2'], rotate_p=Augs['rotate_p'],noise_p=Augs['noise_p2'],\n",
    "                        jitter_p=Augs['jitter_p2'], bw_p=Augs['bw_p2'], blur_p=Augs['blur_p2'],\n",
    "                        sol_p=Augs['sol_p2'],cut_p=Augs['cut_p'], #prob of performing aug\n",
    "                        same_on_batch=False,stats=None)\n",
    "\n",
    "    aug_pipelines = [aug_pipelines_1,aug_pipelines_2]\n",
    "\n",
    "    return aug_pipelines\n",
    "\n",
    "def get_bt_imagenet_aug_pipelines(size):\n",
    "    return helper_get_bt_augs(size,Augs=IMAGENET_Augs)\n",
    "\n",
    "def get_bt_dermnet_aug_pipelines(size):\n",
    "    return helper_get_bt_augs(size,Augs=DERMNET_Augs)\n",
    "\n",
    "\n",
    "\n",
    "bt_aug_func_dict = {'bt_cifar10_aug_pipelines':get_bt_cifar10_aug_pipelines,'bt_imagenet_aug_pipelines':get_bt_imagenet_aug_pipelines,\n",
    "                   'bt_dermnet_aug_pipelines':get_bt_dermnet_aug_pipelines\n",
    "                   }\n",
    "\n",
    "def get_bt_aug_pipelines(bt_augs,size):\n",
    "\n",
    "    return bt_aug_func_dict[bt_augs](size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ssl_dls(dataset,#cifar10, dermnet, etc\n",
    "            bs,\n",
    "            size,\n",
    "            device,\n",
    "            pct_dataset=1.0):\n",
    "    # Define the base package name in a variable for easy modification\n",
    "\n",
    "    try:\n",
    "        # Construct the module path\n",
    "        module_path = f\"{PACKAGE_NAME}.{dataset}_dataloading\"\n",
    "        \n",
    "        # Dynamically import the module\n",
    "        dataloading_module = importlib.import_module(module_path)\n",
    "    except ModuleNotFoundError:\n",
    "        # Handle the case where the module cannot be found\n",
    "        raise ImportError(f\"Could not find a data loading module for '{dataset}'. \"\n",
    "                          f\"Make sure '{module_path}' exists and is correctly named.\") from None\n",
    "    \n",
    "    # Assuming the function name follows a consistent naming convention\n",
    "    func_name = f\"get_bt_{dataset}_train_dls\"\n",
    "\n",
    "    try:\n",
    "        # Retrieve the data loading function from the module\n",
    "        data_loader_func = getattr(dataloading_module, func_name)\n",
    "    except AttributeError:\n",
    "        # Handle the case where the function does not exist in the module\n",
    "        raise AttributeError(f\"The function '{func_name}' was not found in '{module_path}'. \"\n",
    "                             \"Ensure it is defined and named correctly.\") from None\n",
    "    \n",
    "    # Proceed to call the function with arguments from the config\n",
    "    try:\n",
    "        dls_train = data_loader_func(bs=bs,size=size,device=device,pct_dataset=pct_dataset)\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the function call\n",
    "        raise RuntimeError(f\"An error occurred while calling '{func_name}' from '{module_path}': {e}\") from None\n",
    "    \n",
    "    return dls_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base functions / classes we need to train a BT / RBT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#Base functions / classes we need to train a BT / RBT model.\n",
    "\n",
    "#TODO: We can make these more abstract so can incrementally modify to build `bt/rbt` and also `new idea.` But for \n",
    "#sake of readability, might be easier to just modify the defintions elsewhere. Come back to this later...\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):\n",
    "        self.encoder = encoder\n",
    "        self.projector = projector\n",
    "        \n",
    "    def forward(self,x): \n",
    "        \n",
    "        return self.projector(self.encoder(x))\n",
    "\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines,n_in,lmb,sparsity_level, \n",
    "                model_type='barlow_twins',print_augs=False\n",
    "                 ):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        store_attr('sparsity_level')\n",
    "        self.n_in=n_in\n",
    "        self.model_type = model_type\n",
    "        self.index=-1 #Gets updated after each batch\n",
    "        self.acc_dict = {}\n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.index=-1  \n",
    "  \n",
    "    def before_batch(self):\n",
    "        \n",
    "        #TODO: Make this nicer (possibly can load in data as TensorImage(BW) or something?)\n",
    "        #This is a bit of a hack. Can make this more elegant later. But in new version of FastAI\n",
    "        #seems we need to compute TensorImage(BW) here, and depends on whether color or not, i.e. n_in.\n",
    "        if self.n_in == 1:\n",
    "\n",
    "            xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))\n",
    "            \n",
    "            #print(xi.shape)\n",
    "                                    \n",
    "        elif self.n_in == 3:\n",
    "            \n",
    "            xi,xj = self.aug1(TensorImage(self.x)), self.aug2(TensorImage(self.x))\n",
    "\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "\n",
    "        self.index=self.index+1\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1): \n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:]\n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]]\n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#We want access to both representation and projection\n",
    "\n",
    "#TODO: We can make these more abstract so can incrementally modify to build `bt/rbt` and also `new idea.` But for \n",
    "#sake of readability, might be easier to just modify the defintions elsewhere. Come back to this later...\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):\n",
    "        self.encoder = encoder\n",
    "        self.projector = projector\n",
    "        \n",
    "    def forward(self,x): \n",
    "        tem = self.encoder(x)\n",
    "        return tem,self.projector(tem)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'forward returns tuple of (encoder(x),projector(encoder(x)))'\n",
    "\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    \n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    " \n",
    "    return BarlowTwinsModel(encoder, projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write down standard definition of `lf` for `RAT` method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BarlowTwins needs an `lf` method to work properly. Here we provide the `lf` of standard barlow twins. Later we can\n",
    "patch in a new defintion of `lf` that involves random functions, inner maximization etc. The tools needed to do this are provised in `base_lf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt(pred,I,lmb):\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "    \n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    C = (z1norm.T @ z2norm) / bs \n",
    "    cdiff = (C - I)**2\n",
    "    loss = (cdiff*I + cdiff*(1-I)*lmb).sum() \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_sparse_head(pred,I,lmb,projector,sparsity_level):\n",
    "  \n",
    "    bt_loss = lf_bt(pred,I,lmb)\n",
    "    L21 = torch.linalg.norm(projector[-1].weight, ord=2, dim=0).sum()\n",
    "\n",
    "    # print(f\"bt_loss is {bt_loss}, L21 is {L21}, scaled L21 is {sparsity_level*L21}\")\n",
    "    # print(bt_loss)\n",
    "    # print(L21)\n",
    "\n",
    "    \n",
    "    loss =  bt_loss + sparsity_level*L21 #barlow twins loss + L21 norm of last layer of projector\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_indiv_sparse(pred,I,lmb,sparsity_level,\n",
    "                      ):\n",
    "\n",
    "    pred_enc = pred[0]\n",
    "    pred = pred[1]\n",
    "\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    z1_enc, z2_enc = pred_enc[:bs],pred_enc[bs:]\n",
    "\n",
    "    sparsity = lmb*(0.5*torch.abs(z1_enc) + 0.5*torch.abs(z2_enc)).sum()\n",
    "\n",
    "    C = (z1norm.T @ z2norm) / bs\n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    rr = cdiff*(1-I)*lmb #redundancy reduction term (scaled by lmb)\n",
    "\n",
    "    loss = (cdiff*I + rr).sum() #standard bt loss\n",
    "\n",
    "    loss = loss + sparsity_level*sparsity\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_group_sparse(pred,I,lmb,sparsity_level,\n",
    "                      ):\n",
    "\n",
    "    pred_enc = pred[0]\n",
    "    pred = pred[1]\n",
    "\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    z1_enc, z2_enc = pred_enc[:bs],pred_enc[bs:]\n",
    "\n",
    "    sparsity = lmb * ((0.5 * z1_enc.pow(2) + 0.5 * z2_enc.pow(2)).pow(0.5)).sum()\n",
    "\n",
    "    C = (z1norm.T @ z2norm) / bs\n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    rr = cdiff*(1-I)*lmb #redundancy reduction term (scaled by lmb)\n",
    "\n",
    "    loss = (cdiff*I + rr).sum() #standard bt loss\n",
    "\n",
    "    loss = loss + sparsity_level*sparsity\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_group_norm_sparse(pred,I,lmb,sparsity_level,\n",
    "                      ):\n",
    "\n",
    "    pred_enc = pred[0]\n",
    "    pred = pred[1]\n",
    "\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "    \n",
    "\n",
    "    eps = 1e-7\n",
    "    z1_enc, z2_enc = pred_enc[:bs],pred_enc[bs:]\n",
    "    z1norm_enc = (z1_enc - z1_enc.mean(0)) / (z1_enc.std(0, unbiased=False)+eps)\n",
    "    z2norm_enc = (z2_enc - z2_enc.mean(0)) / (z2_enc.std(0, unbiased=False)+eps)\n",
    "\n",
    "    sparsity = lmb * ((0.5 * z1norm_enc.pow(2) + 0.5 * z2norm_enc.pow(2)).pow(0.5)).sum()\n",
    "\n",
    "    C = (z1norm.T @ z2norm) / bs\n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    rr = cdiff*(1-I)*lmb #redundancy reduction term (scaled by lmb)\n",
    "\n",
    "    loss = (cdiff*I + rr).sum() #standard bt loss\n",
    "\n",
    "    loss = loss + sparsity_level*sparsity\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_fun(pred,I,lmb,sparsity_level,\n",
    "                      ):\n",
    "\n",
    "    pred_enc = pred[0]\n",
    "    pred = pred[1]\n",
    "\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "    \n",
    "\n",
    "    eps = 1e-7\n",
    "    z1_enc, z2_enc = pred_enc[:bs],pred_enc[bs:]\n",
    "    z1norm_enc = (z1_enc - z1_enc.mean(0)) / (z1_enc.std(0, unbiased=False)+eps)\n",
    "    z2norm_enc = (z2_enc - z2_enc.mean(0)) / (z2_enc.std(0, unbiased=False)+eps)\n",
    "\n",
    "    sparsity = lmb * ((0.5 * z1norm_enc.pow(2) + 0.5 * z2norm_enc.pow(2)).pow(0.5)).sum()\n",
    "\n",
    "    C = (z1norm_enc.T @ z2norm_enc) / bs\n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    loss = (cdiff*I).sum() #standard bt loss\n",
    "\n",
    "    print(f\"invariance loss is: {loss} and sparsity loss is: {sparsity}\")\n",
    "\n",
    "    loss = loss + sparsity_level*sparsity\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lf_bt_proj_group_sparse(pred,I,lmb,sparsity_level,\n",
    "                           ):\n",
    "\n",
    "    pred_enc = pred[0]\n",
    "    pred = pred[1]\n",
    "\n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    sparsity = lmb * ((0.5 * z1norm.pow(2) + 0.5 * z2norm.pow(2)).pow(0.5)).sum()\n",
    "\n",
    "    C = (z1norm.T @ z2norm) / bs\n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    rr = cdiff*(1-I)*lmb #redundancy reduction term (scaled by lmb)\n",
    "\n",
    "    loss = (cdiff*I + rr).sum() #standard bt loss\n",
    "\n",
    "    loss = loss + sparsity_level*sparsity\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb):\n",
    "    \"Assumes model created according to type p3\"\n",
    "\n",
    "    if self.model_type=='barlow_twins':\n",
    "         pred_enc = pred[0]\n",
    "         pred = pred[1]\n",
    "         return lf_bt(pred, self.I,self.lmb)\n",
    "\n",
    "    elif self.model_type=='sparse_head_barlow_twins':\n",
    "        pred_enc = pred[0]\n",
    "        pred = pred[1]\n",
    "\n",
    "        return lf_bt_sparse_head(pred, self.I,lmb=self.lmb,projector=self.learn.model.projector,sparsity_level=self.sparsity_level)\n",
    "    \n",
    "\n",
    "    elif self.model_type=='indiv_sparse_barlow_twins':\n",
    "        return lf_bt_indiv_sparse(pred, self.I,lmb=self.lmb,sparsity_level=self.sparsity_level)\n",
    "    \n",
    "    elif self.model_type=='group_sparse_barlow_twins':\n",
    "        return lf_bt_group_sparse(pred, self.I,lmb=self.lmb,sparsity_level=self.sparsity_level)\n",
    "\n",
    "    elif self.model_type=='group_norm_sparse_barlow_twins':\n",
    "        return lf_bt_group_norm_sparse(pred, self.I,lmb=self.lmb,sparsity_level=self.sparsity_level)\n",
    "\n",
    "    elif self.model_type=='proj_group_sparse_barlow_twins':\n",
    "        return lf_bt_proj_group_sparse(pred, self.I,lmb=self.lmb,sparsity_level=self.sparsity_level)\n",
    "    \n",
    "    elif self.model_type=='fun':\n",
    "        return lf_bt_fun(pred, self.I,lmb=self.lmb,sparsity_level=self.sparsity_level)\n",
    "\n",
    "\n",
    "    else: raise(Exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def my_splitter_bt(m):\n",
    "    return L(sequential(*m.encoder),m.projector).map(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to use the above functions in an end to end fashion. First we get some data and plonk it into a dls, Then create an encoder, an augmentation pipeline, a learner, then fit\n",
    "the learner. This is the complete process of training BT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_bt_batch(dls,n_in,aug,n=2,print_augs=True):\n",
    "    \"Given a linear learner, show a batch\"\n",
    "        \n",
    "    learn = Learner(dls,model=None, cbs=[BarlowTwins(aug,n_in=n_in,lmb=None,sparsity_level=None,\n",
    "                                                     print_augs=print_augs\n",
    "                                        )])\n",
    "    b = dls.one_batch()\n",
    "    learn._split(b)\n",
    "    learn('before_batch')\n",
    "    axes = learn.barlow_twins.show(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SaveBarlowLearnerCheckpoint(Callback):\n",
    "    \"Save such that can resume training \"\n",
    "    def __init__(self, experiment_dir,start_epoch=0, save_interval=250,with_opt=True):\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.start_epoch = start_epoch\n",
    "        self.save_interval = save_interval\n",
    "        self.with_opt = with_opt  # Decide whether to save optimizer state as well.\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if (self.epoch+1) % self.save_interval == 0 and self.epoch>=self.start_epoch:\n",
    "            print(f\"Saving model and learner state at epoch {self.epoch}\")\n",
    "   \n",
    "            checkpoint_filename = f\"learner_checkpoint_epoch_{self.epoch}\"\n",
    "            checkpoint_path = os.path.join(self.experiment_dir, checkpoint_filename)\n",
    "            # Save the entire learner object, including the model's parameters and optimizer state.\n",
    "            self.learn.save(checkpoint_path, with_opt=self.with_opt)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "class SaveBarlowLearnerModel(Callback):\n",
    "    def __init__(self, experiment_dir):\n",
    "        self.experiment_dir = experiment_dir\n",
    "\n",
    "    def after_fit(self):\n",
    "        model_filename = f\"trained_model_epoch_{self.epoch}.pth\"\n",
    "        model_path = os.path.join(self.experiment_dir, model_filename)\n",
    "        torch.save(self.learn.model.state_dict(), model_path)\n",
    "        print(f\"Model state dict saved to {model_path}\")\n",
    "\n",
    "        encoder_filename = f\"trained_encoder_epoch_{self.epoch}.pth\"\n",
    "        encoder_path = os.path.join(self.experiment_dir, encoder_filename)\n",
    "        torch.save(self.learn.model.encoder.state_dict(), encoder_path)\n",
    "        print(f\"encoder state dict saved to {encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_barlow_model(arch,ps,hs,path):\n",
    "\n",
    "    encoder = resnet_arch_to_encoder(arch=arch, weight_type='random')\n",
    "    model = create_barlow_twins_model(encoder, hidden_size=hs, projection_size=ps)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BarlowTrainer:\n",
    "    \"Setup a learner for training a BT model. Can do transfer learning, normal training, or resume training.\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,#An encoder followed by a projector\n",
    "                 dls,\n",
    "                 bt_aug_pipelines,\n",
    "                 lmb,\n",
    "                 sparsity_level,\n",
    "                 n_in,\n",
    "                 model_type,\n",
    "                 wd,\n",
    "                 device,\n",
    "                 num_it=100, #Number of iterations to run lr_find for.\n",
    "                 load_learner_path=None, #Path to load learner from (optional)\n",
    "                 experiment_dir=None, #Where to save model checkpoints (optional)\n",
    "                 start_epoch=0, #Which epoch to start from\n",
    "                 save_interval=None, #How often to save model checkpoints (optional). \n",
    "                 export=False,\n",
    "                 ):\n",
    "\n",
    "        store_attr()\n",
    "        self.learn = self.setup_learn()\n",
    "\n",
    "    \n",
    "    def setup_learn(self):\n",
    "        \"\"\"\n",
    "        Sets up the learner with the model, callbacks, and metrics.\n",
    "\n",
    "        Returns:\n",
    "        - learn: The Learner object.\n",
    "        \"\"\"\n",
    "      \n",
    "        self.model.to(self.device)\n",
    "\n",
    "        cbs = [BarlowTwins(self.bt_aug_pipelines,n_in=self.n_in,lmb=self.lmb,\n",
    "                           sparsity_level=self.sparsity_level,print_augs=False,\n",
    "                           model_type=self.model_type\n",
    "                           )\n",
    "              ]\n",
    "\n",
    "        learn=Learner(self.dls,self.model,splitter=my_splitter_bt,wd=self.wd, cbs=cbs\n",
    "                     )\n",
    "        \n",
    "        if self.load_learner_path: learn.load(self.load_learner_path,with_opt=True)\n",
    "\n",
    "        return learn\n",
    "    \n",
    "    def _get_training_cbs(self,interrupt_epoch):\n",
    "        \"Add train-time cbs to learner. Note e.g. we don't want these in operation when we're doing lr_find.\"\n",
    "\n",
    "        \n",
    "        cbs=[InterruptCallback(interrupt_epoch)]\n",
    "        \n",
    "        if self.experiment_dir:\n",
    "            cbs.append(SaveBarlowLearnerCheckpoint(experiment_dir=self.experiment_dir,\n",
    "                                             start_epoch = self.start_epoch,\n",
    "                                             save_interval=self.save_interval,\n",
    "                                             )\n",
    "                      )\n",
    "        \n",
    "        if self.export:\n",
    "            cbs.append(SaveBarlowLearnerModel(experiment_dir=self.experiment_dir))\n",
    "   \n",
    "        return cbs\n",
    "                \n",
    "    \n",
    "    def bt_transfer_learning(self,freeze_epochs:int,epochs:int,interrupt_epoch:int):\n",
    "        \"\"\"If the encoder is already pretrained, we can do transfer learning.\n",
    "            Freeze encoder, train projector for a few epochs, then unfreeze and train all. \n",
    "        \"\"\"\n",
    "\n",
    "        self.learn.freeze()\n",
    "        test_grad_off(self.learn.encoder)\n",
    "        #self.learn.fit(freeze_epochs)\n",
    "        self.learn.fit_one_cycle(freeze_epochs, 2e-3, pct_start=0.99) #lifted from fastai `fine_tune`\n",
    "        self.learn.unfreeze()\n",
    "        test_grad_on(self.learn.model)\n",
    "        lrs = self.learn.lr_find(num_it=self.num_it) #lets find a good maximum lr\n",
    "        lr=lrs.valley\n",
    "        discriminative_lrs = slice(lr/10,lr)\n",
    "        self.learn.fit_one_cycle(epochs, discriminative_lrs, pct_start=0.3, div=5.0, cbs=self._get_training_cbs(interrupt_epoch))\n",
    "\n",
    "    def bt_learning(self,epochs:int,interrupt_epoch:int):\n",
    "        \"\"\"If the encoder is not pretrained, we can do normal training.\n",
    "        \"\"\"\n",
    "\n",
    "        lrs = self.learn.lr_find(num_it=self.num_it)\n",
    "\n",
    "        self.learn.fit_one_cycle(epochs, lrs.valley,cbs=self._get_training_cbs(interrupt_epoch))\n",
    "    \n",
    "    def continue_bt_learning(self,epochs:int,start_epoch:int,interrupt_epoch:int):\n",
    "        \"\"\"Resume training with `fit_one_cycle` after loading a learner.\n",
    "        \"\"\"\n",
    "        \n",
    "        test_ne(self.load_learner_path,None)\n",
    "\n",
    "        self.learn.fit_one_cycle(epochs,start_epoch=start_epoch,cbs=self._get_training_cbs(interrupt_epoch))\n",
    "\n",
    "    def train(self,learn_type, freeze_epochs:int,epochs:int,start_epoch:int,interrupt_epoch:int):\n",
    "        \"\"\"Train model using BT\n",
    "        \"\"\"\n",
    "        if learn_type == 'transfer_learning':\n",
    "            \n",
    "            self.bt_transfer_learning(freeze_epochs=freeze_epochs,epochs=epochs,interrupt_epoch=interrupt_epoch)\n",
    "\n",
    "        elif learn_type=='continue_learning':\n",
    "            self.continue_bt_learning(epochs=epochs,start_epoch=start_epoch,interrupt_epoch=interrupt_epoch)\n",
    "        \n",
    "        elif learn_type=='standard':\n",
    "            self.bt_learning(epochs=epochs,interrupt_epoch=interrupt_epoch)\n",
    "\n",
    "        else: raise Exception(\"Invalid weight_type\")\n",
    "\n",
    "        return self.learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def main_bt_train(config,\n",
    "        start_epoch = 0,\n",
    "        interrupt_epoch = 100,\n",
    "        load_learner_path=None,\n",
    "        learn_type = 'standard', #can be 'standard', 'transfer_learning', or 'continue_learning'\n",
    "        experiment_dir=None,\n",
    "        ):\n",
    "    \"Basically map from config to training a BT model. Optionally save checkpoints of learner, to reload and continue;\"\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize the device for model training (CUDA or CPU)\n",
    "    device = default_device()\n",
    "\n",
    "    # Construct the model based on the configuration\n",
    "    # This involves selecting the architecture and setting model-specific hyperparameters.\n",
    "    encoder = resnet_arch_to_encoder(arch=config.arch, weight_type=config.weight_type)\n",
    "    \n",
    "    model = create_barlow_twins_model(encoder, hidden_size=config.hs, projection_size=config.ps)\n",
    "\n",
    "    # Prepare data loaders according to the dataset specified in the configuration\n",
    "    dls = get_ssl_dls(dataset=config.dataset, bs=config.bs,size=config.size, device=device,pct_dataset=config.pct_dataset)\n",
    "\n",
    "    # Set up data augmentation pipelines as specified in the configuration\n",
    "    bt_aug_pipelines = get_bt_aug_pipelines(bt_augs=config.bt_augs, size=config.size)\n",
    "\n",
    "    # Train the model with the specified configurations and save `learn` checkpoints\n",
    "\n",
    "    if experiment_dir and config.epochs == interrupt_epoch:\n",
    "        export=True\n",
    "    else:\n",
    "        export=False\n",
    "\n",
    "    #Setup the bt trainer. basically a `Learner` with a few extra bells and whistles\n",
    "    bt_trainer = BarlowTrainer(model=model,\n",
    "                    dls=dls,\n",
    "                    bt_aug_pipelines=bt_aug_pipelines,\n",
    "                    lmb=config.lmb,\n",
    "                    sparsity_level=config.sparsity_level,\n",
    "                    n_in=config.n_in,\n",
    "                    model_type=config.model_type,\n",
    "                    wd=config.wd,\n",
    "                    num_it=config.num_it,\n",
    "                    device=device,\n",
    "                    load_learner_path=load_learner_path,\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    start_epoch=start_epoch,\n",
    "                    save_interval=config.save_interval,\n",
    "                    export=export\n",
    "                                    )\n",
    "\n",
    "    # Train the model with the specified configurations and save `learn` checkpoints\n",
    "    learn = bt_trainer.train(learn_type=learn_type,freeze_epochs=config.freeze_epochs,epochs=config.epochs,start_epoch=start_epoch,interrupt_epoch=interrupt_epoch)\n",
    "    return learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = '../configs/cifar10/bt_test_config.yaml'\n",
    "# config = load_config(config_path)\n",
    "\n",
    "# load_learner_path=None\n",
    "# experiment_dir=None\n",
    "# start_epoch=0\n",
    "\n",
    "\n",
    "# # Initialize the device for model training (CUDA or CPU)\n",
    "# device = default_device()\n",
    "\n",
    "# # Construct the model based on the configuration\n",
    "# # This involves selecting the architecture and setting model-specific hyperparameters.\n",
    "# encoder = resnet_arch_to_encoder(arch=config.arch, weight_type=config.weight_type)\n",
    "\n",
    "# model = create_barlow_twins_model(encoder, hidden_size=config.hs, projection_size=config.ps)\n",
    "\n",
    "# # Prepare data loaders according to the dataset specified in the configuration\n",
    "# dls = get_ssl_dls(dataset=config.dataset, bs=config.bs,size=config.size, device=device,pct_dataset=config.pct_dataset)\n",
    "\n",
    "# # Set up data augmentation pipelines as specified in the configuration\n",
    "# bt_aug_pipelines = get_bt_aug_pipelines(bt_augs=config.bt_augs, size=config.size)\n",
    "\n",
    "# # Train the model with the specified configurations and save `learn` checkpoints\n",
    "\n",
    "# #Setup the bt trainer. basically a `Learner` with a few extra bells and whistles\n",
    "# bt_trainer = BarlowTrainer(model=model,\n",
    "#                dls=dls,\n",
    "#                bt_aug_pipelines=bt_aug_pipelines,\n",
    "#                lmb=config.lmb,\n",
    "#                sparsity_level=config.sparsity_level,\n",
    "#                n_in=config.n_in,\n",
    "#                model_type=config.model_type,\n",
    "#                wd=config.wd,\n",
    "#                num_it=config.num_it,\n",
    "#                device=device,\n",
    "#                load_learner_path=load_learner_path,\n",
    "#                experiment_dir=experiment_dir,\n",
    "#                start_epoch=start_epoch,\n",
    "#                save_interval=config.save_interval\n",
    "#                               )\n",
    "\n",
    "\n",
    "# bt_trainer.learn.lr_find??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_bt_experiment_state(config,base_dir):\n",
    "    \"\"\"Get the load_learner_path, learn_type, start_epoch, interrupt_epoch for BT experiment.\n",
    "       Basically this tells us how to continue learning (e.g. we have run two sessions for \n",
    "       100 epochs, and want to continue for another 100 epochs). Return values are\n",
    "       None if we are starting from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    load_learner_path, _  = get_highest_num_path(base_dir, config)\n",
    "    #TODO:\n",
    "    #We can get start_epoch, interrupt epoch from `get_highest_epoch_path` + save_interval (may be None!)\n",
    "    start_epoch=0 if load_learner_path is None else int(load_learner_path.split('_')[-1])+1\n",
    "    \n",
    "    if start_epoch >= config.epochs:\n",
    "        print(f\"start_epoch={start_epoch}, but already completed {config.epochs} epochs. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    interrupt_epoch = start_epoch + config.save_interval\n",
    "\n",
    "    #We can also get the learn_type from the load_learner_path + weight_type. \n",
    "    \n",
    "    if config.weight_type == 'random':\n",
    "        learn_type = 'standard'\n",
    "    \n",
    "    elif 'pretrained' in config.weight_type:\n",
    "        learn_type = 'transfer_learning'\n",
    "\n",
    "    learn_type = learn_type if load_learner_path is None else 'continue_learning'\n",
    "\n",
    "    return load_learner_path, learn_type, start_epoch, interrupt_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def main_bt_experiment(config,\n",
    "                      base_dir,\n",
    "                      ):\n",
    "    \"\"\"Run several epochs of the experiment as defined in the config and where we are up to. e.g. epoch 0, or resuming\n",
    "    at epoch 99 etc. Basically a stateful version of `main_bt_train` that can be resumed. And saving.\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    experiment_dir, experiment_hash,git_commit_hash = setup_experiment(config,base_dir)\n",
    "\n",
    "    load_learner_path, learn_type, start_epoch, interrupt_epoch = get_bt_experiment_state(config,base_dir)\n",
    "\n",
    "    main_bt_train(config=config,\n",
    "            start_epoch=start_epoch,\n",
    "            interrupt_epoch=interrupt_epoch,\n",
    "            load_learner_path=load_learner_path,\n",
    "            learn_type=learn_type,\n",
    "            experiment_dir=experiment_dir,\n",
    "            )\n",
    "\n",
    "    # Save a metadata file in the experiment directory with the Git commit hash and other details\n",
    "    save_metadata_file(experiment_dir=experiment_dir, git_commit_hash=git_commit_hash)\n",
    "\n",
    "    # After experiment execution and all processing are complete\n",
    "    update_experiment_index(base_dir,{\n",
    "        \"experiment_hash\": experiment_hash,  # Unique identifier derived from the experiment's configuration\n",
    "        \"experiment_dir\": experiment_dir,  # Absolute path to the experiment's dedicated directory\n",
    "        \"git_commit_hash\": git_commit_hash,  # Git commit hash for the code version used in the experiment\n",
    "        # Potentially include additional details collected during or after the experiment, such as:\n",
    "        # Any other metadata or results summary that is relevant to the experiment\n",
    "                            })\n",
    "    \n",
    "    return experiment_dir,experiment_hash #Return the experiment_dir so we can easily access the results of the experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment_dir is: /var/folders/95/qkdymsl93lz1tvnqky1vn_p00000gn/T/tmpll4p6dsb/SSL/cifar10/smallres/a0f05f92 and the experiment hash is: a0f05f92\n",
      "Configuration saved to /var/folders/95/qkdymsl93lz1tvnqky1vn_p00000gn/T/tmpll4p6dsb/SSL/cifar10/smallres/a0f05f92/config.yaml\n",
      "The git hash is: a3956db8ea770a9ae1a033fd1ef90b22de8227bc\n",
      "Looking in /var/folders/95/qkdymsl93lz1tvnqky1vn_p00000gn/T/tmpll4p6dsb/SSL/cifar10/smallres/a0f05f92 for highest num saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bt_loss is 191.54434204101562, L21 is 1026.555419921875, scaled L21 is 10265.5546875\n",
      "TensorImage(191.5443, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5554, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bt_loss is 207.4569091796875, L21 is 1026.552734375, scaled L21 is 10265.52734375\n",
      "TensorImage(207.4569, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5527, grad_fn=<SumBackward0>)\n",
      "bt_loss is 314.7467041015625, L21 is 1026.536376953125, scaled L21 is 10265.36328125\n",
      "TensorImage(314.7467, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5364, grad_fn=<SumBackward0>)\n",
      "bt_loss is 243.16525268554688, L21 is 1026.433349609375, scaled L21 is 10264.333984375\n",
      "TensorImage(243.1653, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.4333, grad_fn=<SumBackward0>)\n",
      "bt_loss is 233.44761657714844, L21 is 1025.783447265625, scaled L21 is 10257.833984375\n",
      "TensorImage(233.4476, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.7834, grad_fn=<SumBackward0>)\n",
      "bt_loss is 253.50491333007812, L21 is 1021.6915893554688, scaled L21 is 10216.916015625\n",
      "TensorImage(253.5049, grad_fn=<AliasBackward0>)\n",
      "tensor(1021.6916, grad_fn=<SumBackward0>)\n",
      "bt_loss is 203.5117645263672, L21 is 996.17724609375, scaled L21 is 9961.7724609375\n",
      "TensorImage(203.5118, grad_fn=<AliasBackward0>)\n",
      "tensor(996.1772, grad_fn=<SumBackward0>)\n",
      "bt_loss is 153.697509765625, L21 is 846.148681640625, scaled L21 is 8461.486328125\n",
      "TensorImage(153.6975, grad_fn=<AliasBackward0>)\n",
      "tensor(846.1487, grad_fn=<SumBackward0>)\n",
      "bt_loss is 163.50088500976562, L21 is 323.56072998046875, scaled L21 is 3235.607421875\n",
      "TensorImage(163.5009, grad_fn=<AliasBackward0>)\n",
      "tensor(323.5607, grad_fn=<SumBackward0>)\n",
      "bt_loss is 179.24957275390625, L21 is 8650.3564453125, scaled L21 is 86503.5625\n",
      "TensorImage(179.2496, grad_fn=<AliasBackward0>)\n",
      "tensor(8650.3564, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='40' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.00% [40/100 00:53&lt;01:20]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10528.891602</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10541.185547</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10529.244141</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10526.166016</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10538.785156</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10527.768555</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10526.290039</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10523.604492</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10518.746094</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10522.143555</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10517.860352</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10519.826172</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10516.063477</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>10518.006836</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>10517.058594</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>10515.452148</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10514.320312</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10513.753906</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>10516.236328</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>10514.113281</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10515.955078</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>10512.570312</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>10509.685547</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>10507.909180</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>10505.562500</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>10506.193359</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>10506.626953</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>10506.203125</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>10505.510742</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>10505.447266</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10507.955078</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>10507.332031</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>10506.542969</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>10507.415039</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>10507.479492</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>10506.430664</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>10507.310547</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>10506.566406</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>10506.243164</td>\n",
       "      <td>None</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>10504.218750</td>\n",
       "      <td>None</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bt_loss is 263.3471374511719, L21 is 1026.555419921875, scaled L21 is 10265.5546875\n",
      "TensorImage(263.3471, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5554, grad_fn=<SumBackward0>)\n",
      "bt_loss is 287.7316589355469, L21 is 1026.55126953125, scaled L21 is 10265.5126953125\n",
      "TensorImage(287.7317, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5513, grad_fn=<SumBackward0>)\n",
      "bt_loss is 240.61471557617188, L21 is 1026.5467529296875, scaled L21 is 10265.4677734375\n",
      "TensorImage(240.6147, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5468, grad_fn=<SumBackward0>)\n",
      "bt_loss is 251.89401245117188, L21 is 1026.541259765625, scaled L21 is 10265.412109375\n",
      "TensorImage(251.8940, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5413, grad_fn=<SumBackward0>)\n",
      "bt_loss is 321.45654296875, L21 is 1026.5338134765625, scaled L21 is 10265.337890625\n",
      "TensorImage(321.4565, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5338, grad_fn=<SumBackward0>)\n",
      "bt_loss is 210.67214965820312, L21 is 1026.524169921875, scaled L21 is 10265.2421875\n",
      "TensorImage(210.6721, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5242, grad_fn=<SumBackward0>)\n",
      "bt_loss is 252.9123992919922, L21 is 1026.5118408203125, scaled L21 is 10265.1181640625\n",
      "TensorImage(252.9124, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.5118, grad_fn=<SumBackward0>)\n",
      "bt_loss is 241.29986572265625, L21 is 1026.496337890625, scaled L21 is 10264.962890625\n",
      "TensorImage(241.2999, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.4963, grad_fn=<SumBackward0>)\n",
      "bt_loss is 218.45028686523438, L21 is 1026.477294921875, scaled L21 is 10264.7734375\n",
      "TensorImage(218.4503, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.4773, grad_fn=<SumBackward0>)\n",
      "bt_loss is 285.28692626953125, L21 is 1026.4544677734375, scaled L21 is 10264.544921875\n",
      "TensorImage(285.2869, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.4545, grad_fn=<SumBackward0>)\n",
      "bt_loss is 215.20281982421875, L21 is 1026.4276123046875, scaled L21 is 10264.2763671875\n",
      "TensorImage(215.2028, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.4276, grad_fn=<SumBackward0>)\n",
      "bt_loss is 275.06304931640625, L21 is 1026.396240234375, scaled L21 is 10263.962890625\n",
      "TensorImage(275.0630, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.3962, grad_fn=<SumBackward0>)\n",
      "bt_loss is 212.76870727539062, L21 is 1026.3603515625, scaled L21 is 10263.603515625\n",
      "TensorImage(212.7687, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.3604, grad_fn=<SumBackward0>)\n",
      "bt_loss is 276.8118896484375, L21 is 1026.3193359375, scaled L21 is 10263.193359375\n",
      "TensorImage(276.8119, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.3193, grad_fn=<SumBackward0>)\n",
      "bt_loss is 242.88040161132812, L21 is 1026.273193359375, scaled L21 is 10262.732421875\n",
      "TensorImage(242.8804, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.2732, grad_fn=<SumBackward0>)\n",
      "bt_loss is 232.68048095703125, L21 is 1026.2215576171875, scaled L21 is 10262.2158203125\n",
      "TensorImage(232.6805, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.2216, grad_fn=<SumBackward0>)\n",
      "bt_loss is 237.36294555664062, L21 is 1026.1644287109375, scaled L21 is 10261.64453125\n",
      "TensorImage(237.3629, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.1644, grad_fn=<SumBackward0>)\n",
      "bt_loss is 244.67938232421875, L21 is 1026.1015625, scaled L21 is 10261.015625\n",
      "TensorImage(244.6794, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.1016, grad_fn=<SumBackward0>)\n",
      "bt_loss is 292.9844055175781, L21 is 1026.033203125, scaled L21 is 10260.33203125\n",
      "TensorImage(292.9844, grad_fn=<AliasBackward0>)\n",
      "tensor(1026.0332, grad_fn=<SumBackward0>)\n",
      "bt_loss is 221.38311767578125, L21 is 1025.9593505859375, scaled L21 is 10259.59375\n",
      "TensorImage(221.3831, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.9594, grad_fn=<SumBackward0>)\n",
      "bt_loss is 287.14739990234375, L21 is 1025.88037109375, scaled L21 is 10258.8037109375\n",
      "TensorImage(287.1474, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.8804, grad_fn=<SumBackward0>)\n",
      "bt_loss is 197.26641845703125, L21 is 1025.7967529296875, scaled L21 is 10257.9677734375\n",
      "TensorImage(197.2664, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.7968, grad_fn=<SumBackward0>)\n",
      "bt_loss is 201.88058471679688, L21 is 1025.708984375, scaled L21 is 10257.08984375\n",
      "TensorImage(201.8806, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.7090, grad_fn=<SumBackward0>)\n",
      "bt_loss is 219.40118408203125, L21 is 1025.617431640625, scaled L21 is 10256.173828125\n",
      "TensorImage(219.4012, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.6174, grad_fn=<SumBackward0>)\n",
      "bt_loss is 206.16452026367188, L21 is 1025.523193359375, scaled L21 is 10255.232421875\n",
      "TensorImage(206.1645, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.5232, grad_fn=<SumBackward0>)\n",
      "bt_loss is 264.1701965332031, L21 is 1025.4266357421875, scaled L21 is 10254.2666015625\n",
      "TensorImage(264.1702, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.4266, grad_fn=<SumBackward0>)\n",
      "bt_loss is 262.03143310546875, L21 is 1025.3289794921875, scaled L21 is 10253.2900390625\n",
      "TensorImage(262.0314, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.3290, grad_fn=<SumBackward0>)\n",
      "bt_loss is 245.18568420410156, L21 is 1025.230712890625, scaled L21 is 10252.306640625\n",
      "TensorImage(245.1857, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.2307, grad_fn=<SumBackward0>)\n",
      "bt_loss is 239.51983642578125, L21 is 1025.1318359375, scaled L21 is 10251.318359375\n",
      "TensorImage(239.5198, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.1318, grad_fn=<SumBackward0>)\n",
      "bt_loss is 253.75198364257812, L21 is 1025.03271484375, scaled L21 is 10250.3271484375\n",
      "TensorImage(253.7520, grad_fn=<AliasBackward0>)\n",
      "tensor(1025.0327, grad_fn=<SumBackward0>)\n",
      "bt_loss is 314.50323486328125, L21 is 1024.933349609375, scaled L21 is 10249.333984375\n",
      "TensorImage(314.5032, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.9333, grad_fn=<SumBackward0>)\n",
      "bt_loss is 244.7708282470703, L21 is 1024.833984375, scaled L21 is 10248.33984375\n",
      "TensorImage(244.7708, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.8340, grad_fn=<SumBackward0>)\n",
      "bt_loss is 240.80584716796875, L21 is 1024.73486328125, scaled L21 is 10247.3486328125\n",
      "TensorImage(240.8058, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.7349, grad_fn=<SumBackward0>)\n",
      "bt_loss is 281.83599853515625, L21 is 1024.635986328125, scaled L21 is 10246.359375\n",
      "TensorImage(281.8360, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.6360, grad_fn=<SumBackward0>)\n",
      "bt_loss is 263.6956481933594, L21 is 1024.53759765625, scaled L21 is 10245.3759765625\n",
      "TensorImage(263.6956, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.5376, grad_fn=<SumBackward0>)\n",
      "bt_loss is 235.9652557373047, L21 is 1024.439697265625, scaled L21 is 10244.396484375\n",
      "TensorImage(235.9653, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.4397, grad_fn=<SumBackward0>)\n",
      "bt_loss is 286.16510009765625, L21 is 1024.342529296875, scaled L21 is 10243.42578125\n",
      "TensorImage(286.1651, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.3425, grad_fn=<SumBackward0>)\n",
      "bt_loss is 244.91683959960938, L21 is 1024.24609375, scaled L21 is 10242.4609375\n",
      "TensorImage(244.9168, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.2461, grad_fn=<SumBackward0>)\n",
      "bt_loss is 256.274658203125, L21 is 1024.150634765625, scaled L21 is 10241.505859375\n",
      "TensorImage(256.2747, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.1506, grad_fn=<SumBackward0>)\n",
      "bt_loss is 209.58306884765625, L21 is 1024.05615234375, scaled L21 is 10240.5615234375\n",
      "TensorImage(209.5831, grad_fn=<AliasBackward0>)\n",
      "tensor(1024.0562, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m config\u001b[39m.\u001b[39mepochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m config\u001b[39m.\u001b[39msave_interval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m experiment_dir,experiment_hash \u001b[39m=\u001b[39m main_bt_experiment(config,base_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m foo\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mlistdir(experiment_dir))\n",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m experiment_dir, experiment_hash,git_commit_hash \u001b[39m=\u001b[39m setup_experiment(config,base_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m load_learner_path, learn_type, start_epoch, interrupt_epoch \u001b[39m=\u001b[39m get_bt_experiment_state(config,base_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m main_bt_train(config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         start_epoch\u001b[39m=\u001b[39;49mstart_epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         interrupt_epoch\u001b[39m=\u001b[39;49minterrupt_epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         load_learner_path\u001b[39m=\u001b[39;49mload_learner_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         learn_type\u001b[39m=\u001b[39;49mlearn_type,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         experiment_dir\u001b[39m=\u001b[39;49mexperiment_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Save a metadata file in the experiment directory with the Git commit hash and other details\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m save_metadata_file(experiment_dir\u001b[39m=\u001b[39mexperiment_dir, git_commit_hash\u001b[39m=\u001b[39mgit_commit_hash)\n",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m bt_trainer \u001b[39m=\u001b[39m BarlowTrainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                 dls\u001b[39m=\u001b[39mdls,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                 bt_aug_pipelines\u001b[39m=\u001b[39mbt_aug_pipelines,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                 export\u001b[39m=\u001b[39mexport\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Train the model with the specified configurations and save `learn` checkpoints\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m learn \u001b[39m=\u001b[39m bt_trainer\u001b[39m.\u001b[39;49mtrain(learn_type\u001b[39m=\u001b[39;49mlearn_type,freeze_epochs\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mfreeze_epochs,epochs\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mepochs,start_epoch\u001b[39m=\u001b[39;49mstart_epoch,interrupt_epoch\u001b[39m=\u001b[39;49minterrupt_epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mreturn\u001b[39;00m learn\n",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinue_bt_learning(epochs\u001b[39m=\u001b[39mepochs,start_epoch\u001b[39m=\u001b[39mstart_epoch,interrupt_epoch\u001b[39m=\u001b[39minterrupt_epoch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39melif\u001b[39;00m learn_type\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbt_learning(epochs\u001b[39m=\u001b[39;49mepochs,interrupt_epoch\u001b[39m=\u001b[39;49minterrupt_epoch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn\n",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m\"\"\"If the encoder is not pretrained, we can do normal training.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m lrs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn\u001b[39m.\u001b[39mlr_find(num_it\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_it)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn\u001b[39m.\u001b[39;49mfit_one_cycle(epochs, lrs\u001b[39m.\u001b[39;49mvalley,cbs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_training_cbs(interrupt_epoch))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:256\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:245\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:231\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:227\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    225\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 227\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_one_batch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxb)\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x): \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamishhaggerty/base_rbt/nbs/base_model.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tem,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojector(tem)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:2435\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies Batch Normalization for each channel across a batch of data.\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m \n\u001b[1;32m   2431\u001b[0m \u001b[39mSee :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\u001b[39;00m\n\u001b[1;32m   2432\u001b[0m \u001b[39m:class:`~torch.nn.BatchNorm3d` for details.\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2434\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias):\n\u001b[0;32m-> 2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39;49m, running_mean, running_var, weight, bias),\n\u001b[1;32m   2438\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2439\u001b[0m         running_mean,\n\u001b[1;32m   2440\u001b[0m         running_var,\n\u001b[1;32m   2441\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   2442\u001b[0m         bias\u001b[39m=\u001b[39;49mbias,\n\u001b[1;32m   2443\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   2444\u001b[0m         momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1530\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1532\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1534\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[1;32m   1536\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/fastai/torch_core.py:378\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 378\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    379\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/_tensor.py:1279\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1278\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1279\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1280\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1281\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAG1CAYAAADk08CxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOtElEQVR4nO3de3gTdb4/8HeStml6v9GmoVdosUAFoUChyE3kJlQElZVChd0V8aDLQUTRw9FFfgoKKOyR48q6ewAVRRRFFBeLFrnIrRTKHaSl0AK9AC1Nm16SJvP7I+1AaAtpSTtJ+n49T54lk8nMZ9gxefOdT74jEwRBABERERHdkVzqAoiIiIgcAUMTERERkRUYmoiIiIiswNBEREREZAWGJiIiIiIrMDQRERERWYGhiYiIiMgKDE1EREREVnCRugBnYjKZcOXKFXh7e0Mmk0ldDhEREVlBEASUl5dDo9FALm96PImhyYauXLmC8PBwqcsgIiKiFsjPz0dYWFiTrzM02ZC3tzcA81+6j4+PxNUQERGRNbRaLcLDw8Xv8aYwNNlQ/SU5Hx8fhiYiIiIHc7fWGjaCExEREVmBoYmIiIjICrw818ZMJhP0er3UZTgtV1dXKBQKqcsgIiInxNDUhvR6PXJzc2EymaQuxan5+flBrVZz2gciIrIphqY2IggCCgoKoFAoEB4efsd5IKhlBEFAZWUliouLAQChoaESV0RERM6EoamN1NbWorKyEhqNBh4eHlKX47RUKhUAoLi4GMHBwbxUR0RENsPhjjZiNBoBAG5ubhJX4vzqQ6nBYJC4EiIiciYMTW2MfTatj3/HRETUGhiaiIiIiKzA0ERERERkBYYmR2MyArm7geNfm//XZJS6ojuKiorCypUrxecymQybN2+WrB4iIqKW4q/nHMmpLcC2+YD2ys1lPhpg9LtAt0elq4uIiKgdYGhyFKe2ABufBiBYLtcWmJdP+oTBiYjaBZNJQKXBCF1NLcqra6GrMT/K6/63ou6hq6lFRXUtKmqMqKgxwGgCOnfwRFyoN+4L8UHnYE8oXTgtCVmPockRmIzmEabbAxNQt0wGbHsViBsLyG33AbB69WosWrQI+fn5FpNxPvroo/D398cbb7yBuXPnYv/+/dDpdOjatSuWLFmChx9+2Op9XL58GXPnzkVaWhrkcjkefPBB/O1vf0NUVBR27dqF4cOHIz8/H2q1WnzPSy+9hIyMDOzatctmx0pErctkEqDT10JXF2AqaixDj0XQqX9U10Knrw8+9e81LxMa+zi0ws+nb/7ZRS5D5w5euE/tjbhQb3RV+yAu1BtqH3f+CpcaxdDkCC7utbwk14AAaC+b14seZLPdPvnkk5g9ezZ27NiB4cOHAwBKS0vx008/4fvvv0dFRQUeeeQRvPXWW3B3d8e6deuQnJyMs2fPIiIi4q7br6ysxLBhwzBo0CDs2rULLi4ueOuttzB69GgcO3YMgwcPRqdOnfDpp5/i5ZdfBmCeJPSzzz7DO++8Y7PjJLork9H831dFEeAVAkQm2fQfKPbKKAad24OLOfRUVBug0xtvBpw7jPbo9Lbvv1TIZfB0U8BL6QIvdxd4Kl3Mf657iM/rXoMg4FxxBc4UluNMgRba6lqcLSrH2aJybDl6c7s+7i6IC/VBnNobcXVB6r4Qb/M2qF3jGeAIKopsu56VAgICMHr0aHz++ediaPrqq68QEBCA4cOHQ6FQoGfPnuL6b731Fr799lts2bIFL7zwwl23v2HDBsjlcvzzn/8U/1W3Zs0a+Pn54ddff8XIkSPx5z//GWvWrBFD09atW1FZWYlJkybZ9FiJmuRgvYRGkyCGFYsQU337aI45/OhqjBajPbcGnspWCjqWwUYBL3dXeCkV8HQzB5xbQ4+3uws83W758y3vdXeVt3hESBAEFJRV42xhOU4XanGmoBxnC8uRc7UC2upaHMwtwcHcEov3RAR41AUpbzFURQZ6QiHnqFR7wdDkCLxCbLteM0yZMgXPPvssPvzwQyiVSqxfvx5PPfUUFAoFdDod3nzzTfzwww+4cuUKamtrUVVVhby8PKu2nZmZiezsbHh7e1ssr66uRk5ODgBg+vTp+O///m/s378f/fv3x//93/9h0qRJ8PT0tPmxEjXQRr2EtUaTeQSnblTn9ktW9aM4Ffpb/mwxmnNztKfKYPug4yKXmUdr3BoGF0+lAl5Kc+i5fbTHYuSnLgwpXVoedGxJJpNB46eCxk+FYXHB4vKaWiNyinU4W2QOUqcLy3G2UIsibQ3ySiqRV1KJtFM3/4Hq7ipHlxBzkLpP7YOuam/cp/ZGoJdSisOiVsbQ5Agik8z/stUWoPG+Jpn59cgkm+86OTkZJpMJW7duRd++fbF79268//77AICXX34ZP/30E5YvX46YmBioVCo88cQT0Ov1Vm3bZDIhISEB69evb/Bahw4dAADBwcFITk7GmjVr0KlTJ/z444/49ddfbXZ8RE26Sy+hABmEf7+KKyHDoDPglktWliM81oz2VBtMNi/fVSFrEFw868OL281LVt51yz2VCnFU5/bRHnsJOm1B6aJAN40Puml8gF43l5fo9DhTqMXZwnKcKSg3/7moHNUGE45dKsOxS2UW2+ngrbw5KlV3iS8m2IuN5w6OockRyBXmSwEbnwYgg+WHeN0H2eh3WqXHQqVSYeLEiVi/fj2ys7PRpUsXJCQkAAB2796N6dOnY8KECQCAiooKXLhwwept9+7dG19++SWCg4Ph4+PT5HrPPPMMnnrqKYSFhaFz584YOHDgPR0TkVXu0ksogwBZ+WXMe+8j7Dd1s8ku3RTyujBzc/SmsT6d+tGeW0OP5eUuF34521iApxuSOgchqXOQuMxoEpBXUokzBVpzn1RdqLpYUomr5TW4Wl6D3eeuiesr5DJ0CvK8pV/KfJlP48vGc0fB0OQouj1qvhTQaG/FO63aWzFlyhQkJyfj5MmTmDp1qrg8JiYG33zzDZKTkyGTyfD666/DZLL+X8xTpkzBsmXLMH78eCxatAhhYWHIy8vDN998g5dffhlhYWEAgFGjRsHX1xdvvfUWFi1aZPPjI2qUlT2CoYoyBKrcbgsxN/t0mhztuW2Zp1LBoONgFHIZooM8ER3kiTH3h4rLdTW1+L3I3CN1prAcp+tCVVmVAeeKK3CuuALf39J47u3uIo5I3af2RtdQb3QJ8Ya3u6sER0V3wtDkSLo9ap5WoI1/xfPQQw8hICAAZ8+eRUpKirh8xYoV+NOf/oSkpCQEBQVh/vz50Gq1Vm/Xw8MDu3btwvz58zFx4kSUl5ejY8eOGD58uMXIk1wux/Tp07F48WI8/fTTNj02oiZZ2SO44k+jbPqrVXJ8nkoX9IrwR68If3GZIAgo0tbgtHiJzxykcq5WoLy6FhkXSpFxodRiO2H+KvOlvbopEeLUPogK9ICLgjfzkIpMEFo62wXdTqvVwtfXF2VlZQ0uN1VXVyM3NxfR0dFwd3eXqELHNWPGDBQVFWHLli13XZd/12QTJiOwMh6CtgCyO/USzjneLqYfoNahrzXh/LWKuj4p8yW+MwXlKNRWN7q+m4scXUK8boapun6pIDae35M7fX/fiiNNZNfKysqQkZGB9evX47vvvpO6HGpP5AoIo98BNj4NkwBY/qq8dXsJqf1wc5HXBSDLL+oblXpxPqmzReU4XTclQpXBiBOXtThx2XJUP8jLTQxS5kt8PogJ9oK7K89PW2JoIrs2fvx4HDx4EDNnzsSIESOkLofamUyPQfhYPwcLXT9BKG6Zs6cNegmpffPzcEP/ToHo3ylQXGYyCcgvrcTpgptN52cKy3Hhug7XKvTYk30Ne7JvNp7LZUB0XeN517opEeLU3gjzV7HxvIUYmsiucXoBktKn+y/iJ1M/+MWPx7t9dO1uRnCyL3K5DJGBnogM9MTo+Ju3lqrU1+JcUQXOFGrFEakzhVqUVhqQc1WHnKs6bD1WIK7vrXRBF3E6BPMv+O5Te8OHjed3xdBERNSI6xU1+PfxQgDA1AGdgTBfiSsiapyHmwt6hvuhZ7ifuEwQBBSX19y8xFdonqgzu7gc5TW1yLxYisyLlo3nHf1UYtN5/USd0UGebDy/BUNTG2Pffevj3zHZwsZDl6A3mtAzzBf3MzCRg5HJZAjxcUeIjzuGdOkgLjcYTci9phOnQaj/Jd+VsmpcvlGFyzeq8MuZYnF9Nxc5Yjp4iTc0rr+5cQcvZbu8xMfQ1EYUCvNQvl6vh0qlkrga51ZZWQkAcHXlUDO1jNEkYP2BiwCAKf0jJa6GyHZcFebbvnQJ8cb4W5aXVRpwtqj8lkt85tEpnd6IUwVanCrQArgsrh/o6WYOUHW/3otTeyM22BsqN+e+bM3Q1EZcXFzg4eGBq1evwtXVFXI5hzttTRAEVFZWori4GH5+fmJQJWquXb9fxaXSKviqXJHcQyN1OUStztfDFf2iA9AvOkBcZjIJuFRaZZ4GoX46hMJyXLimw3WdHntzrmNvznVxfbkMiAr0FOeUuk9tHp0K81dB7iQ3NWZoaiMymQyhoaHIzc3FxYsXpS7Hqfn5+UGtVt99RaImfLbf/N/oEwlhTv8vZ6KmyOUyRAR6ICLQAyO73/xMrdIbca64bl6pgpthqkSnx/lrOpy/psOPdf2AAODppqhrPPdB11Bv3Bdi/rOvh+NdDWBoakNubm6IjY21+oa21Hyurq4cYaJ7kl9SifSz5p6OKYkREldDZH9Ubgr0CPNDjzA/cZkgCLhaUYMzdb/eq5/5/FxRBXR6I47k3cCRvBsW29H4utf1SN2cqLNTB0+42nHjOUNTG5PL5ZylmsiOfXEwD4IAPBgThE4dvKQuh8ghyGQyBHu7I9jbHYNvazy/cE1nMdv5mcJyXL5RhStl1bhSVo0dZ6+K67sqZIgJtpwOIU7tjWBv+2g8Z2giIqpTU2vElxn5AICp/TnKRHSvXBVyxIZ4IzbEG8k9b/YHllUZ8HtRuTglQv0v+SpqanG6QIvTBZYznvt7uIqN539+MBrhAR5tfSgAGJqIiETbThTiuk6PEB8lHu5q3Q17iaj5fFWu6BsVgL5RNxvPBaG+8dz8673TdYEq95oOpZUG7D9fgv3nS5A6QLpftDI0ERHVWb8/DwAwuV8EJ/QjamMymQzhAR4ID/DAiG43/9FSbTAiu7gCpwu0+L2oHFGBnpLVyNBERATgTKEWBy+UQCGX4am+vDRHZC/cXRWI7+iL+I7STzLLf0oREeHmKNPIbiFQ+/LHGkTUEEMTEbV7FTW1+PaIebbjqZwBnIiawNBERO3e5iOXUVFTi05BnkjqHCh1OURkpxiaiKhdEwRBnAF8Sv9Iu5gLhojsE0MTEbVrh/NKcaawHO6ucjzRO0zqcojIjjE0EVG79uk+8yhTcg+NQ94Li4jaDkMTEbVb1ytqxBuLSjlhHhE5BoYmImq3Nh66BL3RhB5hvhY3HyUiagxDExG1SyaTgM8Pmi/NTU3kKBMR3Z2koWnXrl1ITk6GRqOBTCbD5s2bLV4XBAELFy6ERqOBSqXC0KFDcfLkSYt1hg4dCplMZvF46qmnLNYpLS1FamoqfH194evri9TUVNy4ccNinby8PCQnJ8PT0xNBQUGYPXs29Hp9axw2EdmBneeuIr+kCj7uLhY3EiUiaoqkoUmn06Fnz55YtWpVo68vXboU77//PlatWoWMjAyo1WqMGDEC5eXlFuvNmDEDBQUF4mP16tUWr6ekpCArKwvbtm3Dtm3bkJWVhdTUVPF1o9GIsWPHQqfTYc+ePdiwYQM2bdqEl156yfYHTUR24bO6BvAnEsKhclNIXA0ROQJJ7z03ZswYjBkzptHXBEHAypUrsWDBAkycOBEAsG7dOoSEhODzzz/HzJkzxXU9PDygVqsb3c7p06exbds27N+/H4mJiQCAjz/+GAMGDMDZs2dx3333IS0tDadOnUJ+fj40GvO/ON977z1Mnz4db7/9Nnx8fGx52EQksUullUg/WwwAmNKf95kjIuvYbU9Tbm4uCgsLMXLkSHGZUqnEkCFDsHfvXot1169fj6CgIHTv3h3z5s2zGInat28ffH19xcAEAP3794evr6+4nX379iE+Pl4MTAAwatQo1NTUIDMzs8kaa2pqoNVqLR5EZP++OJgHQQAGxgSicwcvqcshIgch6UjTnRQWmn8GHBISYrE8JCQEFy9eFJ9PmTIF0dHRUKvVOHHiBF577TUcPXoU27dvF7cTHBzcYPvBwcHiPgoLCxvsx9/fH25ubuI6jVmyZAnefPPNlh0gEUlCX2vClxn5ANgATkTNY7ehqd7ttzQQBMFi2YwZM8Q/x8fHIzY2Fn369MHhw4fRu3fvRrfR2HasWed2r732GubOnSs+12q1CA8Pt+KoiEgq204W4lqFHiE+SjzcLeTubyAiqmO3l+fqe5RuH+kpLi5uMCp0q969e8PV1RXnzp0Tt1NUVNRgvatXr4rbUavVDfZTWloKg8Fwx30plUr4+PhYPIjIvtU3gD/VNwKuCrv9CCQiO2S3nxj1l9zqL7MBgF6vx86dO5GUlNTk+06ePAmDwYDQ0FAAwIABA1BWVoaDBw+K6xw4cABlZWXidgYMGIATJ06goKBAXCctLQ1KpRIJCQm2PjQiksjZwnIcvFAChVyGyf3YAE5EzSPp5bmKigpkZ2eLz3Nzc5GVlYWAgABERERgzpw5WLx4MWJjYxEbG4vFixfDw8MDKSkpAICcnBysX78ejzzyCIKCgnDq1Cm89NJL6NWrFwYOHAgA6Nq1K0aPHo0ZM2aIUxE8++yzGDduHO677z4AwMiRI9GtWzekpqZi2bJlKCkpwbx58zBjxgyOHhE5kfUHzKNMI7qGQO3rLnE1RORwBAnt2LFDANDgMW3aNEEQBMFkMgl//etfBbVaLSiVSmHw4MHC8ePHxffn5eUJgwcPFgICAgQ3Nzehc+fOwuzZs4Xr169b7Of69evClClTBG9vb8Hb21uYMmWKUFpaarHOxYsXhbFjxwoqlUoICAgQXnjhBaG6urpZx1NWViYAEMrKylr090FEraei2iB0f2ObEDn/B2H371elLoeI7Ii1398yQRAECTObU9FqtfD19UVZWRlHqIjszPoDF7Hg2xOIDvLEL3OHQC5v+kceRNS+WPv9bbc9TUREtiIIAj6tawCfkhjBwERELcLQRERO73BeKc4UlkPpIscTCWFSl0NEDoqhiYic3mf78wAAj/bUwM/DTeJqiMhRMTQRkVMr0emx9Zh5OpGp/TkDOBG1HEMTETm1jYfyoTeacH9HX/QM95O6HCJyYAxNROS0TCZBnJsplaNMRHSPGJqIyGntPHcV+SVV8HF3QXJPjdTlEJGDY2giIqe1fr95lOnxhDCo3BQSV0NEjo6hiYic0qXSSqSfKQbABnAisg2GJiJySl8czINJAJI6B6JzBy+pyyEiJ8DQREROR19rwpcZ+QA4ykREtsPQRERO56eThbhWoUewtxIjuoVIXQ4ROQmGJiJyOp/WNYA/1S8Crgp+zBGRbfDThIicyu9F5TiYWwKFXIbJ/cKlLoeInAhDExE5lfppBh7uGoxQX5XE1RCRM2FoIiKnoaupxabDlwGwAZyIbI+hiYicxndZV1BRU4voIE8M7BwkdTlE5GQYmojIKQiCIDaAT0mMgFwuk7giInI2DE1E5BQO593A6QItlC5yPJEQJnU5ROSEGJqIyCnUN4An99TAz8NN4mqIyBkxNBGRwyvR6fHDsQIAbAAnotbD0EREDu+rQ/nQG02I7+iDnmG+UpdDRE6KoYmIHJrJJGD9gTwAQGr/SMhkbAAnotbB0EREDm3XuavIK6mEt7sLkntqpC6HiJwYQxMRObTP9ptHmZ5ICIOHm4vE1RCRM2NoIiKHdflGFdLPFAEApiSyAZyIWhdDExE5rC8O5MEkAAM6BSIm2EvqcojIyTE0EZFD0teasCEjHwCQOoCjTETU+hiaiMgh/XSyENcqahDsrcSIbiFSl0NE7QBDExE5pM/qZgB/qm84XBX8KCOi1sdPGiJyOOeKynEgtwQKuQyTEyOkLoeI2gmGJiJyOPWjTMPjghHqq5K4GiJqLxiaiMih6Gpq8c3hywB4nzkialsMTUTkULYcvYLymlpEBXrgwZggqcshonaEoYmIHIYgCPh0n/nS3JTESMjlvM8cEbUdhiYichhH8m/gVIEWbi5yPJEQJnU5RNTOMDQRkcP4rG6UKbmHBv6ebhJXQ0TtDUMTETmEUp0ePxwvAABM7c9pBoio7TE0EZFD+CozH/paE+I7+uCBcD+pyyGidoihiYjsnskkYP2BPADA1MRIyGRsACeitsfQRER2b3f2NVy8Xglvdxc8+oBG6nKIqJ1iaCIiu1c/A/jjvcPg4eYicTVE1F4xNBGRXbt8owq/nC4CwAZwIpIWQxMR2bUNB/NgEoD+nQIQE+wtdTlE1I4xNBGR3dLXmrAhIx8AkNo/StpiiKjdY2giIruVdqoQV8tr0MFbiZHdQ6Quh4jaOYYmIrJb9Q3gT/UNh6uCH1dEJC1+ChGRXTpXVI7950sglwGT+7EBnIikx9BERHapfjLL4V1DoPFTSVwNERFDExHZoUp9LTZlXgIApPaPlLgaIiIzhiYisjvfZV1BeU0tIgM98GBMkNTlEBEBYGgiIjsjCILYAD4lMQJyOe8zR0T2gaGJiOxKVv4NnLyihZuLHE8mhEtdDhGRiKGJiOzKp3WjTON6hMLf003iaoiIbmJoIiK7UarT44djBQCAqWwAJyI7w9BERHbj68xL0Nea0F3jg17hflKXQ0RkgaGJiOyCySTgswPmS3NT+0dCJmMDOBHZF4YmIrILe7Kv4eL1SngrXTD+AY3U5RARNcDQRER2ob4B/PGEMHi4uUhcDRFRQ5KGpl27diE5ORkajQYymQybN2+2eF0QBCxcuBAajQYqlQpDhw7FyZMnG92WIAgYM2ZMo9s5fPgwRowYAT8/PwQGBuLZZ59FRUWFxTp5eXlITk6Gp6cngoKCMHv2bOj1elseLhE14cqNKvxyugiAeW4mIiJ7JGlo0ul06NmzJ1atWtXo60uXLsX777+PVatWISMjA2q1GiNGjEB5eXmDdVeuXNloD8SVK1fw8MMPIyYmBgcOHMC2bdtw8uRJTJ8+XVzHaDRi7Nix0Ol02LNnDzZs2IBNmzbhpZdestmxElHTNhzMg0kA+ncKQGyIt9TlEBE1TrATAIRvv/1WfG4ymQS1Wi2888474rLq6mrB19dX+Oijjyzem5WVJYSFhQkFBQUNtrN69WohODhYMBqN4rIjR44IAIRz584JgiAIP/74oyCXy4XLly+L63zxxReCUqkUysrKrD6GsrIyAUCz3kPU3ulrjUKft7YLkfN/EL4/evnubyAisjFrv7/ttqcpNzcXhYWFGDlypLhMqVRiyJAh2Lt3r7issrISkydPxqpVq6BWqxtsp6amBm5ubpDLbx6qSmW+Y/qePXsAAPv27UN8fDw0mpvNp6NGjUJNTQ0yMzObrLGmpgZardbiQUTNk3ayCFfLaxDkpcTIbg3/GyYishd2G5oKCwsBACEhIRbLQ0JCxNcA4MUXX0RSUhLGjx/f6HYeeughFBYWYtmyZdDr9SgtLcV//dd/AQAKCgrEfd2+H39/f7i5uVns63ZLliyBr6+v+AgP5y0fiJqr/j5zk/uFw83Fbj+SiIjsNzTVu71PSRAEcdmWLVuQnp6OlStXNvn+7t27Y926dXjvvffg4eEBtVqNTp06ISQkBAqFosn93L6vxrz22msoKysTH/n5+c08OqL2Lbu4HPvOX4dcBkzuxwZwIrJvdhua6i+13T7SU1xcLI4KpaenIycnB35+fnBxcYGLi/lnyo8//jiGDh0qviclJQWFhYW4fPkyrl+/joULF+Lq1auIjo4W93X7fkpLS2EwGBqMQN1KqVTCx8fH4kFE1vtsfx4A4KG4EGj8VBJXQ0R0Z3YbmqKjo6FWq7F9+3ZxmV6vx86dO5GUlAQAePXVV3Hs2DFkZWWJDwBYsWIF1qxZ02CbISEh8PLywpdffgl3d3eMGDECADBgwACcOHFCvFwHAGlpaVAqlUhISGjFoyRqvyr1tdiUeQkAkDqA95kjIvsn6QxyFRUVyM7OFp/n5uYiKysLAQEBiIiIwJw5c7B48WLExsYiNjYWixcvhoeHB1JSUgCYR4gaa/6OiIgQR5EAYNWqVUhKSoKXlxe2b9+Ol19+Ge+88w78/PwAACNHjkS3bt2QmpqKZcuWoaSkBPPmzcOMGTM4ekTUSrZkXUF5TS0iAz0wKCZI6nKIiO5K0tB06NAhDBs2THw+d+5cAMC0adOwdu1avPLKK6iqqsKsWbNQWlqKxMREpKWlwdu7efO4HDx4EH/9619RUVGBuLg4rF69GqmpqeLrCoUCW7duxaxZszBw4ECoVCqkpKRg+fLltjlQIrIgCII4A3hKvwjI5bzPHBHZP5kgCILURTgLrVYLX19flJWVcYSK6A6O5JViwod74eYix/7XhiPA003qkoioHbP2+9tue5qIyHnVN4CPuz+UgYmIHAZDExG1qVKdHj8cuwIAmMoGcCJyIAxNRNSmvs68hJpaE7qF+qBXuJ/U5RARWY2hiYjajMkkYP0BcwP41P6Rd5w8lojI3jA0EVGb+S3nGi5cr4S30gXjH9Dc/Q1ERHaEoYmI2syn+8yjTBN7d4SnUtIZT4iImo2hiYjaREFZFX4+XQQAmNKfDeBE5HgYmoioTXxxIA8mAUiMDkCXkOZNUEtEZA8Ymoio1RmMJmzIyAdgbgAnInJEDE1E1Oq2nypCcXkNgryUGNW94f0iiYgcAUMTEbW6+gbwp/qGw82FHztE5Jj46UVErSq7uAL7zl+HXAZMToyQuhwiohZjaCKiVlU/meVDcSHo6KeSuBoiopZjaCKiVlOpr8XXmZcAAFP7c5SJiBwbQxMRtZrvj15BeXUtIgI8MDi2g9TlEBHdE4YmImoVgiDg0/3mS3NTEiMgl/M+c0Tk2BiaiKhVHL1UhhOXtXBzkePJPuFSl0NEdM8YmoioVXxWN8o09v5QBHi6SVwNEdG9Y2giIpu7UanH90evAOAM4ETkPBiaiMjmvs68hJpaE7qG+qB3hJ/U5RAR2QRDExHZlMkkYP2BPADmaQZkMjaAE5FzYGgiIpv6Lecacq/p4KV0wWMPdJS6HCIim2FoIiKbqm8An9i7IzyVLhJXQ0RkOwxNRGQzBWVV+Pl0MQA2gBOR82FoIiKb+eJgPowmAf2iA9AlxFvqcoiIbIqhiYhswmA0YcPB+gZwjjIRkfNhaCIim9h+qgjF5TUI8nLD6O5qqcshIrI5hiYison6BvA/9A2Hmws/WojI+fCTjYjuWXZxBfbmXIdMBkzuFyF1OURErYKhiYju2foD5lGm4XHBCPP3kLgaIqLWwdBERPekSm/EpsxLAIApbAAnIifG0ERE9+T7o1egra5FeIAKQ2I7SF0OEVGrYWgionvyaV0D+JTESMjlvM8cETkvhiYiarGj+Tdw/HIZ3BRyPJkQJnU5REStiqGJiFqsfpqBsT1CEeillLgaIqLWxdBERC1yo1KPLUevAACm9uc0A0Tk/BiaiKhFvs68hJpaE+LU3ugd4S91OURErY6hiYiazWQSsP6A+T5zqQMiIZOxAZyInB9DExE1296c68i9poOX0gWPPdBR6nKIiNoEQxMRNVt9A/iEXh3hqXSRuBoiorbB0EREzVJYVo3tp4sAAFM5AzgRtSMMTUTULF8czIPRJKBfVADuU3tLXQ4RUZthaCIiqxmMJmzIMDeAT+E0A0TUzjA0EZHVfj5VhCJtDYK83DA6Xi11OUREbapFoSk/Px+XLl0Snx88eBBz5szBP/7xD5sVRkT257MD5gbwSX3CoXRRSFwNEVHbalFoSklJwY4dOwAAhYWFGDFiBA4ePIj/+q//wqJFi2xaIBHZh5yrFfgt+zpkMmByP16aI6L2p0Wh6cSJE+jXrx8AYOPGjYiPj8fevXvx+eefY+3atbasj4jsxPr95l6mh+4LRniAh8TVEBG1vRaFJoPBAKXSfHPOn3/+GY8++igAIC4uDgUFBbarjojsQpXeiK8z8wFwmgEiar9aFJq6d++Ojz76CLt378b27dsxevRoAMCVK1cQGBho0wKJSHrfH70CbXUtwgNUGNylg9TlEBFJokWh6d1338Xq1asxdOhQTJ48GT179gQAbNmyRbxsR0TOo74BPKVfJBRy3meOiNqnFt3/YOjQobh27Rq0Wi38/W/e3fzZZ5+Fhwd7HYicydH8Gzh2qQxuCjkm9QmTuhwiIsm0aKSpqqoKNTU1YmC6ePEiVq5cibNnzyI4ONimBRKRtOrvM/fI/WoEeiklroaISDotCk3jx4/HJ598AgC4ceMGEhMT8d577+Gxxx7D3//+d5sWSETSKas04PtjVwCwAZyIqEWh6fDhwxg0aBAA4Ouvv0ZISAguXryITz75BP/zP/9j0wKJSDpfH76EaoMJcWpvJET63/0NREROrEWhqbKyEt7e5ht1pqWlYeLEiZDL5ejfvz8uXrxo0wKJSBqCIGB93aW5qf0jIZOxAZyI2rcWhaaYmBhs3rwZ+fn5+OmnnzBy5EgAQHFxMXx8fGxaIBFJY2/OdZy/poOnmwKP9eoodTlERJJrUWh64403MG/ePERFRaFfv34YMGAAAPOoU69evWxaIBFJ49N95lGmCb07wkvZoh/aEhE5lRZ9Ej7xxBN48MEHUVBQIM7RBADDhw/HhAkTbFYcEUmjsKwa208XAWADOBFRvRb/81GtVkOtVuPSpUuQyWTo2LEjJ7YkchIbMvJgNAnoG+WPODUvuRMRAS28PGcymbBo0SL4+voiMjISERER8PPzw//7f/8PJpPJ6u3s2rULycnJ0Gg0kMlk2Lx5s8XrgiBg4cKF0Gg0UKlUGDp0KE6ePNnotgRBwJgxYxrdzu+//47x48cjKCgIPj4+GDhwIHbs2GGxTl5eHpKTk+Hp6YmgoCDMnj0ber3e6mMhchYGowlfHDTfnJejTEREN7UoNC1YsACrVq3CO++8gyNHjuDw4cNYvHgxPvjgA7z++utWb0en06Fnz55YtWpVo68vXboU77//PlatWoWMjAyo1WqMGDEC5eXlDdZduXJlk7/uGTt2LGpra5Geno7MzEw88MADGDduHAoLCwEARqMRY8eOhU6nw549e7BhwwZs2rQJL730ktXHQuQsfjldhCJtDQI93TA6Xi11OURE9kNogdDQUOG7775rsHzz5s2CRqNpySYFAMK3334rPjeZTIJarRbeeecdcVl1dbXg6+srfPTRRxbvzcrKEsLCwoSCgoIG27l69aoAQNi1a5e4TKvVCgCEn3/+WRAEQfjxxx8FuVwuXL58WVzniy++EJRKpVBWVmb1MZSVlQkAmvUeInsz5eP9QuT8H4R3/n1a6lKIiNqEtd/fLRppKikpQVxcXIPlcXFxKCkpuYcId1Nubi4KCwvF6QwAQKlUYsiQIdi7d6+4rLKyEpMnT8aqVaugVjf8V3FgYCC6du2KTz75BDqdDrW1tVi9ejVCQkKQkJAAANi3bx/i4+Oh0WjE940aNQo1NTXIzMxsssaamhpotVqLB5EjO3+1Anuyr0EmA1L6RUhdDhGRXWlRaGrqktqqVavQo0ePey4KgHjpLCQkxGJ5SEiI+BoAvPjii0hKSsL48eMb3Y5MJsP27dtx5MgReHt7w93dHStWrMC2bdvg5+cn7uv2/fj7+8PNzc1iX7dbsmQJfH19xUd4eHhLDpXIbqw/YO5lGnZfMMIDePNtIqJbtejXc0uXLsXYsWPx888/Y8CAAZDJZNi7dy/y8/Px448/2rTA2/uUBEEQl23ZsgXp6ek4cuRIk+8XBAGzZs1CcHAwdu/eDZVKhX/+858YN24cMjIyEBoa2uh+bt9XY1577TXMnTtXfK7VahmcyGFV6Y34OvMSAGBqf44yERHdrkUjTUOGDMHvv/+OCRMm4MaNGygpKcHEiRNx8uRJrFmzxiaF1V9qu32kp7i4WBwVSk9PR05ODvz8/ODi4gIXF3MGfPzxxzF06FBxnR9++AEbNmzAwIED0bt3b3z44YdQqVRYt26duK/b91NaWgqDwdBgBOpWSqUSPj4+Fg8iR/X9sSsoqzIgzF+FIV2CpS6HiMjutHieJo1Gg7ffftti2dGjR7Fu3Tr83//93z0XFh0dDbVaje3bt4uzjOv1euzcuRPvvvsuAODVV1/FM888Y/G++++/HytWrEBycjIAc88TAMjllvlQLpeL0yMMGDAAb7/9NgoKCsSRp7S0NCiVSrHvicjZ1d9nLiUxAgo57zNHRHQ7Se+NUFFRgezsbPF5bm4usrKyEBAQgIiICMyZMweLFy9GbGwsYmNjsXjxYnh4eCAlJQXAzQk2bxcREYHo6GgA5kDk7++PadOm4Y033oBKpcLHH3+M3NxcjB07FgAwcuRIdOvWDampqVi2bBlKSkowb948zJgxg6NH1C4cu3QDRy+VwVUhw6Q+vMRMRNQYSUPToUOHMGzYMPF5fX/QtGnTsHbtWrzyyiuoqqrCrFmzUFpaisTERKSlpcHb29vqfQQFBWHbtm1YsGABHnroIRgMBnTv3h3fffedeAsYhUKBrVu3YtasWRg4cCBUKhVSUlKwfPly2x4wkZ36rG6U6ZH7QxHkpZS4GiIi+yQTBEGw1caOHj2K3r17w2g02mqTDkWr1cLX1xdlZWUcoSKHUVZpQOKSn1FtMOGr5wagb1SA1CUREbUpa7+/mzXSNHHixDu+fuPGjeZsjojswNeHL6HaYEKc2ht9Iv2lLoeIyG41KzT5+vre9fWnn376ngoiorYjCALWHzBfmpvSP/KOU2wQEbV3zQpNtppOgIjsw76c6zh/VQdPNwUm9OoodTlERHatRfM0EZFz+LSuAXxC747wUkr6uxAiIrvH0ETUThVpq5F2qggAMLV/pMTVEBHZP4YmonZqw8F8GE0C+kT6I07NX3sSEd0NQxNRO1RrNOGLg+ab86YO4CgTEZE1GJqI2qGfTxejUFuNQE83jI5vOKs+ERE1xNBE1A7VzwD+ZJ9wKF0UEldDROQYGJqI2pncazrsyb4GmQyYkhghdTlERA6DoYmonVlfN8o0tEsHhAd4SFwNEZHjYGgiakeqDUZ8lXkJABvAiYiai6GJqB35/ugVlFUZ0NFPhSFdgqUuh4jIoTA0EbUjnx0wTzOQkhgBhZz3mSMiag6GJqJ24vilMhzNvwFXhQx/6BsudTlERA6HoYmonaifZmBMfCiCvJQSV0NE5HgYmojagbIqA747ehkA7zNHRNRSDE1E7cCmzEuoNphwX4g3+kb5S10OEZFDYmgicnKCIOCzA+ZLc1P7R0AmYwM4EVFLMDQRObl9Oddx/qoOHm4KPNaro9TlEBE5LIYmIidXP8o0oVdHeLu7SlwNEZHjYmgicmJF2mqknSwCwAZwIqJ7xdBE5MQ2HMxHrUlAn0h/dA31kbocIiKHxtBE5KRqjSZ8cdA8AzhHmYiI7h1DE5GT+uVMMQq11QjwdMOY+9VSl0NE5PAYmoicVP0M4JP6hEPpopC4GiIix8fQROSEcq/psPvcNchkwJTECKnLISJyCgxNRE5ofd0o05AuHRAe4CFxNUREzoGhicjJVBuM+CrzEgAglQ3gREQ2w9BE5GR+OFaAsioDOvqpMPS+YKnLISJyGgxNRE7m07pLcymJEVDIeZ85IiJbYWgiciInLpfhaP4NuCpkmNQnXOpyiIicCkMTkROpn2ZgdHwoOngrJa6GiMi5MDQROYmyKgM2Z10GwAZwIqLWwNBE5CS+OXwJ1QYTuoR4oW+Uv9TlEBE5HYYmIicgCIJ4aW5q/0jIZGwAJyKyNYYmIiew7/x15FzVwcNNgQm9OkpdDhGRU2JoInIC6/fnAQAe69UR3u6uEldDROScGJqIHFyxtho/nSwEAExNZAM4EVFrYWgicnAbMvJRaxKQEOmPbhofqcshInJaDE1EDqzWaMIXB82X5qb2j5C4GiIi58bQROTAfjlTjIKyavh7uGJMfKjU5RAROTWGJiIHVj/NwKS+4XB3VUhcDRGRc2NoInJQF67psPvcNchkwJR+bAAnImptDE1EDmr9AfMo05AuHRAR6CFxNUREzo+hicgBVRuM+CrzEgBOM0BE1FYYmogc0NZjBbhRaUBHPxWGxQVLXQ4RUbvA0ETkgD6tawBPSYyAQs77zBERtQWGJiIHc+JyGbLyb8BVIcOkPuFSl0NE1G4wNBE5mPppBkZ1V6ODt1LiaoiI2g+GJiIHoq024LusKwCA1P5sACciaksMTUQO5JvMS6gyGNElxAv9ogOkLoeIqF1haCJyEIIg4LMD5vvMTUmMhEzGBnAiorbE0ETkIPafL0F2cQU83BSY0Luj1OUQEbU7DE1EDuKzuhnAxz/QET7urhJXQ0TU/jA0ETmAYm01fjpRCACY2j9C4mqIiNonhiYiB/BlRj5qTQJ6R/ihu8ZX6nKIiNolhiYiO1drNOHzg+YG8KmcZoCISDIMTUR2Lv1MMQrKquHv4YpH7g+VuhwionZL0tC0a9cuJCcnQ6PRQCaTYfPmzRavC4KAhQsXQqPRQKVSYejQoTh58mSj2xIEAWPGjGmwnV9//RUymazRR0ZGhrheXl4ekpOT4enpiaCgIMyePRt6vb41DpuoWeqnGZjUJxzurgqJqyEiar8kDU06nQ49e/bEqlWrGn196dKleP/997Fq1SpkZGRArVZjxIgRKC8vb7DuypUrG523JikpCQUFBRaPZ555BlFRUejTpw8AwGg0YuzYsdDpdNizZw82bNiATZs24aWXXrLtARM104VrOuz6/SoA8815iYhIOi5S7nzMmDEYM2ZMo68JgoCVK1diwYIFmDhxIgBg3bp1CAkJweeff46ZM2eK6x49ehTvv/8+MjIyEBpqefnCzc0NarVafG4wGLBlyxa88MILYshKS0vDqVOnkJ+fD41GAwB47733MH36dLz99tvw8fGx6XETWau+l2lIlw6IDPSUuBoiovbNbnuacnNzUVhYiJEjR4rLlEolhgwZgr1794rLKisrMXnyZKxatcoiHDVly5YtuHbtGqZPny4u27dvH+Lj48XABACjRo1CTU0NMjMzm9xWTU0NtFqtxYPIVqoNRmw8lA+ADeBERPbAbkNTYaF5TpqQkBCL5SEhIeJrAPDiiy8iKSkJ48ePt2q7//rXvzBq1CiEh4db7Ov2/fj7+8PNzc1iX7dbsmQJfH19xcet2yS6V1uPFeBGpQEaX3c8FBcsdTlERO2e3Yamerf3KQmCIC7bsmUL0tPTsXLlSqu2denSJfz000/485//fNf93L6vxrz22msoKysTH/n5+VbVQWSN+hnAUxIjoJDzPnNERFKz29BUf6nt9pGe4uJicVQoPT0dOTk58PPzg4uLC1xczC1ajz/+OIYOHdpgm2vWrEFgYCAeffTRBvu6fT+lpaUwGAwNRqBupVQq4ePjY/EgsoUTl8twJO8GXOQyTOrLEUwiIntgt6EpOjoaarUa27dvF5fp9Xrs3LkTSUlJAIBXX30Vx44dQ1ZWlvgAgBUrVmDNmjUW2xMEAWvWrMHTTz8NV1fL+3YNGDAAJ06cQEFBgbgsLS0NSqUSCQkJrXSERE1bXzfKNCpejWBvd4mrISIiQOJfz1VUVCA7O1t8npubi6ysLAQEBCAiIgJz5szB4sWLERsbi9jYWCxevBgeHh5ISUkBYB4haqz5OyIiAtHR0RbL0tPTkZub2+iluZEjR6Jbt25ITU3FsmXLUFJSgnnz5mHGjBkcPaI2p602YPORKwCAVDaAExHZDUlD06FDhzBs2DDx+dy5cwEA06ZNw9q1a/HKK6+gqqoKs2bNQmlpKRITE5GWlgZvb+9m7+tf//oXkpKS0LVr1wavKRQKbN26FbNmzcLAgQOhUqmQkpKC5cuXt/zgiFrom8xLqDIYERvshcToAKnLISKiOjJBEASpi3AWWq0Wvr6+KCsr4wgVtYggCBixYheyiyvw5qPdMS0pSuqSiIicnrXf33bb00TUHh3ILUF2cQVUrgpM6N1R6nKIiOgWDE1EduTT/eYG8Md6aeDj7nqXtYmIqC0xNBHZieLyavx0wjz1BWcAJyKyPwxNRHZiY0Y+ak0CekX4obvGV+pyiIjoNgxNRHbAaBLw+QHzzXmnJnKUiYjIHjE0EdmB9DPFuFJWDT8PV4ztESp1OURE1AiGJiI78FldA/ikPuFwd1VIXA0RETWGoYlIYhev67Dz96sAgJR+ERJXQ0RETWFoIpJYfS/T4C4dEBXkKXE1RETUFIYmIglVG4zYeCgfADA1kaNMRET2jKGJSEI/Hi9AaaUBGl93PBQXLHU5RER0BwxNRBKqbwCf3C8CLgr+50hEZM/4KU0kkZNXynA47wZc5DL8oV+41OUQEdFdMDQRSeSz/eYG8FHxagR7u0tcDRER3Q1DE5EEtNUGfJd1GQBnACcichQMTUQS+PbwZVTqjYgJ9kL/TgFSl0NERFZgaCJqY4IgiA3gUxMjIJPJJK6IiIiswdBE1MYO5JbgXHEFVK4KTEwIk7ocIiKyEkMTURurH2Ua/4AGPu6uEldDRETWYmgiakPF5dX46WQhAGBqfzaAExE5EoYmoja0MSMfBqOAB8L9EN/RV+pyiIioGRiaiNqI0STgi4Pm+8ylcpSJiMjhMDQRtZEdZ4px+UYV/DxcMbZHqNTlEBFRMzE0EbWRT+sawJ9MCIO7q0LiaoiIqLkYmojaQN71Suw6dxUAMIUzgBMROSSGJqI2sP7gRQgCMCg2CFFBnlKXQ0RELeAidQFEzsxkEvDvE4X44oD55rycZoCIyHExNBG1ApNJwI8nCvA/v5zD70UVAIAuIV4YHhcscWVERNRSDE1ENtRYWPJ2d8GfBkbjTw9Gw0XBK+JERI6KoYnIBowmAT8eN4elc8U3w9KfH4zGHwdGw1fF26UQETk6hiaie2A0CdhaF5ayGZaIiJwaQxNRCxhNAn44dgUfpGeLYcnH3QV/frATpg+MYlgiInJCDE1EzVAflv7nl3PIuaoDYA5LzwwyhyUfd4YlIiJnxdBEZIXGwpKvyhXPPBiNaQxLRETtAkMT0R0YTQK+P3oF/5N+DudvCUszBkVjWlIUvBmWiIjaDYYmokbUGk34vq5nqT4s+XnUjSwxLBERtUsMTUS3EMPSL9k4f+1mWJoxqBOeHhDJsERE1I4xNBHBHJa2HDWPLOXeFpamJUXBS8n/VIiI2jt+E1C7Vms04busK/gg/RwuXK8EAPh7uGLG4E54egDDEhER3cRvBGqXao0mbM66glUMS0REZCV+M1C7Ums04dsjl7FqRzYu3hKWnh3cGU8PiIQnwxIRETWB3xDULjQWlgI83fDs4E5I7c+wREREd8dvCnJqhvqwlJ6NvJKbYWnm4E6YyrBERETNwG8MckoGownfHr6MD3acQ35JFQAgsG5kiWGJiIhagt8c5FQMRhO+OXwJq3Zki2EpyOtmWPJw4ylPREQtw28Qcgr1YemD9GxcKr0ZlmYO7owp/SMYloiI6J7xm4Qcmr725sjSrWHpuSGdMSUxEio3hcQVEhGRs2BoIoekrzVh0+FLWJWejcs36sOSEs8N6cSwRERErYKhiRyKvtaErzMv4X93MCwREVHbYmgih9BYWOrgrcRzQzojpV8EwxIREbU6hiaya/paE77KzMeHO3IswtJ/DOmMlMQIuLsyLBERUdtgaCK7VFNrxFeHLuHDHdm4UlYNAAj2VuI/hnbG5H4MS0RE1PYYmsiu1NQasfHQJfydYYmIiOwMQxPZhfqw9OGObBTcEpZmDe2MpxiWiIjIDjA0kaRqao3YmJGPD3/NEcNSiI8Ss4bG4A99wxmWiIjIbjA0kSSqDUZsPGRu8C7UmsOS2scds4Z1xqQ+DEtERGR/GJqoTVUbjPgyIx9//5VhiYiIHAtDE7WJ+rD04a/ZKNLWADCHpeeHdcakvuFQujAsERGRfWNoolZVbTBiw8E8/H1njhiWQn3dMWtYDCb1CWNYIiIihyGXcue7du1CcnIyNBoNZDIZNm/ebPG6IAhYuHAhNBoNVCoVhg4dipMnTza6LUEQMGbMmEa3AwBbt25FYmIiVCoVgoKCMHHiRIvX8/LykJycDE9PTwQFBWH27NnQ6/W2OtR2p9pgxJrfcjF46Q4s/P4UirQ10Pi6463H4vHry0OR2j+SgYmIiByKpCNNOp0OPXv2xB//+Ec8/vjjDV5funQp3n//faxduxZdunTBW2+9hREjRuDs2bPw9va2WHflypWQyWSN7mfTpk2YMWMGFi9ejIceegiCIOD48ePi60ajEWPHjkWHDh2wZ88eXL9+HdOmTYMgCPjggw9se9BOrtpgxOcH8vDRzhwUl5tHljS+7nj+oRg8kcCRJSIiclwyQRAEqYsAAJlMhm+//RaPPfYYAPPIkUajwZw5czB//nwAQE1NDUJCQvDuu+9i5syZ4nuPHj2KcePGISMjA6GhoRbbqa2tRVRUFN588038+c9/bnTf//73vzFu3Djk5+dDo9EAADZs2IDp06ejuLgYPj4+Vh2DVquFr68vysrKrH6Ps6gPS3/fmYOrdWGpo58Ks4Z1ZlgiIiK7Zu33t6SX5+4kNzcXhYWFGDlypLhMqVRiyJAh2Lt3r7issrISkydPxqpVq6BWqxts5/Dhw7h8+TLkcjl69eqF0NBQjBkzxuIy3759+xAfHy8GJgAYNWoUampqkJmZ2WSNNTU10Gq1Fo/2ptpgxL/25GLQ0h1Y9MMpXC2vQUc/FRZPuB875g3FlERehiMiIudgt43ghYWFAICQkBCL5SEhIbh48aL4/MUXX0RSUhLGjx/f6HbOnz8PAFi4cCHef/99REVF4b333sOQIUPw+++/IyAgAIWFhQ324+/vDzc3N7GOxixZsgRvvvlmi47P0VXpjVh/4CI+2nke1ypujiy98FAMHu8dBjcXu83jRERELWK3oane7X1KgiCIy7Zs2YL09HQcOXKkyfebTCYAwIIFC8S+qTVr1iAsLAxfffWVeJmvsX6oW/fVmNdeew1z584Vn2u1WoSHh1t5ZI6pqbD0l4diMJFhiYiInJjdhqb6S22FhYUIDQ0VlxcXF4ujQunp6cjJyYGfn5/Fex9//HEMGjQIv/76q/jebt26ia8rlUp06tQJeXl54r4OHDhgsY3S0lIYDIYGI1C3UiqVUCqVLT9IB3IzLOXgWoX5V4Vh/iq8MIxhiYiI2ge7/aaLjo6GWq3G9u3bxWV6vR47d+5EUlISAODVV1/FsWPHkJWVJT4AYMWKFVizZg0AICEhAUqlEmfPnhW3YzAYcOHCBURGRgIABgwYgBMnTqCgoEBcJy0tDUqlEgkJCa19qHatUl+Lj3edx6Cl6Xhr62lcq9AjzF+Fdx839yw91S+CgYmIiNoFSUeaKioqkJ2dLT7Pzc1FVlYWAgICEBERgTlz5mDx4sWIjY1FbGwsFi9eDA8PD6SkpAAwjxA11vwdERGB6OhoAICPjw+ee+45/PWvf0V4eDgiIyOxbNkyAMCTTz4JABg5ciS6deuG1NRULFu2DCUlJZg3bx5mzJjR7n4FV69SX4vP9l/E6p3ncV1nHlkKD1DhL8NiMaF3R7gqGJSIiKh9kTQ0HTp0CMOGDROf1/cHTZs2DWvXrsUrr7yCqqoqzJo1C6WlpUhMTERaWlqDOZruZtmyZXBxcUFqaiqqqqqQmJiI9PR0+Pv7AwAUCgW2bt2KWbNmYeDAgVCpVEhJScHy5cttd7AOolJfi0/3XcQ/dt0MSxEBHnjhoRhM6MWwRERE7ZfdzNPkDBx5niaGJSIiaq+s/f6220Zwahu6mlp8ut8clkrqwlJkoAdeGBaDxxiWiIiIRAxN7ZSuphaf7LuIj3dbhqW/PBSLxx7QwIVhiYiIyAJDUztTUVOLT/ZdwMe7zqO00gAAiKoLS+MZloiIiJrE0NRONBaWooM88ZeHYvBoT4YlIiKiu2FocnIVNbVYt/cCPt59HjcYloiIiFqMoclJlVcbxJ6l+rDUKcgTfxkeg+QeDEtERETNxdDkZMqrDVi39wL+uSfXIizNHh6L5J4aKORN30uPiIiImsbQ5CTqw9LHu3NRVlUXljp44j+Hx2JcD4YlIiKie8XQ5OC01Qas+808slQfljp3MI8sMSwRERHZDkOTg9JWG7D2twv45+7z0FbXAmBYIiIiak0MTQ5GW23Amj0X8K89N8NSTLAXZg+Pxdj7QxmWiIiIWglDk4MoqzKPLN0elv5zeCweYVgiIiJqdQxNdq7aYMRHO3Pwrz25KK8LS7F1I0sMS0RERG2HocnOKeQyfHP4MsqraxEb7IX/fDgWj8SHQs6wRERE1KYYmuycq0KO18d1g77WhDHxaoYlIiIiiTA0OYAR3UKkLoGIiKjd4700iIiIiKzA0ERERERkBYYmIiIiIiswNBERERFZgaGJiIiIyAoMTURERERWYGgiIiIisgJDExEREZEVGJqIiIiIrMDQRERERGQFhiYiIiIiKzA0EREREVmBoYmIiIjICi5SF+BMBEEAAGi1WokrISIiImvVf2/Xf483haHJhsrLywEA4eHhEldCREREzVVeXg5fX98mX5cJd4tVZDWTyYQuXbogMzMTMpmsyfX69u2LjIwMq1/TarUIDw9Hfn4+fHx8bFpza7jT8dnT9lu6nea8z5p177ZOS17nOdM62+c5Yx9a+3yx5T6c4Zxp6jVnOmcEQUB5eTk0Gg3k8qY7lzjSZENyuRxubm53TKkAoFAomjzB7vSaj4+P3Z+YwJ2PwZ6239LtNOd91qx7t3Xu5XWeM7bdPs8Z+9Da54st9+EM58zd3uss58zdvrsBNoLb3PPPP39P61jzfnvX2sdgq+23dDvNed+9ng+2eN0R8Jxp3rrt/Zxpi/p5zrSsFntlq2Pg5TkHoNVq4evri7KyMrtP82QfeM5Qc/GcoeZqj+cMR5ocgFKpxF//+lcolUqpSyEHwXOGmovnDDVXezxnONJEREREZAWONBERERFZgaGJiIiIyAoMTURERERWYGgiIiIisgJDExEREZEVGJqczNmzZ/HAAw+ID5VKhc2bN0tdFtmx3NxcDBs2DN26dcP9998PnU4ndUlk51xcXMTPmGeeeUbqcshBVFZWIjIyEvPmzZO6lBbjlANOrKKiAlFRUbh48SI8PT2lLofs1JAhQ/DWW29h0KBBKCkpgY+PD1xceIclalpQUBCuXbsmdRnkYBYsWIBz584hIiICy5cvl7qcFuFIkxPbsmULhg8fzsBETTp58iRcXV0xaNAgAEBAQAADExHZ3Llz53DmzBk88sgjUpdyTxia2tiuXbuQnJwMjUYDmUzW6KWzDz/8ENHR0XB3d0dCQgJ2797don1t3LgRf/jDH+6xYpJSa58v586dg5eXFx599FH07t0bixcvtmH1JIW2+IzRarVISEjAgw8+iJ07d9qocpJKW5wz8+bNw5IlS2xUsXT4T8o2ptPp0LNnT/zxj3/E448/3uD1L7/8EnPmzMGHH36IgQMHYvXq1RgzZgxOnTqFiIgIAEBCQgJqamoavDctLQ0ajQaA+UPtt99+w4YNG1r3gKhVtfb5YjAYsHv3bmRlZSE4OBijR49G3759MWLEiFY/NmodbfEZc+HCBWg0Gpw4cQJjx47F8ePH2829x5xRa58zGRkZ6NKlC7p06YK9e/e2+vG0KoEkA0D49ttvLZb169dPeO655yyWxcXFCa+++mqztv3JJ58IU6ZMudcSyY60xvmyd+9eYdSoUeLzpUuXCkuXLr3nWsk+tOZnTL3Ro0cLGRkZLS2R7ExrnDOvvvqqEBYWJkRGRgqBgYGCj4+P8Oabb9qq5DbFy3N2RK/XIzMzEyNHjrRYPnLkyGanc16ac362OF/69u2LoqIilJaWwmQyYdeuXejatWtrlEt2wBbnTGlpqTiicOnSJZw6dQqdOnWyea1kH2xxzixZsgT5+fm4cOECli9fjhkzZuCNN95ojXJbHS/P2ZFr167BaDQiJCTEYnlISAgKCwut3k5ZWRkOHjyITZs22bpEsiO2OF9cXFywePFiDB48GIIgYOTIkRg3blxrlEt2wBbnzOnTpzFz5kzI5XLIZDL87W9/Q0BAQGuUS3bAVt9LzoKhyQ7JZDKL54IgNFh2J76+vigqKrJ1WWSn7vV8GTNmDMaMGWPrssiO3cs5k5SUhOPHj7dGWWTH7vVzpt706dNtVJE0eHnOjgQFBUGhUDRI78XFxQ1SPhHPF2ounjPUXDxnLDE02RE3NzckJCRg+/btFsu3b9+OpKQkiaoie8XzhZqL5ww1F88ZS7w818YqKiqQnZ0tPs/NzUVWVhYCAgIQERGBuXPnIjU1FX369MGAAQPwj3/8A3l5eXjuueckrJqkwvOFmovnDDUXz5lmkPS3e+3Qjh07BAANHtOmTRPX+d///V8hMjJScHNzE3r37i3s3LlTuoJJUjxfqLl4zlBz8ZyxHu89R0RERGQF9jQRERERWYGhiYiIiMgKDE1EREREVmBoIiIiIrICQxMRERGRFRiaiIiIiKzA0ERERERkBYYmIiIiIiswNBERAYiKisLKlSulLoOI7BhDExG1menTp+Oxxx6TuoxGZWRk4Nlnn231/URFRUEmk0Emk0GlUiEuLg7Lli1Dc2/OwJBH1PZ4w14icmoGgwGurq53Xa9Dhw5tUI3ZokWLMGPGDFRXV+Pnn3/Gf/zHf8DHxwczZ85ssxqIqPk40kREduPUqVN45JFH4OXlhZCQEKSmpuLatWvi69u2bcODDz4IPz8/BAYGYty4ccjJyRFfv3DhAmQyGTZu3IihQ4fC3d0dn332mTjCtXz5coSGhiIwMBDPP/88DAaD+N7bR25kMhn++c9/YsKECfDw8EBsbCy2bNliUe+WLVsQGxsLlUqFYcOGYd26dZDJZLhx48Ydj9Pb2xtqtRpRUVF45pln0KNHD6SlpYmv5+TkYPz48QgJCYGXlxf69u2Ln3/+WXx96NChuHjxIl588UVx1Kre3r17MXjwYKhUKoSHh2P27NnQ6XRW/39ARE1jaCIiu1BQUIAhQ4bggQcewKFDh7Bt2zYUFRVh0qRJ4jo6nQ5z585FRkYGfvnlF8jlckyYMAEmk8liW/Pnz8fs2bNx+vRpjBo1CgCwY8cO5OTkYMeOHVi3bh3Wrl2LtWvX3rGmN998E5MmTcKxY8fwyCOPYMqUKSgpKQFgDmhPPPEEHnvsMWRlZWHmzJlYsGBBs45ZEAT8+uuvOH36tMVoWEVFBR555BH8/PPPOHLkCEaNGoXk5GTk5eUBAL755huEhYVh0aJFKCgoQEFBAQDg+PHjGDVqFCZOnIhjx47hyy+/xJ49e/DCCy80qy4iaoJARNRGpk2bJowfP77R115//XVh5MiRFsvy8/MFAMLZs2cbfU9xcbEAQDh+/LggCIKQm5srABBWrlzZYL+RkZFCbW2tuOzJJ58U/vCHP4jPIyMjhRUrVojPAQj//d//LT6vqKgQZDKZ8O9//1sQBEGYP3++EB8fb7GfBQsWCACE0tLSxv8C6vbj5uYmeHp6Cq6urgIAwd3dXfjtt9+afI8gCEK3bt2EDz74oMl6BUEQUlNThWeffdZi2e7duwW5XC5UVVXdcftEdHccaSIiu5CZmYkdO3bAy8tLfMTFxQGAeAkuJycHKSkp6NSpE3x8fBAdHQ0A4ghMvT59+jTYfvfu3aFQKMTnoaGhKC4uvmNNPXr0EP/s6ekJb29v8T1nz55F3759Ldbv16+fVcf68ssvIysrCzt37sSwYcOwYMECJCUlia/rdDq88sor6NatG/z8/ODl5YUzZ840OM7bZWZmYu3atRZ/h6NGjYLJZEJubq5VtRFR09gITkR2wWQyITk5Ge+++26D10JDQwEAycnJCA8Px8cffwyNRgOTyYT4+Hjo9XqL9T09PRts4/ZmcJlM1uCyXnPeIwiCRS9R/TJrBAUFISYmBjExMdi0aRNiYmLQv39/PPzwwwDMoeqnn37C8uXLERMTA5VKhSeeeKLBcd7OZDJh5syZmD17doPXIiIirKqNiJrG0EREdqF3797YtGkToqKi4OLS8KPp+vXrOH36NFavXo1BgwYBAPbs2dPWZYri4uLw448/Wiw7dOhQs7fj7++Pv/zlL5g3bx6OHDkCmUyG3bt3Y/r06ZgwYQIAc4/ThQsXLN7n5uYGo9Fosax37944efIkYmJiml0HEd0dL88RUZsqKytDVlaWxSMvLw/PP/88SkpKMHnyZBw8eBDnz59HWloa/vSnP8FoNMLf3x+BgYH4xz/+gezsbKSnp2Pu3LmSHcfMmTNx5swZzJ8/H7///js2btwoNpbfPgJ1N88//zzOnj2LTZs2AQBiYmLwzTffICsrC0ePHkVKSkqDUbGoqCjs2rULly9fFn9hOH/+fOzbtw/PP/88srKycO7cOWzZsgV/+ctf7v2AiYihiYja1q+//opevXpZPN544w1oNBr89ttvMBqNGDVqFOLj4/Gf//mf8PX1hVwuh1wux4YNG5CZmYn4+Hi8+OKLWLZsmWTHER0dja+//hrffPMNevTogb///e/ir+eUSmWzttWhQwekpqZi4cKFMJlMWLFiBfz9/ZGUlITk5GSMGjUKvXv3tnjPokWLcOHCBXTu3FmcY6pHjx7YuXMnzp07h0GDBqFXr154/fXXxcubRHRvZIK1F+GJiOiO3n77bXz00UfIz8+XuhQiagXsaSIiaqEPP/wQffv2RWBgIH777TcsW7aMcyIROTGGJiKiFjp37hzeeustlJSUICIiAi+99BJee+01qcsiolbCy3NEREREVmAjOBEREZEVGJqIiIiIrMDQRERERGQFhiYiIiIiKzA0EREREVmBoYmIiIjICgxNRERERFZgaCIiIiKyAkMTERERkRX+P+P4FsBa/hE7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "with tempfile.TemporaryDirectory() as base_dir:\n",
    "    \n",
    "    config_path = '../configs/cifar10/bt_test_config.yaml'\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    # config.model_type = 'sparse_head_barlow_twins'\n",
    "    # config.sparsity_level=10\n",
    "    # config.epochs=100\n",
    "    # config.save_interval=100\n",
    "\n",
    "\n",
    "    experiment_dir,experiment_hash = main_bt_experiment(config,base_dir)\n",
    "    \n",
    "    print(os.listdir(experiment_dir))\n",
    "    print(os.listdir(base_dir))\n",
    "    print('experiment_dir and base_dir')\n",
    "\n",
    "\n",
    "    experiment_dir,experiment_hash = main_bt_experiment(config,base_dir)\n",
    "    print(os.listdir(experiment_dir))\n",
    "    print(os.listdir(base_dir))\n",
    "    print('experiment_dir and base_dir')\n",
    "\n",
    "    #get path to fully fitted model\n",
    "    path = os.path.join(experiment_dir,f'trained_model_epoch_{config.epochs-1}.pth')\n",
    "    model = load_barlow_model(arch=config.arch,ps=config.ps,hs=config.hs,path=path)\n",
    "    print(model)\n",
    "\n",
    "    #New config but the first part of experiment_dir is same - just hash is different\n",
    "    #It shouldnt find a max file path\n",
    "    config.epochs=config.epochs+1\n",
    "    experiment_dir,experiment_hash = main_bt_experiment(config,base_dir)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
