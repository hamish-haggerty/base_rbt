{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972f0960",
   "metadata": {},
   "source": [
    "## Establishing baseline on CIFAR10 with BT  (just about done)\n",
    "\n",
    "Findings: Using similar augmentation parameters to the original BT paper (tested on ImageNet) gives sub-optimal performance on CIFAR10. For example, noticeable performance gains came from reducing the random gaussian blur probability on one branch from 1.0 to 0.5. \n",
    "\n",
    "Using augmentations close to that in the original BT paper gives performance around 0.6 on CIFAR10 (pre-training on entire unlabelled dataset and fine tuning with 1% of labels); with a bit bit of experimentation with the augmentations we can get performance around 0.66.\n",
    "\n",
    "On ImageNet the authors got 55% when using 1% of labels, so the baseline results seem about right - ImageNet is much larger dataset, but they used ResNet with 50 layers (I'm using ResNet with 34 layers), but you would still expect to get better performance on CIFAR10. Visually, the augmentations look reasonable too.\n",
    "\n",
    "This is training for 400 epochs - need to run BT for 1000 epochs to get final baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f25908",
   "metadata": {},
   "source": [
    "## Plan going forward:\n",
    "\n",
    "\n",
    "* Finish training final BT baseline on CIFAR10 (just about done with this).\n",
    "* Concurrent with above: final cross-validation / hyperparameter search on MNIST (e.g. find best sinusoid phase for self-supervised training which then gives best results when fine-tuning linear classifier on top). These hps will be used on CIFAR10 and one other dataset (TinyImageNet). Note that it is \"cheating\" to perform this step on CIFAR10 if we are including CIFAR10 results in the paper. So we have to perform hp search (via cross validation) on MNIST to choose the hps, then we can use them on different datasets. \n",
    "* Train and test RBT (`regularization` modification to BT) and RABT (idea for new architecture and also uses independence regularization) on CIFAR10, comparing to baseline. If results are good, then go to next step; If results are not good, this will be very annoying as have put a large amount of time into this. Will have to then modify direction for masters thesis (it is due end of sem1 next year ...). Hopefully can still build on work I've done. \n",
    "* If good results on CIFAR10 next step is to test on one other dataset - probably TinyImageNet. This can be done with all the hps from CIFAR10 as is, so won't take too long - can basically run code as is. If good results here, then we have all the results need to write paper / thesis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
