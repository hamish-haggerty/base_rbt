{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_rbt\n",
    "\n",
    "> Base functions and classes we use for our hacking on BT / RBT and related ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "!pip install git+https://github.com/hamish-haggerty/base_rbt.git#egg='base_rbt'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing, import like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_rbt.base_model import *\n",
    "from base_rbt.base_lf import *\n",
    "from base_rbt.base_linear import *\n",
    "from base_rbt.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some other libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_supervised\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we give an end to end example. There are only a couple of steps. We first we need a dls i.e. a dataloader; then patch in our own definition of a loss funtion `lf`. Then it is a simple matter of defining an augmentation pipeline and fitting the model. We go through each of these now.\n",
    "First, get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some MNIST data and plonk it into a dls\n",
    "path = untar_data(URLs.MNIST)\n",
    "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
    "items = items.shuffle()\n",
    "items = items[0:10]\n",
    "split = RandomSplitter(valid_pct=0.0)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items))\n",
    "dls = tds.dataloaders(bs=5,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to construct the `lf`. Here is a (silly!) modification to the BT loss function. We are just scaling the bt loss function by $0.01$. However, this illustrates the general API if we want to modify the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return 0.01*lf_bt(pred, self.I,self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we patch in our own definition of a loss function, using the tools from `base_lf`.  First define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def lf_rbt(pred,seed,I,lmb):\n",
    "    \n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #All standard, from BT\n",
    "    z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "    C = (z1norm.T @ z2norm) / bs \n",
    "    cdiff = (C - I)**2\n",
    "\n",
    "    #Get either max corr(f(x),g(y)) {if indep=True} or max 0.5*corr(x,g(y)) + 0.5*corr(f(x),y), {if indep=False}\n",
    "    #where the max is over f and g. Please see base_lf for details\n",
    "    CdiffSup = Cdiff_Sup(I=I,qs=ps,inner_steps=5,indep=False)\n",
    "    cdiff_2 = CdiffSup(z1norm,z2norm) #same shape as cdiff\n",
    "\n",
    "    #As above but f and g are now randomly sampled sinusoid. Please see base_lf for details\n",
    "    CdiffRand = Cdiff_Rand(seed=seed,std=0.1,K=2,indep=True)\n",
    "    cdiff_2_2 = CdiffRand(z1norm,z2norm) #same shape as cdiff\n",
    "\n",
    "    cdiff_2 = 0.5*cdiff_2_2 + 0.5*cdiff_2 #convex combination of rand and sup terms.\n",
    "\n",
    "    rr = cdiff_2*(1-I)*lmb #redundancy reduction term (scaled by lmb)\n",
    "\n",
    "    loss = (cdiff*I + rr).sum() #sum of redundancy reduction term and invariance term\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def lf_rat(pred,I,lmb):\n",
    "    \n",
    "    bs,nf = pred.size(0)//2,pred.size(1)\n",
    "    \n",
    "    pred1=pred[0]\n",
    "    pred2=pred[0]\n",
    "\n",
    "    z1, z2 = pred1[:bs],pred1[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "    #Used to encode, primarily invariance\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    #Used to encode, primarily redundancy-reduction\n",
    "    z1_two,z2_two = pred2[:bs],pred2[bs:]\n",
    "    z1norm_two = (z1_two - z1_two.mean(0)) / z1_two.std(0, unbiased=False)\n",
    "    z2norm_two = (z2_two - z2_two.mean(0)) / z2_two.std(0, unbiased=False)\n",
    "\n",
    "    #The invariance term\n",
    "    Invar = (z1norm-z2norm).pow(2) #add to loss (there are d-terms)\n",
    "\n",
    "    #The redundancy reduction term\n",
    "    CdiffRand = Cdiff_Rand(seed=0,std=0.1,K=2,indep=True)\n",
    "    cdiff = CdiffRand(z1norm_two,z2norm_two)\n",
    "    CdiffSup = Cdiff_Sup(I=I,qs=ps,inner_steps=5,indep=False)\n",
    "    cdiff_2 = CdiffSup(z1norm_two,z2norm_two)\n",
    "    redun_reduc = 0.5*cdiff + 0.5*cdiff_2 #add to loss\n",
    "\n",
    "    #Make the reps different term\n",
    "    CdiffRand = Cdiff_Rand(seed=0,std=0.1,K=2,indep=True)\n",
    "    cdiff1  = CdiffRand(z1norm,z1norm_two)\n",
    "    CdiffSup = Cdiff_Sup(I=I,qs=ps,inner_steps=5,indep=False)\n",
    "    cdiff11 = CdiffSup(z1norm,z1norm_two)\n",
    "    cdiff1 = 0.5*cdiff1 + 0.5*cdiff11\n",
    "\n",
    "            #d terms                   #d^2 + d^2 terms\n",
    "    loss = Invar.sum() + self.lmb*(0.5*redun_reduc + 0.5*cdiff).sum() #Have to work out scaling constants (grid search?)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function has both a `random` component and a `sup` component.\n",
    "Next patch it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @patch\n",
    "# def lf(self:BarlowTwins, pred,*yb): return lf_rbt(pred,seed=self.seed,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return lf_rat(pred,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need an augmentation pipeline. Let's also take a look at what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in=1\n",
    "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1)\n",
    "\n",
    "#model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
    "model = create_p2barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
    "\n",
    "\n",
    "aug_pipelines_1 = get_barlow_twins_aug_pipelines(size=28,\n",
    "                    rotate=False,jitter=False,bw=False,blur=True,solar=False, #Whether to use aug or not\n",
    "                    resize_scale=(0.5, 1.0),resize_ratio=(3/4, 4/3), rotate_deg=45,blur_s=11,s1=3,sol_t=0.05,sol_a=0.05, #hps of augs\n",
    "                    flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.5,sol_p=0.1, #prob of performing aug\n",
    "                    same_on_batch=False,stats=mnist_stats, cuda=(device=='cuda'))\n",
    "\n",
    "aug_pipelines_2 = get_barlow_twins_aug_pipelines(size=28,\n",
    "                    rotate=False,jitter=False,bw=False,blur=True,solar=False, #Whether to use aug or not\n",
    "                    resize_scale=(0.5, 1.0),resize_ratio=(3/4, 4/3), rotate_deg=45,blur_s=11,s1=3,sol_t=0.05,sol_a=0.05, #hps of augs\n",
    "                    flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.5,sol_p=0.1, #prob of performing aug\n",
    "                    same_on_batch=False,stats=mnist_stats, cuda=(device=='cuda'))\n",
    "\n",
    "aug_pipelines = [aug_pipelines_1,aug_pipelines_1]\n",
    "tem = BarlowTwins(aug_pipelines,n_in=n_in,print_augs=True)\n",
    "learn = Learner(dls,model, cbs=[tem])\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,let's train RBT; We construct an encoder and a learner, then fit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full usage of above\n",
    "ps=500 #projection size\n",
    "hs=ps #hidden size in mlp at the end; typically just = ps. \n",
    "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1) #create the encoder\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)#plonk the projector on the end of the encoder\n",
    "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines,n_in=1, print_augs=False)]) #build the learner\n",
    "#learn.fit(1) #train model, i.e. weights of encoder and projector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the `fastai_encoder` can evaluate in various ways. e.g. linear evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok now let's do an end to end CIFAR10 example. Let's comment a little more on the API as we go - this will be helpful when considering how to add functionality to (in particular) base_model**\n",
    "\n",
    "Here are the steps at a high level:\n",
    "- Define hps (e.g. batch size, projector dimension etc). Note that if the model changes then there may be different hps\n",
    "- Get the data (train, tune, test) -> dls,dls_val, dls_test\n",
    "- Patch in loss function definition\n",
    "- Setup/define augmentations, encoder, model, learner\n",
    "- Train BT/RBT (i.e. fit the learner\n",
    "- Train linear classifier and record performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1): We need the data, and to set all the hps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hps's\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cuda = (device=='cuda')\n",
    "seed=42\n",
    "n_in=3\n",
    "indim=1024 #find this by inspection, e.g. for resnet18 is 1024\n",
    "size=32\n",
    "ps=100\n",
    "seed=42 \n",
    "bs=64 #for training BT\n",
    "bs_val=128 #for training linear head\n",
    "bs_test=256 #for evaluating linear head\n",
    "ts=bs*2#00\n",
    "ts_val=bs_val*5\n",
    "ts_test=5*bs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data: dls, dls_val, dls_test\n",
    "#get the data: Need dls, dls_val, dls_test\n",
    "path = untar_data(URLs.CIFAR)\n",
    "fnames = get_image_files(path / \"train\")\n",
    "fnames.sort()\n",
    "#shuffle data (in reproducible way)\n",
    "seed_everything(seed=seed)\n",
    "fnames = fnames.shuffle()\n",
    "\n",
    "#fnames for train, eval and test\n",
    "fnames_train = fnames[0:ts]\n",
    "fnames_val = fnames[ts:ts+ts_val]\n",
    "fnames_test = fnames[ts+ts_val:ts+ts_val+ts_test]\n",
    "\n",
    "def label_func(fname):\n",
    "    return fname.name.split('_')[1].strip('png').strip('.')\n",
    "\n",
    "#labels for train,eval and test\n",
    "labels = [label_func(fname) for fname in fnames]\n",
    "labels_train = labels[0:ts]\n",
    "labels_val = labels[ts:ts+ts_val]\n",
    "labels_test = labels[ts+ts_val:ts+ts_val+ts_test]\n",
    "\n",
    "#Used for training encoder i.e. BT\n",
    "dls = ImageDataLoaders.from_lists(path, fnames_train, labels_train,bs=bs, item_tfms=[Resize(size=size)], #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=12,device=device,seed=seed)\n",
    "#Used for training linear classifier\n",
    "dls_val = ImageDataLoaders.from_lists(path, fnames_val, labels_val,bs=bs_val, item_tfms=[Resize(size=size)], #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=12,device=device,seed=seed)\n",
    "\n",
    "#Used for evaluating linear classifier\n",
    "dls_test = ImageDataLoaders.from_lists(path, fnames_test, labels_test,bs=bs_val, item_tfms=[Resize(size=size)], #batch_tfms=[ToTensor(), IntToFloatTensor()],\n",
    "                                  valid_pct=0.0,num_workers=12,device=device,seed=seed)\n",
    "\n",
    "\n",
    "set(labels) #Check that the labels make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2): Patch in definition of loss function, and also `after_epoch` (where we train linear classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using BT\n",
    "# @patch\n",
    "# def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred, self.I,self.lmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_rat(pred,I,lmb):\n",
    "    \n",
    "    bs,nf = bs,nf = pred[0].size(0)//2,pred[0].size(1)\n",
    "    \n",
    "    pred1=pred[0]\n",
    "    pred2=pred[1]\n",
    "    \n",
    "    z1, z2 = pred1[:bs],pred1[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "    #Used to encode, primarily invariance\n",
    "    z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "\n",
    "    #Used to encode, primarily redundancy-reduction\n",
    "    z1_two,z2_two = pred2[:bs],pred2[bs:]\n",
    "    z1norm_two = (z1_two - z1_two.mean(0)) / z1_two.std(0, unbiased=False)\n",
    "    z2norm_two = (z2_two - z2_two.mean(0)) / z2_two.std(0, unbiased=False)\n",
    "\n",
    "    #The invariance term\n",
    "    Invar = (z1norm-z2norm).pow(2) #add to loss (there are d-terms)\n",
    "\n",
    "    #The redundancy reduction term\n",
    "    CdiffRand = Cdiff_Rand(seed=0,std=0.1,K=2,indep=True)\n",
    "    cdiff = CdiffRand(z1norm_two,z2norm_two)\n",
    "    CdiffSup = Cdiff_Sup(I=I,qs=ps,inner_steps=5,indep=False)\n",
    "    cdiff_2 = CdiffSup(z1norm_two,z2norm_two)\n",
    "    redun_reduc = 0.5*cdiff + 0.5*cdiff_2 #add to loss\n",
    "\n",
    "    #Make the reps different term\n",
    "    CdiffRand = Cdiff_Rand(seed=0,std=0.1,K=2,indep=True)\n",
    "    cdiff1  = CdiffRand(z1norm,z1norm_two)\n",
    "    CdiffSup = Cdiff_Sup(I=I,qs=ps,inner_steps=5,indep=False)\n",
    "    cdiff11 = CdiffSup(z1norm,z1norm_two)\n",
    "    cdiff1 = 0.5*cdiff1 + 0.5*cdiff11\n",
    "\n",
    "            #d terms                   #d^2 + d^2 terms\n",
    "    loss = Invar.sum() + lmb*(0.5*redun_reduc + 0.5*cdiff).sum() #Have to work out scaling constants (grid search?)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def lf(self:BarlowTwins, pred,*yb): return lf_rat(pred,I=self.I,lmb=self.lmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch in `before_epoch` callback - perform linear evaluation every 200th epoch (say):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipelines_val=[get_linear_batch_augs(size=size,stats=cifar_stats,resize=True,resize_scale=(0.3, 1.0))]\n",
    "main_linear_eval = Main_Linear_Eval(size=size,n_in=n_in,numfit=1,indim=1024, #size,n_in=3 (color channels),number of epochs to fit linear, and output dimension of encoder\n",
    "                    dls_val=dls_val,dls_test=dls_test, #dls for training linear and evaluating linear\n",
    "                    stats=cifar_stats,\n",
    "                    aug_pipelines_val=aug_pipelines_val, #aug_pipeline for training \n",
    "                    encoder=None#encoder\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def after_epoch(self:BarlowTwins):\n",
    "\n",
    "    #Put in eval mode and turn gradients off\n",
    "    self.learn.eval()\n",
    "    grad_on(self.learn.model,on=False)\n",
    "    \n",
    "    #Test in eval mode\n",
    "    test_eq(self.learn.model.training,False)\n",
    "    #Test gradients off\n",
    "    test_grad_off(self.learn.model)\n",
    "\n",
    "    ##\n",
    "    main_linear_eval.encoder = self.learn.model.encoder #Update the encoder \n",
    "    main_linear_eval.model = LinearModel(encoder=main_linear_eval.encoder,indim=indim) #update the model (frozen encoder + head)\n",
    "    acc = main_linear_eval() #train linear head on frozen encoder and get accuracy on test set\n",
    "    self.acc_dict[self.epoch]=acc #update the acc_dict\n",
    "    ##\n",
    "    \n",
    "    #Put in train mode and turn gradients back on\n",
    "    self.learn.train()\n",
    "    grad_on(self.learn.model,on=True)\n",
    "\n",
    "    #Test training mode on\n",
    "    test_eq(self.learn.model.training,True)\n",
    "    #Test gradients on\n",
    "    test_grad_on(self.learn.model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3): Define encoder and model; Define augmentation pipelines; Define learner.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=n_in)\n",
    "\n",
    "#If we are using a different model, this call will just look like `create_rat_model(...)`\n",
    "model = create_p2barlow_twins_model(fastai_encoder, hidden_size=ps,projection_size=ps,nlayers=3)\n",
    "test(model.training,True,cmp=operator.eq,cname='model not in training mode')\n",
    "\n",
    "aug_pipelines_1 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                                                 bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n",
    "                                                 resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,\n",
    "                                                 bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.5,sol_p=0.0,\n",
    "                                                 stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n",
    "                                                 )\n",
    "\n",
    "aug_pipelines_2 = get_barlow_twins_aug_pipelines(size=size,\n",
    "                                                 bw=True, rotate=True,noise=True, jitter=True, blur=True,solar=True,\n",
    "                                                 resize_scale=(0.4, 1.0),rotate_deg=45,noise_std=0.0125, jitter_s=1.0, blur_s=math.ceil(size/10)+1,sol_t=0.01,sol_a=0.01,\n",
    "                                                 bw_p=0.2, flip_p=0.5,rotate_p=0.25,noise_p=0.5, jitter_p=0.5, blur_p=0.1,sol_p=0.2,\n",
    "                                                 stats=cifar_stats,same_on_batch=False, xtra_tfms=[]\n",
    "                                                 )\n",
    "\n",
    "aug_pipelines = [aug_pipelines_1,aug_pipelines_2]\n",
    "\n",
    "aug_pipelines = [aug_pipelines_1,aug_pipelines_2]\n",
    "\n",
    "#If we are using a different `callback` to `BarlowTwins` then we can simply replace `BarlowTwins` with \n",
    "#e.g. `BarlowTriplets`. We can define in base_model and just import with no issues. \n",
    "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines,n_in=n_in,lmb=1/ps,print_augs=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3): (Optional): View the augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bt_batch(dls=dls,n_in=n_in,aug=aug_pipelines,n=20,print_augs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4): Fit the learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5): Setup for linear evaluation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipelines_val=[get_linear_batch_augs(size=size,stats=cifar_stats,resize_scale=(0.3, 1.0))]\n",
    "fastai_encoder.eval()\n",
    "grad_on(fastai_encoder,on=False)\n",
    "main_linear_eval = Main_Linear_Eval(size=size,n_in=n_in,numfit=1,indim=1024, #size,n_in=3 (color channels),number of epochs to fit linear, and output dimension of encoder\n",
    "                        dls_val=dls_val,dls_test=dls_test, #dls for training linear and evaluating linear\n",
    "                        stats=cifar_stats,\n",
    "                        aug_pipelines_val=aug_pipelines_val, #aug_pipeline for training \n",
    "                        encoder=fastai_encoder #encoder\n",
    "                                   )\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6): (optional): View validation augs (generally just cropping and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_linear_batch(dls=dls_val,n_in=n_in,n=2,aug=aug_pipelines_val,print_augs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7): (optional): View validation augs (generally just cropping and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=main_linear_eval()\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
