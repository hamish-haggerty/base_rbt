{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32189ee",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> utility stuff.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e903be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39912107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.test import *\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from typing import Literal\n",
    "import random \n",
    "import os \n",
    "import yaml\n",
    "import numpy as np\n",
    "import yaml\n",
    "import configparser\n",
    "from types import SimpleNamespace\n",
    "import importlib\n",
    "from nbdev import config\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# cfg = config.get_config()\n",
    "# PACKAGE_NAME = cfg.lib_name\n",
    "PACKAGE_NAME = 'base_rbt' #hardcoded for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_grad_on(model):\n",
    "    \"\"\"\n",
    "    Test that all grads are on for modules with parameters.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Check each parameter in the module\n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            assert param.requires_grad, f\"Gradients are off for {name}.{param_name}\"\n",
    "\n",
    "def test_grad_off(model):\n",
    "    \"\"\"\n",
    "    Test that all non-batch norm grads are off, but batch norm grads are on.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Distinguish between BatchNorm and other layers\n",
    "        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert param.requires_grad, f\"BatchNorm parameter does not require grad in {name}.{param_name}\"\n",
    "        else:\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert not param.requires_grad, f\"Gradients are on for non-BatchNorm layer {name}.{param_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjust_config_with_derived_values(config):\n",
    "    # Adjust n_in based on dataset\n",
    "\n",
    "    # Adjust encoder_dimension based on architecture\n",
    "        \n",
    "    if config.arch == 'smallres':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet18':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet34':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet50':\n",
    "        config.encoder_dimension = 2048\n",
    "\n",
    "    else :\n",
    "        raise ValueError(f\"Architecture {config.arch} not supported\")\n",
    "\n",
    "    for key, value in list(config.__dict__.items()): \n",
    "        if value == 'none':\n",
    "            config.__dict__[key] = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        config = SimpleNamespace(**config)\n",
    "        config = adjust_config_with_derived_values(config)\n",
    "        \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def pretty_print_ns(ns):\n",
    "    \"\"\"\n",
    "    Pretty print a SimpleNamespace object\n",
    "    \"\"\"\n",
    "    for key, value in ns.__dict__.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d83ee",
   "metadata": {},
   "source": [
    "Basic network mostly used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _SmallRes(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Minimal Intermediate layer\n",
    "        self.intermediate_conv = nn.Conv2d(64, 512, kernel_size=1, stride=1, bias=False)\n",
    "        self.intermediate_bn = nn.BatchNorm2d(512)\n",
    "        self.intermediate_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Last two layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Minimal Intermediate layer\n",
    "        x = self.intermediate_conv(x)\n",
    "        x = self.intermediate_bn(x)\n",
    "        x = self.intermediate_relu(x)\n",
    "\n",
    "        # Last two layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def get_resnet_encoder(model,n_in=3):\n",
    "    model = create_body(model, n_in=n_in, pretrained=False, cut=len(list(model.children()))-1)\n",
    "    model.add_module('flatten', torch.nn.Flatten())\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def resnet_arch_to_encoder(arch: Literal['smallres','resnet18', 'resnet34', 'resnet50'],\n",
    "                           weight_type: Literal['random', 'imgnet_bt_pretrained', 'imgnet_sup_pretrained',\n",
    "                                               'dermnet_bt_pretrained','imgnet_bt_dermnet_bt_pretrained'] = 'random'):\n",
    "    \"\"\"Given a ResNet architecture, return the encoder configured for 3 input channels.\n",
    "       The 'weight_type' argument specifies the weight initialization strategy.\n",
    "\n",
    "    Args:\n",
    "        arch (Literal['smallres','resnet18', 'resnet34', 'resnet50']): The architecture of the ResNet.\n",
    "        weight_type (Literal['random', 'imgnet_bt_pretrained', 'imgnet_bt_dermnet_bt_pretrained','imgnet_sup_pretrained','imgnet_bt_ufes_bt_pretrained']): Specifies the weight initialization strategy. Defaults to 'random'.\n",
    "\n",
    "    Returns:\n",
    "        Encoder: An encoder configured for 3 input channels and specified architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_in=3\n",
    "\n",
    "    if weight_type == 'imgnet_bt_pretrained': test_eq(arch,'resnet50')\n",
    "    \n",
    "    if arch == 'resnet50':\n",
    "\n",
    "        if  weight_type ==  'imgnet_bt_pretrained':\n",
    "            _model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "\n",
    "        elif weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        else: #means we are loading state_dict from elsewhere\n",
    "            _model = resnet50() \n",
    "        \n",
    "    elif arch == 'resnet34':\n",
    "\n",
    "        if weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet34() \n",
    "\n",
    "    elif arch == 'resnet18':\n",
    "        if weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) \n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet18()\n",
    "\n",
    "        elif weight_type == 'cifar10_pretrained':\n",
    "            _model = resnet18() #we load them in elsewhere\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif arch == 'smallres':\n",
    "        _model = _SmallRes()\n",
    "    \n",
    "        \n",
    "    else: raise ValueError('Architecture not recognized')\n",
    "\n",
    "    return get_resnet_encoder(_model,n_in) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d288f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generate_config_hash(config):\n",
    "    \"\"\"\n",
    "    Generates a unique hash for a given experiment configuration.\n",
    "    \n",
    "    Args:\n",
    "    config (dict or Namespace): Experiment configuration. Can be a dictionary or a namespace object.\n",
    "    \n",
    "    Returns:\n",
    "    str: A unique hash representing the experiment configuration.\n",
    "    \"\"\"\n",
    "    # Convert config to dict if it's a Namespace\n",
    "    config_dict = vars(config) if not isinstance(config, dict) else config\n",
    "    \n",
    "    # Serialize configuration to a sorted JSON string to ensure consistency\n",
    "    config_str = json.dumps(config_dict, sort_keys=True)\n",
    "    \n",
    "    # Generate SHA-256 hash from the serialized string\n",
    "    hash_obj = hashlib.sha256(config_str.encode())  # Encode to convert string to bytes\n",
    "    config_hash = hash_obj.hexdigest()\n",
    "    \n",
    "    # Optionally, return a truncated version of the hash for readability\n",
    "    short_hash = config_hash[:8]  # Use the first 8 characters as an example\n",
    "    return short_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879decf",
   "metadata": {},
   "source": [
    "Test `generate_config_hash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = SimpleNamespace(arch='resnet18', dataset='cifar10', n_in=3, encoder_dimension=512)\n",
    "config2 = SimpleNamespace(arch='resnet34', dataset='cifar10', n_in=3, encoder_dimension=512)\n",
    "config3 = SimpleNamespace(arch='resnet50', dataset='cifar10', n_in=3, encoder_dimension=2048)\n",
    "\n",
    "test_eq(generate_config_hash(config1), generate_config_hash(config1))\n",
    "test_ne(generate_config_hash(config1), generate_config_hash(config2))\n",
    "test_ne(generate_config_hash(config1), generate_config_hash(config3))\n",
    "test_ne(generate_config_hash(config2), generate_config_hash(config3))\n",
    "\n",
    "config4 = SimpleNamespace(dataset='cifar10', arch='resnet18', n_in=3, encoder_dimension=512)  # Different order\n",
    "test_eq(generate_config_hash(config1), generate_config_hash(config4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_experiment_directory(base_dir, config):\n",
    "    # Generate a unique hash for the configuration\n",
    "    unique_hash = generate_config_hash(config)\n",
    "    \n",
    "    # Construct the directory path for this experiment\n",
    "    experiment_dir = os.path.join(base_dir, config.train_type, config.dataset, config.arch, unique_hash)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    \n",
    "    return experiment_dir,unique_hash\n",
    "\n",
    "\n",
    "def save_configuration(config, experiment_dir):\n",
    "    \"\"\"\n",
    "    Saves the experiment configuration as a YAML file in the experiment directory.\n",
    "\n",
    "    Args:\n",
    "    config (dict, Namespace, or any serializable object): Experiment configuration.\n",
    "    experiment_dir (str): Path to the directory where the config file will be saved.\n",
    "    \"\"\"\n",
    "    config_file_path = os.path.join(experiment_dir, 'config.yaml')\n",
    "    \n",
    "    # Check if config is not a dictionary (e.g., a Namespace object) and convert if necessary\n",
    "    config_dict = vars(config) if not isinstance(config, dict) else config\n",
    "    \n",
    "    with open(config_file_path, 'w') as file:\n",
    "        yaml.dump(config_dict, file)\n",
    "    \n",
    "    print(f\"Configuration saved to {config_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_metadata_file(experiment_dir, git_commit_hash):\n",
    "    \"\"\"\n",
    "    Saves a metadata file with the Git commit hash\n",
    "    \"\"\"\n",
    "    metadata_file_path = os.path.join(experiment_dir, 'metadata.yaml')\n",
    "    metadata_content = {\n",
    "        \"Git Commit Hash\": git_commit_hash,\n",
    "    }\n",
    "\n",
    "    with open(metadata_file_path, 'w') as file:\n",
    "        yaml.dump(metadata_content, file)\n",
    "\n",
    "    print(f\"Metadata saved to {metadata_file_path}\")\n",
    "\n",
    "\n",
    "def update_experiment_index(project_root, details):\n",
    "    central_json_path = os.path.join(project_root, 'experiment_index.json')\n",
    "    \n",
    "    if os.path.exists(central_json_path):\n",
    "        with open(central_json_path, 'r') as file:\n",
    "            experiments_index = json.load(file)\n",
    "    else:\n",
    "        experiments_index = {}\n",
    "    \n",
    "    experiment_hash = details[\"experiment_hash\"]\n",
    "    experiments_index[experiment_hash] = details\n",
    "    \n",
    "    with open(central_json_path, 'w') as file:\n",
    "        json.dump(experiments_index, file, indent=4)\n",
    "    \n",
    "    print(f\"Updated experiment index for hash: {experiment_hash}\")\n",
    "\n",
    "\n",
    "def get_latest_commit_hash(repo_path):\n",
    "    try:\n",
    "        commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=repo_path).decode('ascii').strip()\n",
    "        return commit_hash\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error obtaining latest commit hash: {e}\")\n",
    "        return None\n",
    "\n",
    "def setup_experiment(config,base_dir):\n",
    "\n",
    "    # Create a unique directory for this experiment based on its configuration\n",
    "    # This directory will contain all artifacts related to the experiment, such as model checkpoints and logs.\n",
    "    experiment_dir, experiment_hash = create_experiment_directory(base_dir, config)\n",
    "\n",
    "    print(f\"The experiment_dir is: {experiment_dir} and the experiment hash is: {experiment_hash}\")\n",
    "\n",
    "    # Save the loaded configuration to the experiment directory as a YAML file\n",
    "    # This ensures that we can reproduce or analyze the experiment later.\n",
    "    save_configuration(config, experiment_dir)\n",
    "\n",
    "    git_commit_hash = get_latest_commit_hash('.')\n",
    "    print(f\"The git hash is: {git_commit_hash}\")\n",
    "\n",
    "    return experiment_dir, experiment_hash,git_commit_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class InterruptCallback(Callback):\n",
    "    def __init__(self, interrupt_epoch):\n",
    "        super().__init__()\n",
    "        self.interrupt_epoch = interrupt_epoch\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if self.epoch == self.interrupt_epoch:\n",
    "            print(f\"Interrupting training before starting epoch {self.interrupt_epoch}\")\n",
    "            raise CancelFitException\n",
    "\n",
    "class SaveLearnerCheckpoint(Callback):\n",
    "    def __init__(self, experiment_dir,start_epoch=0, save_interval=250, with_opt=True):\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.start_epoch = start_epoch\n",
    "        self.save_interval = save_interval\n",
    "        self.with_opt = with_opt  # Decide whether to save optimizer state as well.\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if (self.epoch+1) % self.save_interval == 0 and self.epoch>=self.start_epoch:\n",
    "            print(f\"Saving model and learner state at epoch {self.epoch}\")\n",
    "   \n",
    "            checkpoint_filename = f\"learner_checkpoint_epoch_{self.epoch}\"\n",
    "            checkpoint_path = os.path.join(self.experiment_dir, checkpoint_filename)\n",
    "            # Save the entire learner object, including the model's parameters and optimizer state.\n",
    "            self.learn.save(checkpoint_path, with_opt=self.with_opt)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_number(filename):\n",
    "    \"\"\"Extract the number from end of  filename. e.g. `epoch`\"\"\"\n",
    "    #pattern = re.compile(r\"_epoch_(\\d+)\\.pt[h]?\")\n",
    "    pattern = re.compile(r\"_(\\d+)\\.pt[h]?\")\n",
    "    match = pattern.search(filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def find_largest_file(directory_path):\n",
    "    \"\"\"Find the file with the largest number (e.g. epoch) in a directory.\"\"\"\n",
    "    _max = -1\n",
    "    largest_file = None\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        num = extract_number(filename)\n",
    "        if num is not None and num > _max:\n",
    "            _max = num\n",
    "            largest_file = filename\n",
    "\n",
    "    return largest_file\n",
    "\n",
    "def return_max_filename(filename1, filename2):\n",
    "    # Improved handling for initial cases\n",
    "    if not filename1:\n",
    "        return filename2\n",
    "    if not filename2:\n",
    "        return filename1\n",
    "\n",
    "    # Extract epochs and compare\n",
    "    num1 = extract_number(filename1)\n",
    "    num2 = extract_number(filename2)\n",
    "\n",
    "    # Return the filename with the larger epoch number\n",
    "    return filename1 if num1 >= num2 else filename2\n",
    "\n",
    "def get_highest_num_path(base_dir, config):\n",
    "    \"\"\"\n",
    "    Check in the specific experiment directory derived from the config and return the path to the file\n",
    "    with the highest number along with its experiment directory.\n",
    "    \"\"\"\n",
    "    # Build the specific experiment directory from base_dir and config\n",
    "    experiment_dir, _ = create_experiment_directory(base_dir, config)\n",
    "    print(f\"Looking in {experiment_dir} for highest num saved\")\n",
    "\n",
    "    _max_file_path = None\n",
    "    _max_experiment_dir = experiment_dir\n",
    "\n",
    "    # Find the largest file in the specific experiment directory\n",
    "    _x = find_largest_file(experiment_dir)\n",
    "    if _x:\n",
    "        _max_file_path = os.path.join(experiment_dir, _x)\n",
    "        print(f\"Found max file path: {_max_file_path} and max experiment dir: {_max_experiment_dir}\")\n",
    "        _max_file_path = _max_file_path.split('.')[0]\n",
    "\n",
    "    return _max_file_path, _max_experiment_dir  # Return both file path and directory\n",
    "\n",
    "\n",
    "# def get_highest_num_path(base_dir, config):\n",
    "#     \"\"\"\n",
    "#     Check in all experiment directories derived from the config and return the path\n",
    "#     to the file with the highest number along with its experiment directory.\n",
    "#     \"\"\"\n",
    "\n",
    "#     experiment_index_path = base_dir + '/experiment_index.json'\n",
    "\n",
    "#     try: \n",
    "#         # Load the JSON data from the file\n",
    "#         with open(experiment_index_path, 'r') as file:\n",
    "#             experiment_index = json.load(file)\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         return None,None\n",
    "\n",
    "\n",
    "#     # Build the main part of the experiment directory from base_dir and config\n",
    "#     _experiment_dir, _ = create_experiment_directory(base_dir, config)\n",
    "#     base_experiment_dir = os.path.dirname(_experiment_dir)  # Strip the hash part\n",
    "\n",
    "#     print(f\"looking in {base_experiment_dir} for highest num saved\")\n",
    "#     _max_file_path = None\n",
    "#     _max_experiment_dir = None  # To keep track of the directory of the max file\n",
    "\n",
    "#     for k in list(experiment_index.keys()):\n",
    "#         _experiment_dir = experiment_index[k]['experiment_dir']\n",
    "#         if base_experiment_dir not in _experiment_dir:\n",
    "#             continue\n",
    "#         _x = find_largest_file(_experiment_dir)\n",
    "#         if _x:\n",
    "#             _x_path = os.path.join(_experiment_dir, _x)\n",
    "#             # Update max_file_path and the corresponding experiment_dir if a new max is found\n",
    "#             if not _max_file_path or return_max_filename(_x_path, _max_file_path) == _x_path:\n",
    "#                 _max_file_path = _x_path\n",
    "#                 _max_experiment_dir = _experiment_dir\n",
    "    \n",
    "\n",
    "#     print(f\"Found max file path: {_max_file_path} and max experiment dir: {_max_experiment_dir}\")\n",
    "    \n",
    "#     _max_file_path = _max_file_path.split('.')[0] if _max_file_path else None\n",
    "\n",
    "#     return _max_file_path, _max_experiment_dir  # Return both file path and directory\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bfa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_dict_to_gdrive(d,directory, filename):\n",
    "    #e.g. directory='/content/drive/My Drive/random_initial_weights'\n",
    "    filepath = directory + '/' + filename + '.pkl'\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(d, f)\n",
    "\n",
    "def load_dict_from_gdrive(directory,filename):\n",
    "    #e.g. directory='/content/drive/My Drive/random_initial_weights'\n",
    "    filepath = directory + '/' + filename + '.pkl'\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def download_weights():\n",
    "\n",
    "    # Define paths\n",
    "    zip_path = '/content/drive/MyDrive/model_weights.zip'\n",
    "    extract_path = '/content/drive/MyDrive'\n",
    "    \n",
    "    # Check if weights are already unzipped\n",
    "    example_path = '/content/drive/MyDrive/Experiments/barlow_twins/SSL/isicufes/resnet50/c34772a2/trained_encoder_epoch_99.pth'\n",
    "    \n",
    "    if os.path.exists(example_path):\n",
    "        print(\"Model weights are already set up.\")\n",
    "    else:\n",
    "        print(\"Model weights not found. Attempting to unzip...\")\n",
    "        \n",
    "        if os.path.exists(zip_path):\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            print(f\"Model weights unzipped to: {extract_path}/Experiments\")\n",
    "        else:\n",
    "            print(f\"Error: Zip file not found at: {zip_path}\")\n",
    "            print(\"Please ensure you've uploaded 'shared_model_weights.zip' to your Google Drive root\")\n",
    "# Usage:\n",
    "# setup_and_verify_model_weights() (only have to call once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
