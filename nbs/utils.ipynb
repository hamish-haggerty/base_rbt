{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32189ee",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> utility stuff.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e903be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39912107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.test import *\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "import random \n",
    "import os \n",
    "import numpy as np\n",
    "import yaml\n",
    "import configparser\n",
    "from types import SimpleNamespace\n",
    "import importlib\n",
    "from nbdev import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "cfg = config.get_config()\n",
    "PACKAGE_NAME = cfg.lib_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_grad_on(model):\n",
    "    \"\"\"\n",
    "    Test that all grads are on for modules with parameters.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Check each parameter in the module\n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            assert param.requires_grad, f\"Gradients are off for {name}.{param_name}\"\n",
    "\n",
    "def test_grad_off(model):\n",
    "    \"\"\"\n",
    "    Test that all non-batch norm grads are off, but batch norm grads are on.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Distinguish between BatchNorm and other layers\n",
    "        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert param.requires_grad, f\"BatchNorm parameter does not require grad in {name}.{param_name}\"\n",
    "        else:\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert not param.requires_grad, f\"Gradients are on for non-BatchNorm layer {name}.{param_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjust_config_with_derived_values(config):\n",
    "    # Adjust n_in based on dataset\n",
    "    if config.dataset == 'cifar10':\n",
    "        config.n_in = 3\n",
    "\n",
    "    # Adjust encoder_dimension based on architecture\n",
    "    if config.arch == 'resnet18':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet34':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet50':\n",
    "        config.encoder_dimension = 2048\n",
    "\n",
    "    return config\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        config = SimpleNamespace(**config)\n",
    "        config = adjust_config_with_derived_values(config)\n",
    "        \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae73551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ssl_dls(dataset,bs,device):\n",
    "    # Define the base package name in a variable for easy modification\n",
    "\n",
    "    try:\n",
    "        # Construct the module path\n",
    "        module_path = f\"{PACKAGE_NAME}.{dataset}_dataloading\"\n",
    "        \n",
    "        # Dynamically import the module\n",
    "        dataloading_module = importlib.import_module(module_path)\n",
    "    except ModuleNotFoundError:\n",
    "        # Handle the case where the module cannot be found\n",
    "        raise ImportError(f\"Could not find a data loading module for '{dataset}'. \"\n",
    "                          f\"Make sure '{module_path}' exists and is correctly named.\") from None\n",
    "    \n",
    "    # Assuming the function name follows a consistent naming convention\n",
    "    func_name = f\"get_bt_{dataset}_train_dls\"\n",
    "    try:\n",
    "        # Retrieve the data loading function from the module\n",
    "        data_loader_func = getattr(dataloading_module, func_name)\n",
    "    except AttributeError:\n",
    "        # Handle the case where the function does not exist in the module\n",
    "        raise AttributeError(f\"The function '{func_name}' was not found in '{module_path}'. \"\n",
    "                             \"Ensure it is defined and named correctly.\") from None\n",
    "    \n",
    "    # Proceed to call the function with arguments from the config\n",
    "    try:\n",
    "        dls_train = data_loader_func(bs=bs,device=device)\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the function call\n",
    "        raise RuntimeError(f\"An error occurred while calling '{func_name}' from '{module_path}': {e}\") from None\n",
    "    \n",
    "    return dls_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#| export\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_resnet_encoder(model,n_in=3):\n",
    "    model = create_body(model, n_in=n_in, pretrained=False, cut=len(list(model.children()))-1)\n",
    "    model.add_module('flatten', torch.nn.Flatten())\n",
    "    return model\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def create_resnet50_encoder(weight_type):\n",
    "\n",
    "#     #pretrained=True if 'weight_type' in ['bt_pretrain', 'supervised_pretrain'] else False\n",
    "\n",
    "#     if weight_type == 'bt_pretrain': model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "    \n",
    "#     elif weight_type == 'no_pretrain': model = resnet50()\n",
    "\n",
    "#     elif weight_type == 'supervised_pretrain': model = resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "#     #ignore the 'pretrained=False' argument here. Just means we use the weights above \n",
    "#     #(which themselves are either pretrained or not)\n",
    "#     encoder = get_resnet_encoder(model)\n",
    "\n",
    "#     return encoder\n",
    "\n",
    "@torch.no_grad()\n",
    "def resnet_arch_to_encoder(arch:str,weight_type='random'):\n",
    "    \"\"\"Given resnet architecture, return the encoder. Works for 3 channels.\n",
    "       The 'weight_type' argument is used to specify whether the model is pretrained or not\n",
    "    \"\"\"\n",
    "\n",
    "    n_in=3\n",
    "\n",
    "    test_eq(arch in ['resnet18','resnet34','resnet50'],True)\n",
    "    test_eq(weight_type in ['bt_pretrained','supervised_pretrained','random'],True)\n",
    "\n",
    "    if weight_type == 'bt_pretrained': test_eq(arch,'resnet50')\n",
    "\n",
    "    \n",
    "    if arch == 'resnet50':\n",
    "\n",
    "        if weight_type == 'bt_pretrained':\n",
    "            _model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "\n",
    "        elif weight_type == 'supervised_pretrained':\n",
    "            _model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet50()\n",
    "        \n",
    "\n",
    "    elif arch == 'resnet34':\n",
    "\n",
    "        if weight_type == 'supervised_pretrained':\n",
    "            _model = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet34() \n",
    "\n",
    "    elif arch == 'resnet18':\n",
    "        if weight_type == 'supervised_pretrained':\n",
    "            _model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) \n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet18()\n",
    "        \n",
    "    else: raise ValueError('Architecture not recognized')\n",
    "\n",
    "    return get_resnet_encoder(_model,n_in) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
