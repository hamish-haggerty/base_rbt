{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32189ee",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> utility stuff.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e903be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39912107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.test import *\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from typing import Literal\n",
    "import random \n",
    "import os \n",
    "import yaml\n",
    "import numpy as np\n",
    "import yaml\n",
    "import configparser\n",
    "from types import SimpleNamespace\n",
    "import importlib\n",
    "from nbdev import config\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "cfg = config.get_config()\n",
    "PACKAGE_NAME = cfg.lib_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_grad_on(model):\n",
    "    \"\"\"\n",
    "    Test that all grads are on for modules with parameters.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Check each parameter in the module\n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            assert param.requires_grad, f\"Gradients are off for {name}.{param_name}\"\n",
    "\n",
    "def test_grad_off(model):\n",
    "    \"\"\"\n",
    "    Test that all non-batch norm grads are off, but batch norm grads are on.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Distinguish between BatchNorm and other layers\n",
    "        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert param.requires_grad, f\"BatchNorm parameter does not require grad in {name}.{param_name}\"\n",
    "        else:\n",
    "            for param_name, param in module.named_parameters(recurse=False):\n",
    "                assert not param.requires_grad, f\"Gradients are on for non-BatchNorm layer {name}.{param_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjust_config_with_derived_values(config):\n",
    "    # Adjust n_in based on dataset\n",
    "\n",
    "    # Adjust encoder_dimension based on architecture\n",
    "        \n",
    "    if config.arch == 'smallres':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet18':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet34':\n",
    "        config.encoder_dimension = 512\n",
    "    elif config.arch == 'resnet50':\n",
    "        config.encoder_dimension = 2048\n",
    "\n",
    "    else :\n",
    "        raise ValueError(f\"Architecture {config.arch} not supported\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        config = SimpleNamespace(**config)\n",
    "        config = adjust_config_with_derived_values(config)\n",
    "        \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae73551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_ssl_dls(dataset,bs,size,device,pct_dataset=1.0):\n",
    "    # Define the base package name in a variable for easy modification\n",
    "\n",
    "    try:\n",
    "        # Construct the module path\n",
    "        module_path = f\"{PACKAGE_NAME}.{dataset}_dataloading\"\n",
    "        \n",
    "        # Dynamically import the module\n",
    "        dataloading_module = importlib.import_module(module_path)\n",
    "    except ModuleNotFoundError:\n",
    "        # Handle the case where the module cannot be found\n",
    "        raise ImportError(f\"Could not find a data loading module for '{dataset}'. \"\n",
    "                          f\"Make sure '{module_path}' exists and is correctly named.\") from None\n",
    "    \n",
    "    # Assuming the function name follows a consistent naming convention\n",
    "    func_name = f\"get_bt_{dataset}_train_dls\"\n",
    "    try:\n",
    "        # Retrieve the data loading function from the module\n",
    "        data_loader_func = getattr(dataloading_module, func_name)\n",
    "    except AttributeError:\n",
    "        # Handle the case where the function does not exist in the module\n",
    "        raise AttributeError(f\"The function '{func_name}' was not found in '{module_path}'. \"\n",
    "                             \"Ensure it is defined and named correctly.\") from None\n",
    "    \n",
    "    # Proceed to call the function with arguments from the config\n",
    "    try:\n",
    "        dls_train = data_loader_func(bs=bs,size=size,device=device,pct_dataset=pct_dataset)\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the function call\n",
    "        raise RuntimeError(f\"An error occurred while calling '{func_name}' from '{module_path}': {e}\") from None\n",
    "    \n",
    "    return dls_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65225c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_supervised_dls(dataset,bs,bs_test,size,device):\n",
    "    \"Get train and test dataloaders for supervised learning\"\n",
    "\n",
    "    try:\n",
    "        # Construct the module path\n",
    "        module_path = f\"{PACKAGE_NAME}.{dataset}_dataloading\"\n",
    "        \n",
    "        # Dynamically import the module\n",
    "        dataloading_module = importlib.import_module(module_path)\n",
    "    except ModuleNotFoundError:\n",
    "        # Handle the case where the module cannot be found\n",
    "        raise ImportError(f\"Could not find a data loading module for '{dataset}'. \"\n",
    "                          f\"Make sure '{module_path}' exists and is correctly named.\") from None\n",
    "    \n",
    "    # Assuming the function name follows a consistent naming convention\n",
    "    func_name_train = f\"get_supervised_{dataset}_train_dls\"\n",
    "    try:\n",
    "        # Retrieve the data loading function from the module\n",
    "        train_data_loader_func = getattr(dataloading_module, func_name_train)\n",
    "    except AttributeError:\n",
    "        # Handle the case where the function does not exist in the module\n",
    "        raise AttributeError(f\"The function '{func_name_train}' was not found in '{module_path}'. \"\n",
    "                             \"Ensure it is defined and named correctly.\") from None\n",
    "    \n",
    "    # Proceed to call the function with arguments from the config\n",
    "    try:\n",
    "        dls_train = train_data_loader_func(bs=bs,size=size,device=device)\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the function call\n",
    "        raise RuntimeError(f\"An error occurred while calling '{func_name_train}' from '{module_path}': {e}\") from None\n",
    "    \n",
    "    \n",
    "      # Assuming the function name follows a consistent naming convention\n",
    "    func_name_test = f\"get_supervised_{dataset}_test_dls\"\n",
    "    try:\n",
    "        # Retrieve the data loading function from the module\n",
    "        test_data_loader_func = getattr(dataloading_module, func_name_test)\n",
    "    except AttributeError:\n",
    "        # Handle the case where the function does not exist in the module\n",
    "        raise AttributeError(f\"The function '{func_name_test}' was not found in '{module_path}'. \"\n",
    "                             \"Ensure it is defined and named correctly.\") from None\n",
    "    \n",
    "    # Proceed to call the function with arguments from the config\n",
    "    try:\n",
    "        dls_test = test_data_loader_func(bs=bs_test,size=size,device=device)\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the function call\n",
    "        raise RuntimeError(f\"An error occurred while calling '{func_name_test}' from '{module_path}': {e}\") from None\n",
    "    \n",
    "    \n",
    "\n",
    "    return {'train_dls':dls_train,'test_dls':dls_test}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d83ee",
   "metadata": {},
   "source": [
    "Basic network mostly used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _SmallRes(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Minimal Intermediate layer\n",
    "        self.intermediate_conv = nn.Conv2d(64, 512, kernel_size=1, stride=1, bias=False)\n",
    "        self.intermediate_bn = nn.BatchNorm2d(512)\n",
    "        self.intermediate_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Last two layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Minimal Intermediate layer\n",
    "        x = self.intermediate_conv(x)\n",
    "        x = self.intermediate_bn(x)\n",
    "        x = self.intermediate_relu(x)\n",
    "\n",
    "        # Last two layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def get_resnet_encoder(model,n_in=3):\n",
    "    model = create_body(model, n_in=n_in, pretrained=False, cut=len(list(model.children()))-1)\n",
    "    model.add_module('flatten', torch.nn.Flatten())\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def resnet_arch_to_encoder(arch: Literal['smallres','resnet18', 'resnet34', 'resnet50'],\n",
    "                           weight_type: Literal['random', 'partialtrained', 'imgnet_bt_pretrained', 'imgnet_sup_pretrained'] = 'random'):\n",
    "    \"\"\"Given a ResNet architecture, return the encoder configured for 3 input channels.\n",
    "       The 'weight_type' argument specifies the weight initialization strategy.\n",
    "\n",
    "    Args:\n",
    "        arch (Literal['smallres','resnet18', 'resnet34', 'resnet50']): The architecture of the ResNet.\n",
    "        weight_type (Literal['random', 'partialtrained', 'imgnet_bt_pretrained', 'imgnet_sup_pretrained']): Specifies the weight initialization strategy. Defaults to 'random'.\n",
    "\n",
    "    Returns:\n",
    "        Encoder: An encoder configured for 3 input channels and specified architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_in=3\n",
    "\n",
    "\n",
    "    if weight_type == 'imgnet_bt_pretrained': test_eq(arch,'resnet50')\n",
    "\n",
    "    #This means we will load the state_dict from a path.\n",
    "    if weight_type == 'partialtrained': weight_type = 'random'\n",
    "\n",
    "    \n",
    "    if arch == 'resnet50':\n",
    "\n",
    "        if weight_type == 'imgnet_bt_pretrained':\n",
    "            _model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "\n",
    "        elif weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet50()\n",
    "        \n",
    "\n",
    "    elif arch == 'resnet34':\n",
    "\n",
    "        if weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet34() \n",
    "\n",
    "    elif arch == 'resnet18':\n",
    "        if weight_type == 'imgnet_sup_pretrained':\n",
    "            _model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) \n",
    "\n",
    "        elif weight_type == 'random':\n",
    "            _model = resnet18()\n",
    "\n",
    "    elif arch == 'smallres':\n",
    "        _model = _SmallRes()\n",
    "    \n",
    "        \n",
    "    else: raise ValueError('Architecture not recognized')\n",
    "\n",
    "    return get_resnet_encoder(_model,n_in) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d288f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generate_config_hash(config):\n",
    "    \"\"\"\n",
    "    Generates a unique hash for a given experiment configuration.\n",
    "    \n",
    "    Args:\n",
    "    config (dict or Namespace): Experiment configuration. Can be a dictionary or a namespace object.\n",
    "    \n",
    "    Returns:\n",
    "    str: A unique hash representing the experiment configuration.\n",
    "    \"\"\"\n",
    "    # Convert config to dict if it's a Namespace\n",
    "    config_dict = vars(config) if not isinstance(config, dict) else config\n",
    "    \n",
    "    # Serialize configuration to a sorted JSON string to ensure consistency\n",
    "    config_str = json.dumps(config_dict, sort_keys=True)\n",
    "    \n",
    "    # Generate SHA-256 hash from the serialized string\n",
    "    hash_obj = hashlib.sha256(config_str.encode())  # Encode to convert string to bytes\n",
    "    config_hash = hash_obj.hexdigest()\n",
    "    \n",
    "    # Optionally, return a truncated version of the hash for readability\n",
    "    short_hash = config_hash[:8]  # Use the first 8 characters as an example\n",
    "    return short_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879decf",
   "metadata": {},
   "source": [
    "Test `generate_config_hash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = SimpleNamespace(arch='resnet18', dataset='cifar10', n_in=3, encoder_dimension=512)\n",
    "config2 = SimpleNamespace(arch='resnet34', dataset='cifar10', n_in=3, encoder_dimension=512)\n",
    "config3 = SimpleNamespace(arch='resnet50', dataset='cifar10', n_in=3, encoder_dimension=2048)\n",
    "\n",
    "test_eq(generate_config_hash(config1), generate_config_hash(config1))\n",
    "test_ne(generate_config_hash(config1), generate_config_hash(config2))\n",
    "test_ne(generate_config_hash(config1), generate_config_hash(config3))\n",
    "test_ne(generate_config_hash(config2), generate_config_hash(config3))\n",
    "\n",
    "config4 = SimpleNamespace(dataset='cifar10', arch='resnet18', n_in=3, encoder_dimension=512)  # Different order\n",
    "test_eq(generate_config_hash(config1), generate_config_hash(config4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_experiment_directory(base_dir, config):\n",
    "    # Generate a unique hash for the configuration\n",
    "    unique_hash = generate_config_hash(config)\n",
    "    \n",
    "    # Construct the directory path for this experiment\n",
    "    experiment_dir = os.path.join(base_dir, config.train_type, config.dataset, config.arch, unique_hash)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    \n",
    "    return experiment_dir,unique_hash\n",
    "\n",
    "\n",
    "def save_configuration(config, experiment_dir):\n",
    "    \"\"\"\n",
    "    Saves the experiment configuration as a YAML file in the experiment directory.\n",
    "\n",
    "    Args:\n",
    "    config (dict, Namespace, or any serializable object): Experiment configuration.\n",
    "    experiment_dir (str): Path to the directory where the config file will be saved.\n",
    "    \"\"\"\n",
    "    config_file_path = os.path.join(experiment_dir, 'config.yaml')\n",
    "    \n",
    "    # Check if config is not a dictionary (e.g., a Namespace object) and convert if necessary\n",
    "    config_dict = vars(config) if not isinstance(config, dict) else config\n",
    "    \n",
    "    with open(config_file_path, 'w') as file:\n",
    "        yaml.dump(config_dict, file)\n",
    "    \n",
    "    print(f\"Configuration saved to {config_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_metadata_file(experiment_dir, git_commit_hash):\n",
    "    \"\"\"\n",
    "    Saves a metadata file with the Git commit hash\n",
    "    \"\"\"\n",
    "    metadata_file_path = os.path.join(experiment_dir, 'metadata.yaml')\n",
    "    metadata_content = {\n",
    "        \"Git Commit Hash\": git_commit_hash,\n",
    "    }\n",
    "\n",
    "    with open(metadata_file_path, 'w') as file:\n",
    "        yaml.dump(metadata_content, file)\n",
    "\n",
    "    print(f\"Metadata saved to {metadata_file_path}\")\n",
    "\n",
    "\n",
    "def update_experiment_index(project_root, details):\n",
    "    central_json_path = os.path.join(project_root, 'experiment_index.json')\n",
    "    \n",
    "    if os.path.exists(central_json_path):\n",
    "        with open(central_json_path, 'r') as file:\n",
    "            experiments_index = json.load(file)\n",
    "    else:\n",
    "        experiments_index = {}\n",
    "    \n",
    "    experiment_hash = details[\"experiment_hash\"]\n",
    "    experiments_index[experiment_hash] = details\n",
    "    \n",
    "    with open(central_json_path, 'w') as file:\n",
    "        json.dump(experiments_index, file, indent=4)\n",
    "    \n",
    "    print(f\"Updated experiment index for hash: {experiment_hash}\")\n",
    "\n",
    "\n",
    "def get_latest_commit_hash(repo_path):\n",
    "    try:\n",
    "        commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=repo_path).decode('ascii').strip()\n",
    "        return commit_hash\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error obtaining latest commit hash: {e}\")\n",
    "        return None\n",
    "\n",
    "def setup_experiment(config,base_dir):\n",
    "\n",
    "    # Create a unique directory for this experiment based on its configuration\n",
    "    # This directory will contain all artifacts related to the experiment, such as model checkpoints and logs.\n",
    "    experiment_dir, experiment_hash = create_experiment_directory(base_dir, config)\n",
    "\n",
    "    print(f\"The experiment_dir is: {experiment_dir} and the experiment hash is: {experiment_hash}\")\n",
    "\n",
    "    # Save the loaded configuration to the experiment directory as a YAML file\n",
    "    # This ensures that we can reproduce or analyze the experiment later.\n",
    "    save_configuration(config, experiment_dir)\n",
    "\n",
    "    git_commit_hash = get_latest_commit_hash('.')\n",
    "    print(f\"The git hash is: {git_commit_hash}\")\n",
    "\n",
    "    return experiment_dir, experiment_hash,git_commit_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class InterruptCallback(Callback):\n",
    "    def __init__(self, interrupt_epoch):\n",
    "        super().__init__()\n",
    "        self.interrupt_epoch = interrupt_epoch\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if self.epoch == self.interrupt_epoch:\n",
    "            print(f\"Interrupting training before starting epoch {self.interrupt_epoch}\")\n",
    "            raise CancelFitException\n",
    "\n",
    "class SaveLearnerCheckpoint(Callback):\n",
    "    def __init__(self, experiment_dir,start_epoch=0, save_interval=250, with_opt=True):\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.start_epoch = start_epoch\n",
    "        self.save_interval = save_interval\n",
    "        self.with_opt = with_opt  # Decide whether to save optimizer state as well.\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if (self.epoch+1) % self.save_interval == 0 and self.epoch>=self.start_epoch:\n",
    "            print(f\"Saving model and learner state at epoch {self.epoch}\")\n",
    "   \n",
    "            checkpoint_filename = f\"learner_checkpoint_epoch_{self.epoch}\"\n",
    "            checkpoint_path = os.path.join(self.experiment_dir, checkpoint_filename)\n",
    "            # Save the entire learner object, including the model's parameters and optimizer state.\n",
    "            self.learn.save(checkpoint_path, with_opt=self.with_opt)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba403c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving at epoch 5\n",
      "saving at epoch 11\n"
     ]
    }
   ],
   "source": [
    "save_interval=6\n",
    "for epoch in range(12):\n",
    "\n",
    "    if (epoch+1) % save_interval == 0:\n",
    "\n",
    "        print(f'saving at epoch {epoch}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_epoch(filename):\n",
    "    \"\"\"Extract the epoch number from a filename.\"\"\"\n",
    "    pattern = re.compile(r\"_epoch_(\\d+)\\.pt[h]?\")\n",
    "    match = pattern.search(filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def find_largest_epoch_file(directory_path):\n",
    "    \"\"\"Find the file with the largest epoch number in a directory.\"\"\"\n",
    "    max_epoch = -1\n",
    "    largest_epoch_file = None\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        epoch = extract_epoch(filename)\n",
    "        if epoch is not None and epoch > max_epoch:\n",
    "            max_epoch = epoch\n",
    "            largest_epoch_file = filename\n",
    "\n",
    "    return largest_epoch_file\n",
    "\n",
    "def return_max_filename(filename1, filename2):\n",
    "    # Improved handling for initial cases\n",
    "    if not filename1:\n",
    "        return filename2\n",
    "    if not filename2:\n",
    "        return filename1\n",
    "\n",
    "    # Extract epochs and compare\n",
    "    epoch1 = extract_epoch(filename1)\n",
    "    epoch2 = extract_epoch(filename2)\n",
    "\n",
    "    # Return the filename with the larger epoch number\n",
    "    return filename1 if epoch1 >= epoch2 else filename2\n",
    "\n",
    "\n",
    "def get_highest_epoch_path(base_dir, config):\n",
    "    \"\"\"\n",
    "    Check in all experiment directories derived from the config and return the path\n",
    "    to the file with the highest epoch along with its experiment directory.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_index_path = base_dir + '/experiment_index.json'\n",
    "\n",
    "    try: \n",
    "        # Load the JSON data from the file\n",
    "        with open(experiment_index_path, 'r') as file:\n",
    "            experiment_index = json.load(file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None,None\n",
    "\n",
    "\n",
    "    # Build the main part of the experiment directory from base_dir and config\n",
    "    _experiment_dir, _ = create_experiment_directory(base_dir, config)\n",
    "    base_experiment_dir = os.path.dirname(_experiment_dir)  # Strip the hash part\n",
    "\n",
    "    print(f\"looking in {base_experiment_dir} for highest epoch saved\")\n",
    "\n",
    "    _max_file_path = None\n",
    "    _max_experiment_dir = None  # To keep track of the directory of the max file\n",
    "\n",
    "    for k in list(experiment_index.keys()):\n",
    "        _experiment_dir = experiment_index[k]['experiment_dir']\n",
    "        if base_experiment_dir not in _experiment_dir:\n",
    "            continue\n",
    "        _x = find_largest_epoch_file(_experiment_dir)\n",
    "        if _x:\n",
    "            _x_path = os.path.join(_experiment_dir, _x)\n",
    "            # Update max_file_path and the corresponding experiment_dir if a new max is found\n",
    "            if not _max_file_path or return_max_filename(_x_path, _max_file_path) == _x_path:\n",
    "                _max_file_path = _x_path\n",
    "                _max_experiment_dir = _experiment_dir\n",
    "    \n",
    "\n",
    "    print(f\"Found max file path: {_max_file_path} and max experiment dir: {_max_experiment_dir}\")\n",
    "    \n",
    "    return _max_file_path.split('.')[0], _max_experiment_dir  # Return both file path and directory\n",
    "\n",
    "\n",
    "\n",
    "def get_experiment_state(config,base_dir):\n",
    "    \"\"\"Get the load_learner_path, learn_type, start_epoch, interrupt_epoch for BT experiment.\n",
    "       Basically this tells us how to continue learning (e.g. we have run two sessions for \n",
    "       100 epochs, and want to continue for another 100 epochs). Return values are\n",
    "       None if we are starting from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    load_learner_path, _  = get_highest_epoch_path(base_dir, config)\n",
    "\n",
    "    #TODO:\n",
    "    #We can get start_epoch, interrupt epoch from `get_highest_epoch_path` + save_interval (may be None!)\n",
    "    start_epoch=0 if load_learner_path is None else int(load_learner_path.split('_')[-1])+1\n",
    "    interrupt_epoch = start_epoch + config.save_interval\n",
    "\n",
    "    #We can also get the learn_type from the load_learner_path + weight_type. \n",
    "    \n",
    "    if config.weight_type == 'random':\n",
    "        learn_type = 'standard'\n",
    "    \n",
    "    elif 'pretrained' in config.weight_type:\n",
    "        learn_type = 'transfer_learning'\n",
    "\n",
    "    learn_type = learn_type if load_learner_path is None else 'continue_learning'\n",
    "\n",
    "    return load_learner_path, learn_type, start_epoch, interrupt_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
